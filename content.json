{"meta":{"title":"帅的掉渣的博客","subtitle":null,"description":null,"author":"listwebit","url":"http://yoursite.com"},"pages":[{"title":"about","date":"2018-08-10T06:22:54.000Z","updated":"2018-08-10T06:22:54.035Z","comments":true,"path":"about/index-1.html","permalink":"http://yoursite.com/about/index-1.html","excerpt":"","text":""},{"title":"深度学习","date":"2018-08-11T00:36:47.000Z","updated":"2018-08-11T00:36:47.751Z","comments":true,"path":"深度学习/index.html","permalink":"http://yoursite.com/深度学习/index.html","excerpt":"","text":""},{"title":"'about'","date":"2018-08-10T06:10:04.000Z","updated":"2018-08-10T06:10:38.230Z","comments":true,"path":"about/index.html","permalink":"http://yoursite.com/about/index.html","excerpt":"","text":"这里是关于页面"}],"posts":[{"title":"文章观点抽取.md","slug":"NLP/3-短语分析/观点抽取/文章观点抽取/文章观点抽取","date":"2018-05-18T05:17:35.000Z","updated":"2018-09-26T12:13:05.016Z","comments":true,"path":"2018/05/18/NLP/3-短语分析/观点抽取/文章观点抽取/文章观点抽取/","link":"","permalink":"http://yoursite.com/2018/05/18/NLP/3-短语分析/观点抽取/文章观点抽取/文章观点抽取/","excerpt":"","text":"","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"},{"name":"3-短语分析","slug":"NLP/3-短语分析","permalink":"http://yoursite.com/categories/NLP/3-短语分析/"},{"name":"观点抽取","slug":"NLP/3-短语分析/观点抽取","permalink":"http://yoursite.com/categories/NLP/3-短语分析/观点抽取/"},{"name":"文章观点抽取","slug":"NLP/3-短语分析/观点抽取/文章观点抽取","permalink":"http://yoursite.com/categories/NLP/3-短语分析/观点抽取/文章观点抽取/"}],"tags":[]},{"title":"评论观点抽取","slug":"NLP/3-短语分析/观点抽取/评论观点抽取/评论观点抽取","date":"2018-05-16T05:17:35.000Z","updated":"2018-09-26T12:12:36.848Z","comments":true,"path":"2018/05/16/NLP/3-短语分析/观点抽取/评论观点抽取/评论观点抽取/","link":"","permalink":"http://yoursite.com/2018/05/16/NLP/3-短语分析/观点抽取/评论观点抽取/评论观点抽取/","excerpt":"","text":"","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"},{"name":"3-短语分析","slug":"NLP/3-短语分析","permalink":"http://yoursite.com/categories/NLP/3-短语分析/"},{"name":"观点抽取","slug":"NLP/3-短语分析/观点抽取","permalink":"http://yoursite.com/categories/NLP/3-短语分析/观点抽取/"},{"name":"评论观点抽取","slug":"NLP/3-短语分析/观点抽取/评论观点抽取","permalink":"http://yoursite.com/categories/NLP/3-短语分析/观点抽取/评论观点抽取/"}],"tags":[]},{"title":"观点抽取分类","slug":"NLP/3-短语分析/观点抽取/观点抽取分类","date":"2018-05-16T05:17:35.000Z","updated":"2018-09-26T12:11:59.135Z","comments":true,"path":"2018/05/16/NLP/3-短语分析/观点抽取/观点抽取分类/","link":"","permalink":"http://yoursite.com/2018/05/16/NLP/3-短语分析/观点抽取/观点抽取分类/","excerpt":"","text":"一、观点抽取的分类我认为按照观点抽取的对象可以分为两类，一种是对文章进行观点抽取，得到文章的类似摘要、分类、关键词等性质的句子、短语或者关键词。一种是对评论进行观点抽取，得到一个短语，然后在对短语进行分类。 1.文章观点抽取文章的观点抽取结果可以是一句话（类似标题），一段话（类似摘要），一个短语，一个关键词（打标签，其实是文本分类）。 2.评论观点抽取评论的观点抽取结果一般是一个短语。类似淘宝评论的观点抽取。例如可以将某一件衣服评论观点分为：质量好，超划算，性价比高，穿上好看。","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"},{"name":"3-短语分析","slug":"NLP/3-短语分析","permalink":"http://yoursite.com/categories/NLP/3-短语分析/"},{"name":"观点抽取","slug":"NLP/3-短语分析/观点抽取","permalink":"http://yoursite.com/categories/NLP/3-短语分析/观点抽取/"}],"tags":[]},{"title":"回归","slug":"MachineLearning/2-回归/回归","date":"2018-01-09T08:02:27.000Z","updated":"2018-01-09T08:02:27.288Z","comments":true,"path":"2018/01/09/MachineLearning/2-回归/回归/","link":"","permalink":"http://yoursite.com/2018/01/09/MachineLearning/2-回归/回归/","excerpt":"","text":"","categories":[{"name":"MachineLearning","slug":"MachineLearning","permalink":"http://yoursite.com/categories/MachineLearning/"},{"name":"2-回归","slug":"MachineLearning/2-回归","permalink":"http://yoursite.com/categories/MachineLearning/2-回归/"}],"tags":[]},{"title":"回归的分类","slug":"MachineLearning/2-回归/回归的分类","date":"2018-01-09T08:02:27.000Z","updated":"2018-01-09T08:31:26.000Z","comments":true,"path":"2018/01/09/MachineLearning/2-回归/回归的分类/","link":"","permalink":"http://yoursite.com/2018/01/09/MachineLearning/2-回归/回归的分类/","excerpt":"一、回归可以分为以下几类 1.线性回归 2.非线性回归 3.逻辑回归","text":"一、回归可以分为以下几类 1.线性回归 2.非线性回归 3.逻辑回归 二、回归的概念1.线性回归可以简单理解为线性就是每个变量的指数都是1。 2.非线性回归而非线性就是至少有一个变量的指数不是1。 3.逻辑回归回归和分类的区别分类和回归的区别在于输出变量的类型。 定量输出称为回归，或者说是连续变量预测；定性输出称为分类，或者说是离散变量预测。 举个例子：预测明天的气温是多少度，这是一个回归任务；预测明天是阴、晴还是雨，就是一个分类任务。","categories":[{"name":"MachineLearning","slug":"MachineLearning","permalink":"http://yoursite.com/categories/MachineLearning/"},{"name":"2-回归","slug":"MachineLearning/2-回归","permalink":"http://yoursite.com/categories/MachineLearning/2-回归/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/机器学习/"}]},{"title":"代价函数，损失函数，目标函数区别","slug":"MachineLearning/0-基本概念/代价函数，损失函数，目标函数区别","date":"2018-01-09T08:02:27.000Z","updated":"2018-11-01T09:46:59.187Z","comments":true,"path":"2018/01/09/MachineLearning/0-基本概念/代价函数，损失函数，目标函数区别/","link":"","permalink":"http://yoursite.com/2018/01/09/MachineLearning/0-基本概念/代价函数，损失函数，目标函数区别/","excerpt":"","text":"首先给出结论： 损失函数（Loss Function ）是定义在单个样本上的，算的是一个样本的误差。 代价函数（Cost Function ）是定义在整个训练集上的，是所有样本误差的平均，也就是损失函数的平均。 目标函数（Object Function）定义为：最终需要优化的函数。等于经验风险+结构风险（也就是Cost Function + 正则化项）。 参考：https://www.zhihu.com/question/52398145","categories":[{"name":"MachineLearning","slug":"MachineLearning","permalink":"http://yoursite.com/categories/MachineLearning/"},{"name":"0-基本概念","slug":"MachineLearning/0-基本概念","permalink":"http://yoursite.com/categories/MachineLearning/0-基本概念/"}],"tags":[]},{"title":"2017年总结","slug":"总结/2017年总结","date":"2018-01-09T06:55:28.000Z","updated":"2018-08-11T01:06:32.802Z","comments":true,"path":"2018/01/09/总结/2017年总结/","link":"","permalink":"http://yoursite.com/2018/01/09/总结/2017年总结/","excerpt":"","text":"","categories":[{"name":"总结","slug":"总结","permalink":"http://yoursite.com/categories/总结/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/tags/深度学习/"}]},{"title":"2017年","slug":"总结/2017年","date":"2018-01-09T06:55:03.000Z","updated":"2018-01-09T06:55:03.415Z","comments":true,"path":"2018/01/09/总结/2017年/","link":"","permalink":"http://yoursite.com/2018/01/09/总结/2017年/","excerpt":"","text":"","categories":[{"name":"总结","slug":"总结","permalink":"http://yoursite.com/categories/总结/"}],"tags":[]},{"title":"1-判别模型和生成模型.md","slug":"MachineLearning/1-模型/1-判别模型和生成模型","date":"2017-12-19T09:17:35.000Z","updated":"2018-10-08T06:02:20.795Z","comments":true,"path":"2017/12/19/MachineLearning/1-模型/1-判别模型和生成模型/","link":"","permalink":"http://yoursite.com/2017/12/19/MachineLearning/1-模型/1-判别模型和生成模型/","excerpt":"","text":"判别模型 生成模型 特点 寻找不同类别之间的最优分类面，反映的是异类数据之间的差异 对后验概率建模，从统计的角度表示数据的分布情况，能够反映同类数据本身的相似度 区别(假定输入x, 类别标签y) 估计的是条件概率分布(conditional distribution) : P(yx) 估计的是联合概率分布（joint probability distribution: P(x, y) 联系 由判别式模型得不到产生式模型 由产生式模型可以得到判别式模型 常见模型 – logistic regression – SVMs – traditional neural networks – Nearest neighbor –Gaussians, Naive Bayes –Mixtures of Gaussians, Mixtures of experts, HMMs–Sigmoidal belief networks, Bayesian networks– Markov random fields","categories":[{"name":"MachineLearning","slug":"MachineLearning","permalink":"http://yoursite.com/categories/MachineLearning/"},{"name":"1-模型","slug":"MachineLearning/1-模型","permalink":"http://yoursite.com/categories/MachineLearning/1-模型/"}],"tags":[]},{"title":"2-模型评估的指标","slug":"MachineLearning/1-模型/2-模型评估的指标","date":"2017-12-19T09:17:35.000Z","updated":"2018-10-08T06:03:43.875Z","comments":true,"path":"2017/12/19/MachineLearning/1-模型/2-模型评估的指标/","link":"","permalink":"http://yoursite.com/2017/12/19/MachineLearning/1-模型/2-模型评估的指标/","excerpt":"","text":"多模型评估的指标可以分为以下几个类别 一.Accuracy，Precision，Recall要计算这几个指标先要了解几个概念：FN：False Negative,被判定为负样本，但事实上是正样本。FP：False Positive,被判定为正样本，但事实上是负样本。TN：True Negative,被判定为负样本，事实上也是负样本。TP：True Positive,被判定为正样本，事实上也是证样本。1.Accuracy (正确率)$ac=\\frac {TP+TN}{TP+TN+FP+FN}$ 2.Precision精确率，准确率，查准率$P = \\frac {TP}{TP+FP}$解释：正样本占分类器所分的正样本的比例 3.Recall(召回率，查全率)$R = \\frac {TP}{TP + FN}$解释：正样本占真正的正样本的比例","categories":[{"name":"MachineLearning","slug":"MachineLearning","permalink":"http://yoursite.com/categories/MachineLearning/"},{"name":"1-模型","slug":"MachineLearning/1-模型","permalink":"http://yoursite.com/categories/MachineLearning/1-模型/"}],"tags":[]},{"title":"3-模型假设函数,损失函数,代价函数,目标函数","slug":"MachineLearning/1-模型/3-模型假设函数,损失函数,代价函数,目标函数","date":"2017-12-19T09:17:35.000Z","updated":"2018-11-27T15:15:23.023Z","comments":true,"path":"2017/12/19/MachineLearning/1-模型/3-模型假设函数,损失函数,代价函数,目标函数/","link":"","permalink":"http://yoursite.com/2017/12/19/MachineLearning/1-模型/3-模型假设函数,损失函数,代价函数,目标函数/","excerpt":"","text":"假设函数(hypothesis function):预测函数损失函数(loss function):计算的是一个样本的误差代价函数(cost function,成本函数):是整个训练集上所有样本误差的平均目标函数(objective function):代价函数 + 正则化项","categories":[{"name":"MachineLearning","slug":"MachineLearning","permalink":"http://yoursite.com/categories/MachineLearning/"},{"name":"1-模型","slug":"MachineLearning/1-模型","permalink":"http://yoursite.com/categories/MachineLearning/1-模型/"}],"tags":[]},{"title":"0-语言模型","slug":"NLP/11-语言模型/0-语言模型","date":"2017-12-18T09:17:35.000Z","updated":"2018-11-13T10:13:35.720Z","comments":true,"path":"2017/12/18/NLP/11-语言模型/0-语言模型/","link":"","permalink":"http://yoursite.com/2017/12/18/NLP/11-语言模型/0-语言模型/","excerpt":"","text":"语言模型就是用于评估文本符合语言使用习惯程度的模型。 一、定义我们目前所说的语言模型主要指的是统计语言模型。统计语言模型是一个单词序列上的概率分布，对于一个给定长度为m的序列，它可以为整个序列产生一个概率 P(w1,w2,…,wm)。其实就是想办法找到一个概率分布，它可以表示任意一个句子或序列出现的概率。 二、统计语言模型对于一个由T个词按顺序构成的句子$s=w_1w_2w_3…w_t$，p(s)实际上求解的是字符串$w_1w_2w_3…w_t$的联合概率，利用贝叶斯公式，链式分解如下：$P(s)=P(w_1w_2w_3…w_t) =P(w_1)P(w_2|w_1)P(w_3|w_1w_2)P(w_T|w1…w{T-1})$从上面可以看到，一个统计语言模型可以表示成，给定前面的的词，求后面一个词出现的条件概率。我们在求p(s)时实际上就已经建立了一个模型，这里的p(*)就是模型的参数，如果这些参数已经求解得到，那么很容易就能够得到字符串s的概率。 由于上式中的参数过多，因此需要近似的计算方法。常见的方法有n-gram模型方法、决策树方法、最大熵模型方法、最大熵马尔科夫模型方法、条件随机域方法、神经网络方法，等等。 三、","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"},{"name":"11-语言模型","slug":"NLP/11-语言模型","permalink":"http://yoursite.com/categories/NLP/11-语言模型/"}],"tags":[]},{"title":"这里是词法分析","slug":"NLP/1-词法分析/这里是词法分析","date":"2017-12-18T09:17:35.000Z","updated":"2018-08-12T08:48:26.854Z","comments":true,"path":"2017/12/18/NLP/1-词法分析/这里是词法分析/","link":"","permalink":"http://yoursite.com/2017/12/18/NLP/1-词法分析/这里是词法分析/","excerpt":"","text":"","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"},{"name":"1-词法分析","slug":"NLP/1-词法分析","permalink":"http://yoursite.com/categories/NLP/1-词法分析/"}],"tags":[]},{"title":"0文本分类算法总结","slug":"NLP/7-文本分类/0.文本分类算法总结","date":"2017-12-18T09:17:35.000Z","updated":"2018-09-18T10:05:46.154Z","comments":true,"path":"2017/12/18/NLP/7-文本分类/0.文本分类算法总结/","link":"","permalink":"http://yoursite.com/2017/12/18/NLP/7-文本分类/0.文本分类算法总结/","excerpt":"","text":"","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"},{"name":"7-文本分类","slug":"NLP/7-文本分类","permalink":"http://yoursite.com/categories/NLP/7-文本分类/"}],"tags":[]},{"title":"PageRank算法的前世今生","slug":"NLP/1-词法分析/4-关键词提取/1-PageRank的前世今生","date":"2017-12-18T09:17:35.000Z","updated":"2018-08-13T00:08:23.140Z","comments":true,"path":"2017/12/18/NLP/1-词法分析/4-关键词提取/1-PageRank的前世今生/","link":"","permalink":"http://yoursite.com/2017/12/18/NLP/1-词法分析/4-关键词提取/1-PageRank的前世今生/","excerpt":"","text":"一、算法定义PageRank，网页排名，又称网页级别、Google左侧排名或佩奇排名，是一种由 [1] 根据网页之间相互的超链接计算的技术，而作为网页排名的要素之一，以Google公司创办人拉里·佩奇（Larry Page）之姓来命名。Google用它来体现网页的相关性和重要性，在搜索引擎优化操作中是经常被用来评估网页优化的成效因素之一。Google的创始人拉里·佩奇和谢尔盖·布林于1998年在斯坦福大学发明了这项技术。 PageRank通过网络浩瀚的超链接关系来确定一个页面的等级。Google把从A页面到B页面的链接解释为A页面给B页面投票，Google根据投票来源（甚至来源的来源，即链接到A页面的页面）和投票目标的等级来决定新的等级。简单的说，一个高等级的页面可以使其他低等级页面的等级提升。 二、算法来源这个要从搜索引擎的发展讲起。最早的搜索引擎采用的是 分类目录[^ref_1] 的方法，即通过人工进行网页分类并整理出高质量的网站。那时 Yahoo 和国内的 hao123 就是使用的这种方法。 后来网页越来越多，人工分类已经不现实了。搜索引擎进入了 文本检索 的时代，即计算用户查询关键词与网页内容的相关程度来返回搜索结果。这种方法突破了数量的限制，但是搜索结果不是很好。因为总有某些网页来回地倒腾某些关键词使自己的搜索排名靠前。 于是我们的主角要登场了。没错，谷歌的两位创始人，当时还是美国斯坦福大学 (Stanford University) 研究生的佩奇 (Larry Page) 和布林 (Sergey Brin) 开始了对网页排序问题的研究。他们的借鉴了学术界评判学术论文重要性的通用方法， 那就是看论文的引用次数。由此想到网页的重要性也可以根据这种方法来评价。于是PageRank的核心思想就诞生了[^ref_2]，非常简单：121.如果一个网页被很多其他网页链接到的话说明这个网页比较重要，也就是PageRank值会相对较高2.如果一个PageRank值很高的网页链接到一个其他的网页，那么被链接到的网页的PageRank值会相应地因此而提高 三、搜索引擎搜索引擎的功能主要是：根据用户输入的关键字，返回文档的链接结果。搜索引擎主要解决的三大问题：(1)如何获取文档资料 (2)如何根据关键词检索到相关文档 (3)如何对文档进行排序，返回给用户满意的页面。 1.获取文档资料利用爬虫程序，获取互联网的页面资料。爬虫技术先从一个网页出发，将该网页的内容记录下来，保存到资料库，接着分析页面中的超链接，分别递归得去获取超链接页面。 2.如何根据关键词检索到相关文档采用倒排索引算法。简单的说，倒排索引是一对key-value对，key代表关键词，value代表拥有这些关键词的文档编号或者url。 3.文档排序这是搜索引擎最核心的问题，也是google发家致富的法宝 – PageRank算法。 四、算法原理4.1PageRank的两个假设 数量假设：1.如果一个网页被很多其他网页链接到的话说明这个网页比较重要，也就是PageRank值会相对较高。质量假设：2.如果一个PageRank值很高的网页链接到一个其他的网页，那么被链接到的网页的PageRank值会相应地因此而提高。 4.2PageRank的算法原理PageRank算法总的来说就是预先给每个网页一个PR值（下面用PR值指代PageRank值），由于PR值物理意义上为一个网页被访问概率，所以一般是1/N，其中N为网页总数。预先给定PR值后，通过下面的算法不断迭代，直至达到平稳分布为止。 PageRank值主要是的是节点的入链值。 4.3PageRank的简单计算4.4PageRank的修正计算","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"},{"name":"1-词法分析","slug":"NLP/1-词法分析","permalink":"http://yoursite.com/categories/NLP/1-词法分析/"},{"name":"4-关键词提取","slug":"NLP/1-词法分析/4-关键词提取","permalink":"http://yoursite.com/categories/NLP/1-词法分析/4-关键词提取/"}],"tags":[]},{"title":"关键词提取的分类","slug":"NLP/1-词法分析/4-关键词提取/0-关键词提取的分类","date":"2017-12-18T09:17:35.000Z","updated":"2018-08-13T00:11:22.877Z","comments":true,"path":"2017/12/18/NLP/1-词法分析/4-关键词提取/0-关键词提取的分类/","link":"","permalink":"http://yoursite.com/2017/12/18/NLP/1-词法分析/4-关键词提取/0-关键词提取的分类/","excerpt":"","text":"一、关键词抽取分类关键词抽取算法可以分为两类 1.使用外部的知识库(1).TF-IDFTF-IDF关键词提取算法就是需要保存每个词的IDF值作为外部知识库(2).LDA模型 2.不适用外部知识库(1).TextRank(2).PageRank","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"},{"name":"1-词法分析","slug":"NLP/1-词法分析","permalink":"http://yoursite.com/categories/NLP/1-词法分析/"},{"name":"4-关键词提取","slug":"NLP/1-词法分析/4-关键词提取","permalink":"http://yoursite.com/categories/NLP/1-词法分析/4-关键词提取/"}],"tags":[]},{"title":"1.朴素贝叶斯算法","slug":"NLP/7-文本分类/1.朴素贝叶斯算法","date":"2017-12-18T09:17:35.000Z","updated":"2018-09-18T10:06:14.531Z","comments":true,"path":"2017/12/18/NLP/7-文本分类/1.朴素贝叶斯算法/","link":"","permalink":"http://yoursite.com/2017/12/18/NLP/7-文本分类/1.朴素贝叶斯算法/","excerpt":"","text":"一、基本概念1.分类原理通过某对象的先验概率，利用贝叶斯公式，计算出其后验概率。即该对象属于某一类的概率，选择具有最大后验概率的类作为该对象所属的类。 2.贝叶斯公式$$P(A|B) = \\frac{P(B|A)*P(A)}{P(B)}$$ (1).其中P(A)为先验概率：先验概率（prior probability）是指根据以往经验和分析得到的概率，如全概率公式，它往往作为”由因求果”问题中的”因”出现的概率。； (2).其中P(B|A)为似然概率(likelihood)：是先前统计的事件中，A事件发生情况下B事件发生的概率 (3).其中P(B)为边界似然概率； (4).其中P(A|B)为后验概率；3.相关概念 (1).先验概率 (2).—-- ①后验概率 - ②.最大后验概率 (3).— ①.条件概率：指事件A在另外一个事件B已经发生条件下的发生概率。条件概率表示为：P(A|B)。若只有两个事件A，B，那么$P(A|B) = \\frac{P(AB)}{P(B)}$ ②.联合概率：表示两个事件共同发生的概率。A与B的联合概率表示为 P(AB) 或者P(A,B),或者P（A∩B） ③.全概率 (4).似然概率 联合概率的乘法公式为：P(AB) = P(A|B)*P(B),变形后可得到$P(A|B) = \\frac{P(AB)}{P(B)}$ 二、朴素贝叶斯分类器1.朴素贝叶斯分类的定义朴素贝叶斯分类的正式定义如下：(1).设 $x = \\left{ f{1},f{2},f{3},…,f{m} \\right}$为一个待分类项，而每个f为x的一个特征。(2).有类别集合$C = \\left{ y{1},y{2},y{3},…,y{n} \\right}$(3).计算$P(y{1}|x),P(y{2}|x),….,P(y{n}|x)$(4).如果$P(y{k}|x) = max \\left{ P(y{1}|x),P(y{2}|x),….,P(y{n}|x) \\right}，则x∈y{k}$。那么现在的关键就是如何计算第3步中的各个条件概率。我们可以这么做：(1)、找到一个已知分类的待分类项集合，这个集合叫做训练样本集。(2)、统计得到在各类别下各个特征属性的条件概率估计。即$P(x{1}|y{1}),P(x{2}|y{1}),…,P(x{3}|y{1})$;$P(x{1}|y{2}),P(x{2}|y{2}),…,P(x{3}|y{2})$;……$P(x{1}|y{3}),P(x{2}|y{3}),…,P(x{3}|y{3})$;(3)、如果各个特征属性是条件独立的，则根据贝叶斯定理有如下推导：$P(y{i}|x) = \\frac {P(x|y{i})P(y{i})}{P(x)}$ 因为分母对于所有类别为常数，因为我们只要将分子最大化皆可。又因为各特征属性是条件独立的，所以有： $P(x|y{i})P(y{i}) = P(a{1}|y{i})P(a{2}|y{i})…P(a{m}|y{i})P(y{i})=P(y(i))\\prod\\limits{j=1}^mP(a_j|yi)$ 2.朴素贝叶斯分类的举例我们知道朴素贝叶斯的公式为：$P(C|f) = \\frac{P(f|C)P(C)}{P(f)}$如果换个表达式就会明朗很多，如下：$P(类别|特征) = \\frac{P(特征|分类)P(分类)}{P(特征)}$其中P(分类)和P(特征)都是已知的，我们只需求P(特征|分类)即可。 例题分析给定数据如下：| 高否 | 富否 | 帅否 | 嫁否 || – | — | – | – || 高 | 富 | 帅 | 嫁 || 高 | 不富 | 帅 | 嫁 || 不高 | 富 | 不帅 | 嫁 || 高 | 富 | 不帅 | 不嫁 || 不高 | 不富 | 帅 | 不嫁 || 高 | 不富 | 不帅 | 不嫁 || 不高 | 不富 | 不帅 | 不嫁 |那我们现在的问题是，一个男生向一个女生求婚，这个男生具有以下三个特点：不高、富、帅，请你判断以下该女孩是否会嫁？ 这是一个典型的分类问题，转化为概率论问题就是$P(嫁|不高、富、帅)$ 与 $P(不嫁|不高、富、帅)那个概率更大？ 这里我们就使用朴素贝叶斯公式来分别求以下两种情况下的概率： ①$P(嫁|不高、富、帅) = \\frac {P(不高、富、帅|嫁)*P(嫁)}{P(不高、富、帅)}$ ②$P(不嫁|不高、富、帅) = \\frac {P(不高、富、帅|不嫁)*P(不嫁)}{P(不高、富、帅)}$对①求解我们先对要求①进行求解。要求$P(嫁|不高、富、帅)$的概率只需求$P(不高、富、帅|嫁)、P(嫁)、P(不高、富、帅)$即可。根据“朴素”一词也就是各个特征之间是独立的，可以得到如下公式和公式(只需求如下公式即可)：$P(不高、富、帅|嫁) = P(不高|嫁)P(富|嫁)P(帅|嫁)$$P(不高、富、帅) = P(不高)P(富)P(帅)$同时只需要再求出公式问题就得到解决$P(嫁)$ 我们从表格中统计所有嫁的样本共有3条，其中不高的样本有1条，所以$P(不高|嫁) = 1/3$，同理可以得到$P(富|嫁) = 2/3$， $P(帅|嫁) = 2/3$。 我们从表格中统计所有样本共有7条，其中嫁的样本有3条，所以 $P(嫁) = 3/7$。 我们从表格中统计所有样本共有7条，其中不高的样本有3条，所以 $P(不高) = 3/7$，其中富的样本有3条所以$P(富) = 3/7$，其中帅的样本有3条所以$P(帅) = 3/7$。 综上①$P(嫁|不高、富、帅) = \\frac {P(不高、富、帅|嫁)P(嫁)}{ \\; P(不高、富、帅) \\; } $ 此公式 $= \\frac {(\\frac {1}{3} \\frac {2}{3}\\frac {2}{3})\\frac {3}{7}} {\\frac {3}{7} \\frac {3}{7}\\frac {3}{7}} =\\frac {196}{243}$ 对②求解$P(不嫁|不高、富、帅) = \\frac {P(不高、富、帅|不嫁)P(不嫁)}{P(不高、富、帅)}$此公式 $= \\frac {(\\frac {1}{2} \\frac {1}{4}\\frac {1}{4})\\frac {4}{7}} {\\frac {3}{7} \\frac {3}{7}\\frac {3}{7}} =\\frac {49}{216}$ 所以最终的答案是“嫁”参考：1.https://blog.csdn.net/xueyingxue001/article/details/523961702.https://blog.csdn.net/amds123/article/details/70173402","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"},{"name":"7-文本分类","slug":"NLP/7-文本分类","permalink":"http://yoursite.com/categories/NLP/7-文本分类/"}],"tags":[]},{"title":"0-文本向量化","slug":"NLP/6-文本向量化/0-文本向量化","date":"2017-12-18T09:17:35.000Z","updated":"2018-11-13T12:45:56.226Z","comments":true,"path":"2017/12/18/NLP/6-文本向量化/0-文本向量化/","link":"","permalink":"http://yoursite.com/2017/12/18/NLP/6-文本向量化/0-文本向量化/","excerpt":"","text":"一、简介二、分类1.one-hot(将词向量化)这最简单的一种想法，就是对一句话用one-hot编码:比如对于这句话：1234ohn likes to watch movies,Mary likes too.John likes to watch movies,Mary likes too.John also likes to watch football games.John also likes to watch football games. 可以通过以上语料构建一个字典：dic123456789101:&quot;John&quot;2:&quot;likes&quot;3:&quot;to&quot;4:&quot;watch&quot;5:&quot;movies&quot;6:&quot;also&quot;7:&quot;football&quot;8:&quot;games&quot;9:&quot;Mary&quot;10:&quot;too&quot; 用one-hot可以表示为：第一句话：John :[1,0,0,0,0,0,0,0,0,0]likes :[0,1,0,0,0,0,0,0,0,0]…too :[0,0,0,0,0,0,0,0,0,1] 第二句话：John :[1,0,0,0,0,0,0,0,0,0]likes :[0,1,0,0,0,0,0,0,0,0]…too :[0,0,0,0,0,0,0,0,0,1]但事实是，这样做耗费的资源太多，而且不能很好的表征每句话的特性。 2.Bag-of-words(将句子向量化)另一种方法则是词袋模型，它相当于一个词袋，不考虑词/句之间的相关性，只要出现了该词，就会记为1，再次出现就会+1。比如前面的那句话：1John likes to watch movies,Mary likes too. 可以表示为[1,2,1,1,1,0,0,0,1,1] 与其相似的是binary weighting,它就是看一下每个词是否出现，出现记为1，没出现记为0[1,1,1,1,1,0,0,0,1,1] 3.TF-IDF(将句子向量化：考虑词的重要性)因此，就有tf-idf解决这个问题，它的主要思路就是有两方面：A—第一就是如果这个词在我们当前文档出现的频率非常高，说明它在当前文档应该是比较重要的。B-但如果它在所有的文档中出现的频次都非常好，大家可能就会觉得这个词应该不是那么重要的。 比如中文的“的“，或者我们上面那两个句子中的to.因此，tf-idf就是一个在当前文档和所有文档中权衡他们的重要性，然后计算出每个词的重要度的方法。 4.word2vec(词的维度固定)","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"},{"name":"6-文本向量化","slug":"NLP/6-文本向量化","permalink":"http://yoursite.com/categories/NLP/6-文本向量化/"}],"tags":[]},{"title":"这里是句法分析","slug":"NLP/2-句法分析/这里是句法分析","date":"2017-12-18T09:17:35.000Z","updated":"2018-08-12T08:48:45.815Z","comments":true,"path":"2017/12/18/NLP/2-句法分析/这里是句法分析/","link":"","permalink":"http://yoursite.com/2017/12/18/NLP/2-句法分析/这里是句法分析/","excerpt":"","text":"","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"},{"name":"2-句法分析","slug":"NLP/2-句法分析","permalink":"http://yoursite.com/categories/NLP/2-句法分析/"}],"tags":[]},{"title":"word2ve源码解析(python版gensim实现)","slug":"NLP/6-文本向量化/2.1-word2ve源码解析(python版gensim实现)","date":"2017-10-12T04:18:15.000Z","updated":"2018-11-27T03:13:23.258Z","comments":true,"path":"2017/10/12/NLP/6-文本向量化/2.1-word2ve源码解析(python版gensim实现)/","link":"","permalink":"http://yoursite.com/2017/10/12/NLP/6-文本向量化/2.1-word2ve源码解析(python版gensim实现)/","excerpt":"","text":"————————–base_any2vec.py——————- class BaseAny2VecModel() def init(): def _worker_loop(): def _job_producer(): def _log_epoch_progress(): def _train_epoch(): #在这里才是真正的训练 def train(): for cur_epoch in range(self.epochs): self._train_epoch() def load(): return super(BaseAny2VecModel, cls).load() def save(): super(BaseAny2VecModel, self).save() class BaseWordEmbeddingsModel(BaseAny2VecModel) def __init__(): super(BaseWordEmbeddingsModel, self).__init__() #开始建立词典1 self.build_vocab(sentences) #开始训练 self.train() def build_vocab(): total_words, corpus_count = self.vocabulary.scan_vocab() #开始构建词典2 report_values = self.vocabulary.prepare_vocab() #开始构建网络模型 self.trainables.prepare_weights() def build_vocab_from_freq(): def estimate_memory(): def train(): return super(BaseWordEmbeddingsModel, self).train() def load(): def similarity(): def most_similar(): ##———————–word2vec.py———————– #word2vec 共有三个类：Word2Vec 、Word2VecVocab、Word2VecTrainablesdef train_batch_sg()def train_batch_cbow()def score_sentence_sg()def score_sentence_cbow()def train_sg_pair()def train_cbow_pair()def score_sg_pair()def score_cbow_pair() class Word2Vec(BaseWordEmbeddingsModel) def __init__(): #wv: 是类 ~gensim.models.keyedvectors.Word2VecKeyedVectors生产的对象，在word2vec是一个属性 为了在不同的训练算法（Word2Vec，Fastext，WordRank，VarEmbed）之间共享单词向量查询代码，gensim将单词向量的存储和查询分离为一个单独的类 KeyedVectors包含单词和对应向量的映射。可以通过它进行词向量的查询 #This object essentially contains the mapping between words and embeddings. self.wv = Word2VecKeyedVectors(size) #词汇表 self.vocabulary = Word2VecVocab() #网络模型初始化 self.trainables = Word2VecTrainables() super(Word2Vec, self).__init__() def _do_train_job(): def _clear_post_train(self): def _set_train_params(): def train(): return super(Word2Vec, self).train() def score(): #非主要 def clear_sims(self): #非主要 def intersect_word2vec_format(): def predict_output_word(): def init_sims(): def reset_from(): #非主要 def log_accuracy() #非主要 def accuracy(): def delete_temporary_training_data(): #保存模型 def save(): def get_latest_training_loss(): def load_word2vec_format(): def save_word2vec_format(): #加载模型 def load(): #词库的构建：huffman树的构建class Word2VecVocab() def init(): #该方法是从句子序列中构建单词表 def scan_vocab(): def sort_vocab(): #开始构建词典3 def prepare_vocab(): #如果是层次softmax构建霍夫曼树 if hs: self.create_binary_tree(wv) #如果是负采样 if negative: self.make_cum_table(wv) #使用存储的词汇单词及其词频创建一个二叉霍夫曼树 def create_binary_tree(): def add_null_word(): def make_cum_table(): #训练词向量的内部浅层神经网络class Word2VecTrainables(): def init(): def prepare_weights(): def seeded_vector(): def reset_weights(): def update_weights(): 参考资料：[1]https://blog.csdn.net/zynash2/article/details/81636338[2]https://blog.csdn.net/u012416045/article/details/78237060[3]https://blog.csdn.net/google19890102/article/details/51887344","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"},{"name":"6-文本向量化","slug":"NLP/6-文本向量化","permalink":"http://yoursite.com/categories/NLP/6-文本向量化/"}],"tags":[]},{"title":"0-word2vec","slug":"NLP/6-文本向量化/1-NNLP(神经网络模型)训练的词向量","date":"2017-10-12T04:18:15.000Z","updated":"2018-11-13T12:50:14.912Z","comments":true,"path":"2017/10/12/NLP/6-文本向量化/1-NNLP(神经网络模型)训练的词向量/","link":"","permalink":"http://yoursite.com/2017/10/12/NLP/6-文本向量化/1-NNLP(神经网络模型)训练的词向量/","excerpt":"","text":"","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"},{"name":"6-文本向量化","slug":"NLP/6-文本向量化","permalink":"http://yoursite.com/categories/NLP/6-文本向量化/"}],"tags":[]},{"title":"0-word2vec","slug":"NLP/6-文本向量化/2.0-word2vec","date":"2017-10-12T04:18:15.000Z","updated":"2018-11-13T12:50:14.912Z","comments":true,"path":"2017/10/12/NLP/6-文本向量化/2.0-word2vec/","link":"","permalink":"http://yoursite.com/2017/10/12/NLP/6-文本向量化/2.0-word2vec/","excerpt":"","text":"","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"},{"name":"6-文本向量化","slug":"NLP/6-文本向量化","permalink":"http://yoursite.com/categories/NLP/6-文本向量化/"}],"tags":[]},{"title":"3-层次聚类法之CANOPY","slug":"NLP/8-文本聚类/3-层次聚类法之CANOPY","date":"2017-05-18T09:17:35.000Z","updated":"2018-09-25T10:45:58.395Z","comments":true,"path":"2017/05/18/NLP/8-文本聚类/3-层次聚类法之CANOPY/","link":"","permalink":"http://yoursite.com/2017/05/18/NLP/8-文本聚类/3-层次聚类法之CANOPY/","excerpt":"","text":"","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"},{"name":"8-文本聚类","slug":"NLP/8-文本聚类","permalink":"http://yoursite.com/categories/NLP/8-文本聚类/"}],"tags":[]},{"title":"2-密度聚类法之DBSCAN","slug":"NLP/8-文本聚类/2-密度聚类法之DBSCAN","date":"2017-03-18T09:17:35.000Z","updated":"2018-09-25T10:45:53.121Z","comments":true,"path":"2017/03/18/NLP/8-文本聚类/2-密度聚类法之DBSCAN/","link":"","permalink":"http://yoursite.com/2017/03/18/NLP/8-文本聚类/2-密度聚类法之DBSCAN/","excerpt":"","text":"","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"},{"name":"8-文本聚类","slug":"NLP/8-文本聚类","permalink":"http://yoursite.com/categories/NLP/8-文本聚类/"}],"tags":[]},{"title":"人工智能机器学习NLP算法分类总结","slug":"人工智能机器学习NLP算法分类总结","date":"2017-03-10T02:27:01.000Z","updated":"2018-09-18T09:49:24.191Z","comments":true,"path":"2017/03/10/人工智能机器学习NLP算法分类总结/","link":"","permalink":"http://yoursite.com/2017/03/10/人工智能机器学习NLP算法分类总结/","excerpt":"","text":"一、人工智能学习算法分类 人工智能算法大体上来说可以分类两类：基于统计的机器学习算法(Machine Learning)和深度学习算法(Deep Learning) 总的来说，在sklearn中机器学习算法大概的分类如下：1. 纯算法类 (1).回归算法 (2).分类算法 (3).聚类算法 (4)降维算法 (5)概率图模型算法 (6)文本挖掘算法 (7)优化算法 (8)深度学习算法2.建模方面 (1).模型优化 (2).数据预处理 二、详细算法1.分类算法 (1).LR (Logistic Regression，逻辑回归又叫逻辑分类) (2).SVM (Support Vector Machine，支持向量机) (3).NB (Naive Bayes，朴素贝叶斯) (4).DT (Decision Tree，决策树) 1).C4.5 2).ID3 3).CART (5).集成算法 1).Bagging 2).Random Forest (随机森林) 3).GB(梯度提升,Gradient boosting) 4).GBDT (Gradient Boosting Decision Tree) 5).AdaBoost 6).Xgboost (6).最大熵模型 2.回归算法 (1).LR (Linear Regression，线性回归) (2).SVR (支持向量机回归) (3). RR (Ridge Regression，岭回归)3.聚类算法 (1).Knn (2).Kmeans 算法 (3).层次聚类 (4).密度聚类4.降维算法 (1).SGD (随机梯度下降) (2). 5.概率图模型算法 (1).贝叶斯网络 (2).HMM (3).CRF (条件随机场)6.文本挖掘算法(1).模型 1).LDA (主题生成模型，Latent Dirichlet Allocation) 4).最大熵模型(2).关键词提取 1).tf-idf 2).bm25 3).textrank 4).pagerank 5).左右熵 :左右熵高的作为关键词 6).互信息： (3).词法分析 1).分词 ①HMM (因马尔科夫) ②CRF (条件随机场) 2).词性标注 3).命名实体识别(4).句法分析 1).句法结构分析 2).依存句法分析(5).文本向量化 1).tf-idf 2).word2vec 3).doc2vec 4).cw2vec(6).距离计算 1).欧氏距离 2).相似度计算 7.优化算法 (1).正则化 1).L1正则化 2).L2正则化8.深度学习算法 (1).BP (2).CNN (3).DNN (3).RNN (4).LSTM 三、建模方面1.模型优化· (1).特征选择 (2).梯度下降 (3).交叉验证 (4).参数调优 (5).模型评估：准确率、召回率、F1、AUC、ROC、损失函数 2.数据预处理 (1).标准化 (2).异常值处理 (3).二值化 (4).缺失值填充： 支持均值、中位数、特定值补差、多重插补","categories":[],"tags":[]},{"title":"1-划分聚类法之k-means","slug":"NLP/8-文本聚类/1-划分聚类法之k-means","date":"2017-01-18T09:17:35.000Z","updated":"2018-09-25T10:45:46.632Z","comments":true,"path":"2017/01/18/NLP/8-文本聚类/1-划分聚类法之k-means/","link":"","permalink":"http://yoursite.com/2017/01/18/NLP/8-文本聚类/1-划分聚类法之k-means/","excerpt":"","text":"","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"},{"name":"8-文本聚类","slug":"NLP/8-文本聚类","permalink":"http://yoursite.com/categories/NLP/8-文本聚类/"}],"tags":[]},{"title":"感知机算法","slug":"DeepLearning/Perceptron/Perceptron","date":"2017-01-09T08:02:27.000Z","updated":"2018-09-20T06:11:58.139Z","comments":true,"path":"2017/01/09/DeepLearning/Perceptron/Perceptron/","link":"","permalink":"http://yoursite.com/2017/01/09/DeepLearning/Perceptron/Perceptron/","excerpt":"","text":"二、学习资料 Michael Nielsen的《Neural Network and Deep Learning》中文翻译《神经网络与深度学习》","categories":[{"name":"DeepLearning","slug":"DeepLearning","permalink":"http://yoursite.com/categories/DeepLearning/"},{"name":"Perceptron","slug":"DeepLearning/Perceptron","permalink":"http://yoursite.com/categories/DeepLearning/Perceptron/"}],"tags":[]},{"title":"deeplearning算法分类","slug":"DeepLearning/deeplearning","date":"2017-01-09T08:02:27.000Z","updated":"2018-09-20T06:11:12.756Z","comments":true,"path":"2017/01/09/DeepLearning/deeplearning/","link":"","permalink":"http://yoursite.com/2017/01/09/DeepLearning/deeplearning/","excerpt":"","text":"一、常用神经网络分类1.Perceptron(感知机,神经网络的前身)2.前向传播算法3.BP(反向传播算法)4.DNN(深度神经网络)5.CNN(卷积神经网络)6.RNN(循环神经网络)7.LSTM(..神经网络)二、学习资料 Michael Nielsen的《Neural Network and Deep Learning》中文翻译《神经网络与深度学习》","categories":[{"name":"DeepLearning","slug":"DeepLearning","permalink":"http://yoursite.com/categories/DeepLearning/"}],"tags":[]},{"title":"CNN算法","slug":"DeepLearning/CNN/CNN","date":"2017-01-09T08:02:27.000Z","updated":"2018-09-20T06:12:27.399Z","comments":true,"path":"2017/01/09/DeepLearning/CNN/CNN/","link":"","permalink":"http://yoursite.com/2017/01/09/DeepLearning/CNN/CNN/","excerpt":"","text":"二、学习资料 Michael Nielsen的《Neural Network and Deep Learning》中文翻译《神经网络与深度学习》","categories":[{"name":"DeepLearning","slug":"DeepLearning","permalink":"http://yoursite.com/categories/DeepLearning/"},{"name":"CNN","slug":"DeepLearning/CNN","permalink":"http://yoursite.com/categories/DeepLearning/CNN/"}],"tags":[]},{"title":"CNN算法","slug":"DeepLearning/CNN/CNN算法之文本分类","date":"2017-01-09T08:02:27.000Z","updated":"2018-09-20T06:12:27.399Z","comments":true,"path":"2017/01/09/DeepLearning/CNN/CNN算法之文本分类/","link":"","permalink":"http://yoursite.com/2017/01/09/DeepLearning/CNN/CNN算法之文本分类/","excerpt":"","text":"二、学习资料 Michael Nielsen的《Neural Network and Deep Learning》中文翻译《神经网络与深度学习》","categories":[{"name":"DeepLearning","slug":"DeepLearning","permalink":"http://yoursite.com/categories/DeepLearning/"},{"name":"CNN","slug":"DeepLearning/CNN","permalink":"http://yoursite.com/categories/DeepLearning/CNN/"}],"tags":[]},{"title":"RNN算法","slug":"DeepLearning/RNN/RNN","date":"2017-01-09T08:02:27.000Z","updated":"2018-09-29T06:30:25.521Z","comments":true,"path":"2017/01/09/DeepLearning/RNN/RNN/","link":"","permalink":"http://yoursite.com/2017/01/09/DeepLearning/RNN/RNN/","excerpt":"","text":"","categories":[{"name":"DeepLearning","slug":"DeepLearning","permalink":"http://yoursite.com/categories/DeepLearning/"},{"name":"RNN","slug":"DeepLearning/RNN","permalink":"http://yoursite.com/categories/DeepLearning/RNN/"}],"tags":[]},{"title":"1-联合概率","slug":"数学/统计学/1-联合概率","date":"2016-12-18T09:17:35.000Z","updated":"2018-11-01T08:49:14.375Z","comments":true,"path":"2016/12/18/数学/统计学/1-联合概率/","link":"","permalink":"http://yoursite.com/2016/12/18/数学/统计学/1-联合概率/","excerpt":"","text":"一、定义联合概率是指在多元的概率分布中多个随机变量分别满足各自条件的概率。假设X和Y都服从正态分布，那么P{X&lt;4,Y&lt;0}就是一个联合概率，表示X&lt;4,Y&lt;0两个条件同时成立的概率。表示两个事件共同发生的概率。A与B的联合概率表示为 P(AB) 或者P(A,B),或者P（A∩B）。 二、公式$P(AB) = P(A)*P(B|A)$ $P(AB) = P(B)*P(A|B)$以上公式还可以变形为： $P(B|A) = \\frac {P(A|B)*P(B)}{P(A)}$ $P(A|B) = \\frac {P(B|A)*P(A)}{P(B)}$","categories":[{"name":"数学","slug":"数学","permalink":"http://yoursite.com/categories/数学/"},{"name":"统计学","slug":"数学/统计学","permalink":"http://yoursite.com/categories/数学/统计学/"}],"tags":[]},{"title":"0-条件概率","slug":"数学/统计学/0-条件概率","date":"2016-12-18T09:17:35.000Z","updated":"2018-10-08T06:07:42.683Z","comments":true,"path":"2016/12/18/数学/统计学/0-条件概率/","link":"","permalink":"http://yoursite.com/2016/12/18/数学/统计学/0-条件概率/","excerpt":"","text":"一、定义条件概率是指事件A在另外一个事件B已经发生条件下的发生概率。条件概率表示为：P（A|B），读作“在B的条件下A的概率”。若只有两个事件A，B，那么， $P(A|B) = \\frac {P(AB)}{P(B)}$。 二、公式$P(A|B) = \\frac {P(AB)}{P(B)}$根据贝叶斯公式条件概率还可以是$P(A|B) = \\frac {P(B|A)*P(A)}{P(B)}$","categories":[{"name":"数学","slug":"数学","permalink":"http://yoursite.com/categories/数学/"},{"name":"统计学","slug":"数学/统计学","permalink":"http://yoursite.com/categories/数学/统计学/"}],"tags":[]},{"title":"0-文本聚类算法总结","slug":"NLP/8-文本聚类/0-文本聚类算法总结","date":"2016-12-18T09:17:35.000Z","updated":"2018-09-25T10:45:35.496Z","comments":true,"path":"2016/12/18/NLP/8-文本聚类/0-文本聚类算法总结/","link":"","permalink":"http://yoursite.com/2016/12/18/NLP/8-文本聚类/0-文本聚类算法总结/","excerpt":"","text":"一、文本聚类算法总结1.划分法(partitioning methods)：给定一个有N个元组或者纪录的数据集，分裂法将构造K个分组，每一个分组就代表一个聚类，K&lt;N。而且这K个分组满足下列条件：（1） 每一个分组至少包含一个数据纪录；（2）每一个数据纪录属于且仅属于一个分组（注意：这个要求在某些模糊聚类算法中可以放宽）；对于给定的K，算法首先给出一个初始的分组方法，以后通过反复迭代的方法改变分组，使得每一次改进之后的分组方案都较前一次好，而所谓好的标准就是：同一分组中的记录越近越好，而不同分组中的纪录越远越好。使用这个基本思想的算法有：K-MEANS算法、K-MEDOIDS算法、CLARANS算法； 2.层次法2.1层次发的总结(hierarchical methods)：这种方法对给定的数据集进行层次似的分解，直到某种条件满足为止。具体又可分为“自底向上”和“自顶向下”两种方案。例如在“自底向上”方案中，初始时每一个数据纪录都组成一个单独的组，在接下来的迭代中，它把那些相互邻近的组合并成一个组，直到所有的记录组成一个分组或者某个条件满足为止。代表算法有：CANOPY算法、BIRCH算法、CURE算法、CHAMELEON算法等； 2.2层次法的优点(1).距离和规则的相似度容易定义，限制少(2).不需要预先定制聚类书(3).可以发现类的层次关系(4).可以聚类成其他形状 2.3层次法的缺点(1).计算复杂度太高(2).奇异值也能产生很大影响(3).算法可能聚成链状 3.基于密度的方法（density-based methods):基于密度的方法与其它方法的一个根本区别是：它不是基于各种各样的距离的，而是基于密度的。这样就能克服基于距离的算法只能发现“类圆形”的聚类的缺点。这个方法的指导思想就是，只要一个区域中的点的密度大过某个阀值，就把它加到与之相近的聚类中去。代表算法有：DBSCAN算法、OPTICS算法、DENCLUE算法等； 4.基于网格的方法(grid-based methods):这种方法首先将数据空间划分成为有限个单元（cell）的网格结构,所有的处理都是以单个的单元为对象的。这么处理的一个突出的优点就是处理速度很快，通常这是与目标数据库中记录的个数无关的，它只与把数据空间分为多少个单元有关。代表算法有：STING算法、CLIQUE算法、WAVE-CLUSTER算法； 5.基于模型的方法(model-based methods):基于模型的方法给每一个聚类假定一个模型，然后去寻找一个能很好的满足这个模型的数据集。这样一个模型可能是数据点在空间中的密度分布函数或者其它。它的一个潜在的假定就是：目标数据集是由一系列的概率分布所决定的。通常有两种尝试方向：统计的方案和神经网络的方案 参考：[1].https://wenku.baidu.com/view/8713cffc581b6bd97e19eaea.html","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"},{"name":"8-文本聚类","slug":"NLP/8-文本聚类","permalink":"http://yoursite.com/categories/NLP/8-文本聚类/"}],"tags":[]},{"title":"0-文本特征提取算法总结","slug":"NLP/9-特征提取/0-文本特征提取算法总结","date":"2016-01-18T05:17:35.000Z","updated":"2018-10-08T06:09:59.092Z","comments":true,"path":"2016/01/18/NLP/9-特征提取/0-文本特征提取算法总结/","link":"","permalink":"http://yoursite.com/2016/01/18/NLP/9-特征提取/0-文本特征提取算法总结/","excerpt":"","text":"为分类文本作处理的特征提取算法也对最终效果有巨大影响，而特征提取算法又分为特征选择和特征抽取两大类，其中特征选择算法有互信息，文档频率，信息增益，开方检验等等十数种","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"},{"name":"9-特征提取","slug":"NLP/9-特征提取","permalink":"http://yoursite.com/categories/NLP/9-特征提取/"}],"tags":[]},{"title":"1-TF-IDF算法","slug":"NLP/9-特征提取/1-TF-IDF","date":"2016-01-18T05:17:35.000Z","updated":"2018-10-08T06:10:07.347Z","comments":true,"path":"2016/01/18/NLP/9-特征提取/1-TF-IDF/","link":"","permalink":"http://yoursite.com/2016/01/18/NLP/9-特征提取/1-TF-IDF/","excerpt":"","text":"TF-IDF算法可以分解为两部分：TF和IDF 一、TF算法1.词频（term frequency，TF）2.公式：$TF =\\frac {N{i,j}}{ \\sum{k}N_{k,j}}$3.解释：以上式子中分子是该词在文件中的出现次数，而分母则是在文件中所有字词的出现次数之和。 二、IDF算法1.逆向文件频率（inverse document frequency，IDF）2.公式：$IDF =lg \\frac {|D|}{|j:t_i ∈d_j|}$3.解释：逆向文件频率（inverse document frequency，IDF）是一个词语普遍重要性的度量。某一特定词语的IDF，可以由总文件数目除以包含该词语之文件的数目，再将得到的商取以10为底的对数得到","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"},{"name":"9-特征提取","slug":"NLP/9-特征提取","permalink":"http://yoursite.com/categories/NLP/9-特征提取/"}],"tags":[]}]}