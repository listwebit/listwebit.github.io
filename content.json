{"meta":{"title":"帅的掉渣的博客","subtitle":null,"description":null,"author":"listwebit","url":"http://yoursite.com"},"pages":[{"title":"深度学习","date":"2018-08-11T00:36:47.000Z","updated":"2018-08-11T00:36:47.751Z","comments":true,"path":"深度学习/index.html","permalink":"http://yoursite.com/深度学习/index.html","excerpt":"","text":""},{"title":"'about'","date":"2018-08-10T06:10:04.000Z","updated":"2018-08-10T06:10:38.230Z","comments":true,"path":"about/index.html","permalink":"http://yoursite.com/about/index.html","excerpt":"","text":"这里是关于页面"},{"title":"about","date":"2018-08-10T06:22:54.000Z","updated":"2018-08-10T06:22:54.035Z","comments":true,"path":"about/index-1.html","permalink":"http://yoursite.com/about/index-1.html","excerpt":"","text":""}],"posts":[{"title":"文章观点抽取.md","slug":"NLP/3-短语分析/观点抽取/文章观点抽取/文章观点抽取","date":"2018-05-18T05:17:35.000Z","updated":"2018-09-26T12:13:05.016Z","comments":true,"path":"2018/05/18/NLP/3-短语分析/观点抽取/文章观点抽取/文章观点抽取/","link":"","permalink":"http://yoursite.com/2018/05/18/NLP/3-短语分析/观点抽取/文章观点抽取/文章观点抽取/","excerpt":"","text":"","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"},{"name":"3-短语分析","slug":"NLP/3-短语分析","permalink":"http://yoursite.com/categories/NLP/3-短语分析/"},{"name":"观点抽取","slug":"NLP/3-短语分析/观点抽取","permalink":"http://yoursite.com/categories/NLP/3-短语分析/观点抽取/"},{"name":"文章观点抽取","slug":"NLP/3-短语分析/观点抽取/文章观点抽取","permalink":"http://yoursite.com/categories/NLP/3-短语分析/观点抽取/文章观点抽取/"}],"tags":[]},{"title":"评论观点抽取","slug":"NLP/3-短语分析/观点抽取/评论观点抽取/评论观点抽取","date":"2018-05-16T05:17:35.000Z","updated":"2018-09-26T12:12:36.848Z","comments":true,"path":"2018/05/16/NLP/3-短语分析/观点抽取/评论观点抽取/评论观点抽取/","link":"","permalink":"http://yoursite.com/2018/05/16/NLP/3-短语分析/观点抽取/评论观点抽取/评论观点抽取/","excerpt":"","text":"","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"},{"name":"3-短语分析","slug":"NLP/3-短语分析","permalink":"http://yoursite.com/categories/NLP/3-短语分析/"},{"name":"观点抽取","slug":"NLP/3-短语分析/观点抽取","permalink":"http://yoursite.com/categories/NLP/3-短语分析/观点抽取/"},{"name":"评论观点抽取","slug":"NLP/3-短语分析/观点抽取/评论观点抽取","permalink":"http://yoursite.com/categories/NLP/3-短语分析/观点抽取/评论观点抽取/"}],"tags":[]},{"title":"观点抽取分类","slug":"NLP/3-短语分析/观点抽取/观点抽取分类","date":"2018-05-16T05:17:35.000Z","updated":"2018-09-26T12:11:59.135Z","comments":true,"path":"2018/05/16/NLP/3-短语分析/观点抽取/观点抽取分类/","link":"","permalink":"http://yoursite.com/2018/05/16/NLP/3-短语分析/观点抽取/观点抽取分类/","excerpt":"","text":"一、观点抽取的分类我认为按照观点抽取的对象可以分为两类，一种是对文章进行观点抽取，得到文章的类似摘要、分类、关键词等性质的句子、短语或者关键词。一种是对评论进行观点抽取，得到一个短语，然后在对短语进行分类。 1.文章观点抽取文章的观点抽取结果可以是一句话（类似标题），一段话（类似摘要），一个短语，一个关键词（打标签，其实是文本分类）。 2.评论观点抽取评论的观点抽取结果一般是一个短语。类似淘宝评论的观点抽取。例如可以将某一件衣服评论观点分为：质量好，超划算，性价比高，穿上好看。","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"},{"name":"3-短语分析","slug":"NLP/3-短语分析","permalink":"http://yoursite.com/categories/NLP/3-短语分析/"},{"name":"观点抽取","slug":"NLP/3-短语分析/观点抽取","permalink":"http://yoursite.com/categories/NLP/3-短语分析/观点抽取/"}],"tags":[]},{"title":"回归的分类","slug":"MachineLearning/回归的分类","date":"2018-01-09T08:02:27.000Z","updated":"2018-01-09T08:31:26.000Z","comments":true,"path":"2018/01/09/MachineLearning/回归的分类/","link":"","permalink":"http://yoursite.com/2018/01/09/MachineLearning/回归的分类/","excerpt":"一、回归可以分为以下几类 1.线性回归 2.非线性回归 3.逻辑回归","text":"一、回归可以分为以下几类 1.线性回归 2.非线性回归 3.逻辑回归 二、回归的概念1.线性回归可以简单理解为线性就是每个变量的指数都是1。 2.非线性回归而非线性就是至少有一个变量的指数不是1。 3.逻辑回归回归和分类的区别分类和回归的区别在于输出变量的类型。 定量输出称为回归，或者说是连续变量预测；定性输出称为分类，或者说是离散变量预测。 举个例子：预测明天的气温是多少度，这是一个回归任务；预测明天是阴、晴还是雨，就是一个分类任务。","categories":[{"name":"MachineLearning","slug":"MachineLearning","permalink":"http://yoursite.com/categories/MachineLearning/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/机器学习/"}]},{"title":"回归","slug":"MachineLearning/回归","date":"2018-01-09T08:02:27.000Z","updated":"2018-01-09T08:02:27.288Z","comments":true,"path":"2018/01/09/MachineLearning/回归/","link":"","permalink":"http://yoursite.com/2018/01/09/MachineLearning/回归/","excerpt":"","text":"","categories":[{"name":"MachineLearning","slug":"MachineLearning","permalink":"http://yoursite.com/categories/MachineLearning/"}],"tags":[]},{"title":"2017年总结","slug":"总结/2017年总结","date":"2018-01-09T06:55:28.000Z","updated":"2018-08-11T01:06:32.802Z","comments":true,"path":"2018/01/09/总结/2017年总结/","link":"","permalink":"http://yoursite.com/2018/01/09/总结/2017年总结/","excerpt":"","text":"","categories":[{"name":"总结","slug":"总结","permalink":"http://yoursite.com/categories/总结/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/tags/深度学习/"}]},{"title":"2017年","slug":"总结/2017年","date":"2018-01-09T06:55:03.000Z","updated":"2018-01-09T06:55:03.415Z","comments":true,"path":"2018/01/09/总结/2017年/","link":"","permalink":"http://yoursite.com/2018/01/09/总结/2017年/","excerpt":"","text":"","categories":[{"name":"总结","slug":"总结","permalink":"http://yoursite.com/categories/总结/"}],"tags":[]},{"title":"这里是词法分析","slug":"NLP/1-词法分析/这里是词法分析","date":"2017-12-18T09:17:35.000Z","updated":"2018-08-12T08:48:26.854Z","comments":true,"path":"2017/12/18/NLP/1-词法分析/这里是词法分析/","link":"","permalink":"http://yoursite.com/2017/12/18/NLP/1-词法分析/这里是词法分析/","excerpt":"","text":"","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"},{"name":"1-词法分析","slug":"NLP/1-词法分析","permalink":"http://yoursite.com/categories/NLP/1-词法分析/"}],"tags":[]},{"title":"这里是句法分析","slug":"NLP/2-句法分析/这里是句法分析","date":"2017-12-18T09:17:35.000Z","updated":"2018-08-12T08:48:45.815Z","comments":true,"path":"2017/12/18/NLP/2-句法分析/这里是句法分析/","link":"","permalink":"http://yoursite.com/2017/12/18/NLP/2-句法分析/这里是句法分析/","excerpt":"","text":"","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"},{"name":"2-句法分析","slug":"NLP/2-句法分析","permalink":"http://yoursite.com/categories/NLP/2-句法分析/"}],"tags":[]},{"title":"关键词提取的分类","slug":"NLP/1-词法分析/4-关键词提取/0.关键词提取的分类","date":"2017-12-18T09:17:35.000Z","updated":"2018-08-13T00:11:22.877Z","comments":true,"path":"2017/12/18/NLP/1-词法分析/4-关键词提取/0.关键词提取的分类/","link":"","permalink":"http://yoursite.com/2017/12/18/NLP/1-词法分析/4-关键词提取/0.关键词提取的分类/","excerpt":"","text":"一、关键词抽取分类关键词抽取算法可以分为两类 1.使用外部的知识库(1).TF-IDFTF-IDF关键词提取算法就是需要保存每个词的IDF值作为外部知识库(2).LDA模型 2.不适用外部知识库(1).TextRank(2).PageRank","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"},{"name":"1-词法分析","slug":"NLP/1-词法分析","permalink":"http://yoursite.com/categories/NLP/1-词法分析/"},{"name":"4-关键词提取","slug":"NLP/1-词法分析/4-关键词提取","permalink":"http://yoursite.com/categories/NLP/1-词法分析/4-关键词提取/"}],"tags":[]},{"title":"PageRank算法的前世今生","slug":"NLP/1-词法分析/4-关键词提取/1.PageRank的前世今生","date":"2017-12-18T09:17:35.000Z","updated":"2018-08-13T00:08:23.140Z","comments":true,"path":"2017/12/18/NLP/1-词法分析/4-关键词提取/1.PageRank的前世今生/","link":"","permalink":"http://yoursite.com/2017/12/18/NLP/1-词法分析/4-关键词提取/1.PageRank的前世今生/","excerpt":"","text":"一、算法定义PageRank，网页排名，又称网页级别、Google左侧排名或佩奇排名，是一种由 [1] 根据网页之间相互的超链接计算的技术，而作为网页排名的要素之一，以Google公司创办人拉里·佩奇（Larry Page）之姓来命名。Google用它来体现网页的相关性和重要性，在搜索引擎优化操作中是经常被用来评估网页优化的成效因素之一。Google的创始人拉里·佩奇和谢尔盖·布林于1998年在斯坦福大学发明了这项技术。 PageRank通过网络浩瀚的超链接关系来确定一个页面的等级。Google把从A页面到B页面的链接解释为A页面给B页面投票，Google根据投票来源（甚至来源的来源，即链接到A页面的页面）和投票目标的等级来决定新的等级。简单的说，一个高等级的页面可以使其他低等级页面的等级提升。 二、算法来源这个要从搜索引擎的发展讲起。最早的搜索引擎采用的是 分类目录[^ref_1] 的方法，即通过人工进行网页分类并整理出高质量的网站。那时 Yahoo 和国内的 hao123 就是使用的这种方法。 后来网页越来越多，人工分类已经不现实了。搜索引擎进入了 文本检索 的时代，即计算用户查询关键词与网页内容的相关程度来返回搜索结果。这种方法突破了数量的限制，但是搜索结果不是很好。因为总有某些网页来回地倒腾某些关键词使自己的搜索排名靠前。 于是我们的主角要登场了。没错，谷歌的两位创始人，当时还是美国斯坦福大学 (Stanford University) 研究生的佩奇 (Larry Page) 和布林 (Sergey Brin) 开始了对网页排序问题的研究。他们的借鉴了学术界评判学术论文重要性的通用方法， 那就是看论文的引用次数。由此想到网页的重要性也可以根据这种方法来评价。于是PageRank的核心思想就诞生了[^ref_2]，非常简单：121.如果一个网页被很多其他网页链接到的话说明这个网页比较重要，也就是PageRank值会相对较高2.如果一个PageRank值很高的网页链接到一个其他的网页，那么被链接到的网页的PageRank值会相应地因此而提高 三、搜索引擎搜索引擎的功能主要是：根据用户输入的关键字，返回文档的链接结果。搜索引擎主要解决的三大问题：(1)如何获取文档资料 (2)如何根据关键词检索到相关文档 (3)如何对文档进行排序，返回给用户满意的页面。 1.获取文档资料利用爬虫程序，获取互联网的页面资料。爬虫技术先从一个网页出发，将该网页的内容记录下来，保存到资料库，接着分析页面中的超链接，分别递归得去获取超链接页面。 2.如何根据关键词检索到相关文档采用倒排索引算法。简单的说，倒排索引是一对key-value对，key代表关键词，value代表拥有这些关键词的文档编号或者url。 3.文档排序这是搜索引擎最核心的问题，也是google发家致富的法宝 – PageRank算法。 四、算法原理4.1PageRank的两个假设 数量假设：1.如果一个网页被很多其他网页链接到的话说明这个网页比较重要，也就是PageRank值会相对较高。质量假设：2.如果一个PageRank值很高的网页链接到一个其他的网页，那么被链接到的网页的PageRank值会相应地因此而提高。 4.2PageRank的算法原理PageRank算法总的来说就是预先给每个网页一个PR值（下面用PR值指代PageRank值），由于PR值物理意义上为一个网页被访问概率，所以一般是1/N，其中N为网页总数。预先给定PR值后，通过下面的算法不断迭代，直至达到平稳分布为止。 PageRank值主要是的是节点的入链值。 4.3PageRank的简单计算4.4PageRank的修正计算","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"},{"name":"1-词法分析","slug":"NLP/1-词法分析","permalink":"http://yoursite.com/categories/NLP/1-词法分析/"},{"name":"4-关键词提取","slug":"NLP/1-词法分析/4-关键词提取","permalink":"http://yoursite.com/categories/NLP/1-词法分析/4-关键词提取/"}],"tags":[]},{"title":"1.朴素贝叶斯算法","slug":"NLP/7-文本分类/1.朴素贝叶斯算法","date":"2017-12-18T09:17:35.000Z","updated":"2018-09-18T10:06:14.531Z","comments":true,"path":"2017/12/18/NLP/7-文本分类/1.朴素贝叶斯算法/","link":"","permalink":"http://yoursite.com/2017/12/18/NLP/7-文本分类/1.朴素贝叶斯算法/","excerpt":"","text":"一、基本概念1.分类原理通过某对象的先验概率，利用贝叶斯公式，计算出其后验概率。即该对象属于某一类的概率，选择具有最大后验概率的类作为该对象所属的类。 2.贝叶斯公式$$P(A|B) = \\frac{P(B|A)*P(A)}{P(B)}$$ (1).其中P(A)为先验概率：先验概率（prior probability）是指根据以往经验和分析得到的概率，如全概率公式，它往往作为”由因求果”问题中的”因”出现的概率。； (2).其中P(B|A)为似然概率(likelihood)：是先前统计的事件中，A事件发生情况下B事件发生的概率 (3).其中P(B)为边界似然概率； (4).其中P(A|B)为后验概率；3.相关概念 (1).先验概率 (2).—-- ①后验概率 - ②.最大后验概率 (3).— ①.条件概率：指事件A在另外一个事件B已经发生条件下的发生概率。条件概率表示为：P(A|B)。若只有两个事件A，B，那么$P(A|B) = \\frac{P(AB)}{P(B)}$ ②.联合概率：表示两个事件共同发生的概率。A与B的联合概率表示为 P(AB) 或者P(A,B),或者P（A∩B） ③.全概率 (4).似然概率 联合概率的乘法公式为：P(AB) = P(A|B)*P(B),变形后可得到$P(A|B) = \\frac{P(AB)}{P(B)}$ 二、朴素贝叶斯分类器1.朴素贝叶斯分类的定义朴素贝叶斯分类的正式定义如下：(1).设 $x = \\left{ f{1},f{2},f{3},…,f{m} \\right}$为一个待分类项，而每个f为x的一个特征。(2).有类别集合$C = \\left{ y{1},y{2},y{3},…,y{n} \\right}$(3).计算$P(y{1}|x),P(y{2}|x),….,P(y{n}|x)$(4).如果$P(y{k}|x) = max \\left{ P(y{1}|x),P(y{2}|x),….,P(y{n}|x) \\right}，则x∈y{k}$。那么现在的关键就是如何计算第3步中的各个条件概率。我们可以这么做：(1)、找到一个已知分类的待分类项集合，这个集合叫做训练样本集。(2)、统计得到在各类别下各个特征属性的条件概率估计。即$P(x{1}|y{1}),P(x{2}|y{1}),…,P(x{3}|y{1})$;$P(x{1}|y{2}),P(x{2}|y{2}),…,P(x{3}|y{2})$;……$P(x{1}|y{3}),P(x{2}|y{3}),…,P(x{3}|y{3})$;(3)、如果各个特征属性是条件独立的，则根据贝叶斯定理有如下推导：$P(y{i}|x) = \\frac {P(x|y{i})P(y{i})}{P(x)}$ 因为分母对于所有类别为常数，因为我们只要将分子最大化皆可。又因为各特征属性是条件独立的，所以有： $P(x|y{i})P(y{i}) = P(a{1}|y{i})P(a{2}|y{i})…P(a{m}|y{i})P(y{i})=P(y(i))\\prod\\limits{j=1}^mP(a_j|yi)$ 2.朴素贝叶斯分类的举例我们知道朴素贝叶斯的公式为：$P(C|f) = \\frac{P(f|C)P(C)}{P(f)}$如果换个表达式就会明朗很多，如下：$P(类别|特征) = \\frac{P(特征|分类)P(分类)}{P(特征)}$其中P(分类)和P(特征)都是已知的，我们只需求P(特征|分类)即可。 例题分析给定数据如下：| 高否 | 富否 | 帅否 | 嫁否 || – | — | – | – || 高 | 富 | 帅 | 嫁 || 高 | 不富 | 帅 | 嫁 || 不高 | 富 | 不帅 | 嫁 || 高 | 富 | 不帅 | 不嫁 || 不高 | 不富 | 帅 | 不嫁 || 高 | 不富 | 不帅 | 不嫁 || 不高 | 不富 | 不帅 | 不嫁 |那我们现在的问题是，一个男生向一个女生求婚，这个男生具有以下三个特点：不高、富、帅，请你判断以下该女孩是否会嫁？ 这是一个典型的分类问题，转化为概率论问题就是$P(嫁|不高、富、帅)$ 与 $P(不嫁|不高、富、帅)那个概率更大？ 这里我们就使用朴素贝叶斯公式来分别求以下两种情况下的概率： ①$P(嫁|不高、富、帅) = \\frac {P(不高、富、帅|嫁)*P(嫁)}{P(不高、富、帅)}$ ②$P(不嫁|不高、富、帅) = \\frac {P(不高、富、帅|不嫁)*P(不嫁)}{P(不高、富、帅)}$对①求解我们先对要求①进行求解。要求$P(嫁|不高、富、帅)$的概率只需求$P(不高、富、帅|嫁)、P(嫁)、P(不高、富、帅)$即可。根据“朴素”一词也就是各个特征之间是独立的，可以得到如下公式和公式(只需求如下公式即可)：$P(不高、富、帅|嫁) = P(不高|嫁)P(富|嫁)P(帅|嫁)$$P(不高、富、帅) = P(不高)P(富)P(帅)$同时只需要再求出公式问题就得到解决$P(嫁)$ 我们从表格中统计所有嫁的样本共有3条，其中不高的样本有1条，所以$P(不高|嫁) = 1/3$，同理可以得到$P(富|嫁) = 2/3$， $P(帅|嫁) = 2/3$。 我们从表格中统计所有样本共有7条，其中嫁的样本有3条，所以 $P(嫁) = 3/7$。 我们从表格中统计所有样本共有7条，其中不高的样本有3条，所以 $P(不高) = 3/7$，其中富的样本有3条所以$P(富) = 3/7$，其中帅的样本有3条所以$P(帅) = 3/7$。 综上①$P(嫁|不高、富、帅) = \\frac {P(不高、富、帅|嫁)P(嫁)}{ \\; P(不高、富、帅) \\; } $ 此公式 $= \\frac {(\\frac {1}{3} \\frac {2}{3}\\frac {2}{3})\\frac {3}{7}} {\\frac {3}{7} \\frac {3}{7}\\frac {3}{7}} =\\frac {196}{243}$ 对②求解$P(不嫁|不高、富、帅) = \\frac {P(不高、富、帅|不嫁)P(不嫁)}{P(不高、富、帅)}$此公式 $= \\frac {(\\frac {1}{2} \\frac {1}{4}\\frac {1}{4})\\frac {4}{7}} {\\frac {3}{7} \\frac {3}{7}\\frac {3}{7}} =\\frac {49}{216}$ 所以最终的答案是“嫁”参考：1.https://blog.csdn.net/xueyingxue001/article/details/523961702.https://blog.csdn.net/amds123/article/details/70173402","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"},{"name":"7-文本分类","slug":"NLP/7-文本分类","permalink":"http://yoursite.com/categories/NLP/7-文本分类/"}],"tags":[]},{"title":"0文本分类算法总结","slug":"NLP/7-文本分类/0.文本分类算法总结","date":"2017-12-18T09:17:35.000Z","updated":"2018-09-18T10:05:46.154Z","comments":true,"path":"2017/12/18/NLP/7-文本分类/0.文本分类算法总结/","link":"","permalink":"http://yoursite.com/2017/12/18/NLP/7-文本分类/0.文本分类算法总结/","excerpt":"","text":"","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"},{"name":"7-文本分类","slug":"NLP/7-文本分类","permalink":"http://yoursite.com/categories/NLP/7-文本分类/"}],"tags":[]},{"title":"3-层次聚类法之CANOPY","slug":"NLP/8-文本聚类/3-层次聚类法之CANOPY","date":"2017-05-18T09:17:35.000Z","updated":"2018-09-25T10:45:58.395Z","comments":true,"path":"2017/05/18/NLP/8-文本聚类/3-层次聚类法之CANOPY/","link":"","permalink":"http://yoursite.com/2017/05/18/NLP/8-文本聚类/3-层次聚类法之CANOPY/","excerpt":"","text":"","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"},{"name":"8-文本聚类","slug":"NLP/8-文本聚类","permalink":"http://yoursite.com/categories/NLP/8-文本聚类/"}],"tags":[]},{"title":"2-密度聚类法之DBSCAN","slug":"NLP/8-文本聚类/2-密度聚类法之DBSCAN - 副本","date":"2017-03-18T09:17:35.000Z","updated":"2018-09-25T10:45:53.121Z","comments":true,"path":"2017/03/18/NLP/8-文本聚类/2-密度聚类法之DBSCAN - 副本/","link":"","permalink":"http://yoursite.com/2017/03/18/NLP/8-文本聚类/2-密度聚类法之DBSCAN - 副本/","excerpt":"","text":"","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"},{"name":"8-文本聚类","slug":"NLP/8-文本聚类","permalink":"http://yoursite.com/categories/NLP/8-文本聚类/"}],"tags":[]},{"title":"人工智能机器学习NLP算法分类总结","slug":"人工智能机器学习NLP算法分类总结","date":"2017-03-10T02:27:01.000Z","updated":"2018-09-18T09:49:24.191Z","comments":true,"path":"2017/03/10/人工智能机器学习NLP算法分类总结/","link":"","permalink":"http://yoursite.com/2017/03/10/人工智能机器学习NLP算法分类总结/","excerpt":"","text":"一、人工智能学习算法分类 人工智能算法大体上来说可以分类两类：基于统计的机器学习算法(Machine Learning)和深度学习算法(Deep Learning) 总的来说，在sklearn中机器学习算法大概的分类如下：1. 纯算法类 (1).回归算法 (2).分类算法 (3).聚类算法 (4)降维算法 (5)概率图模型算法 (6)文本挖掘算法 (7)优化算法 (8)深度学习算法2.建模方面 (1).模型优化 (2).数据预处理 二、详细算法1.分类算法 (1).LR (Logistic Regression，逻辑回归又叫逻辑分类) (2).SVM (Support Vector Machine，支持向量机) (3).NB (Naive Bayes，朴素贝叶斯) (4).DT (Decision Tree，决策树) 1).C4.5 2).ID3 3).CART (5).集成算法 1).Bagging 2).Random Forest (随机森林) 3).GB(梯度提升,Gradient boosting) 4).GBDT (Gradient Boosting Decision Tree) 5).AdaBoost 6).Xgboost (6).最大熵模型 2.回归算法 (1).LR (Linear Regression，线性回归) (2).SVR (支持向量机回归) (3). RR (Ridge Regression，岭回归)3.聚类算法 (1).Knn (2).Kmeans 算法 (3).层次聚类 (4).密度聚类4.降维算法 (1).SGD (随机梯度下降) (2). 5.概率图模型算法 (1).贝叶斯网络 (2).HMM (3).CRF (条件随机场)6.文本挖掘算法(1).模型 1).LDA (主题生成模型，Latent Dirichlet Allocation) 4).最大熵模型(2).关键词提取 1).tf-idf 2).bm25 3).textrank 4).pagerank 5).左右熵 :左右熵高的作为关键词 6).互信息： (3).词法分析 1).分词 ①HMM (因马尔科夫) ②CRF (条件随机场) 2).词性标注 3).命名实体识别(4).句法分析 1).句法结构分析 2).依存句法分析(5).文本向量化 1).tf-idf 2).word2vec 3).doc2vec 4).cw2vec(6).距离计算 1).欧氏距离 2).相似度计算 7.优化算法 (1).正则化 1).L1正则化 2).L2正则化8.深度学习算法 (1).BP (2).CNN (3).DNN (3).RNN (4).LSTM 三、建模方面1.模型优化· (1).特征选择 (2).梯度下降 (3).交叉验证 (4).参数调优 (5).模型评估：准确率、召回率、F1、AUC、ROC、损失函数 2.数据预处理 (1).标准化 (2).异常值处理 (3).二值化 (4).缺失值填充： 支持均值、中位数、特定值补差、多重插补","categories":[],"tags":[]},{"title":"1-划分聚类法之k-means","slug":"NLP/8-文本聚类/1-划分聚类法之k-means","date":"2017-01-18T09:17:35.000Z","updated":"2018-09-25T10:45:46.632Z","comments":true,"path":"2017/01/18/NLP/8-文本聚类/1-划分聚类法之k-means/","link":"","permalink":"http://yoursite.com/2017/01/18/NLP/8-文本聚类/1-划分聚类法之k-means/","excerpt":"","text":"","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"},{"name":"8-文本聚类","slug":"NLP/8-文本聚类","permalink":"http://yoursite.com/categories/NLP/8-文本聚类/"}],"tags":[]},{"title":"CNN算法","slug":"DeepLearning/CNN/CNN","date":"2017-01-09T08:02:27.000Z","updated":"2018-09-20T06:12:27.399Z","comments":true,"path":"2017/01/09/DeepLearning/CNN/CNN/","link":"","permalink":"http://yoursite.com/2017/01/09/DeepLearning/CNN/CNN/","excerpt":"","text":"二、学习资料 Michael Nielsen的《Neural Network and Deep Learning》中文翻译《神经网络与深度学习》","categories":[{"name":"DeepLearning","slug":"DeepLearning","permalink":"http://yoursite.com/categories/DeepLearning/"},{"name":"CNN","slug":"DeepLearning/CNN","permalink":"http://yoursite.com/categories/DeepLearning/CNN/"}],"tags":[]},{"title":"CNN算法","slug":"DeepLearning/CNN/CNN算法之文本分类","date":"2017-01-09T08:02:27.000Z","updated":"2018-09-20T06:12:27.399Z","comments":true,"path":"2017/01/09/DeepLearning/CNN/CNN算法之文本分类/","link":"","permalink":"http://yoursite.com/2017/01/09/DeepLearning/CNN/CNN算法之文本分类/","excerpt":"","text":"二、学习资料 Michael Nielsen的《Neural Network and Deep Learning》中文翻译《神经网络与深度学习》","categories":[{"name":"DeepLearning","slug":"DeepLearning","permalink":"http://yoursite.com/categories/DeepLearning/"},{"name":"CNN","slug":"DeepLearning/CNN","permalink":"http://yoursite.com/categories/DeepLearning/CNN/"}],"tags":[]},{"title":"感知机算法","slug":"DeepLearning/Perceptron/Perceptron","date":"2017-01-09T08:02:27.000Z","updated":"2018-09-20T06:11:58.139Z","comments":true,"path":"2017/01/09/DeepLearning/Perceptron/Perceptron/","link":"","permalink":"http://yoursite.com/2017/01/09/DeepLearning/Perceptron/Perceptron/","excerpt":"","text":"二、学习资料 Michael Nielsen的《Neural Network and Deep Learning》中文翻译《神经网络与深度学习》","categories":[{"name":"DeepLearning","slug":"DeepLearning","permalink":"http://yoursite.com/categories/DeepLearning/"},{"name":"Perceptron","slug":"DeepLearning/Perceptron","permalink":"http://yoursite.com/categories/DeepLearning/Perceptron/"}],"tags":[]},{"title":"deeplearning算法分类","slug":"DeepLearning/deeplearning","date":"2017-01-09T08:02:27.000Z","updated":"2018-09-20T06:11:12.756Z","comments":true,"path":"2017/01/09/DeepLearning/deeplearning/","link":"","permalink":"http://yoursite.com/2017/01/09/DeepLearning/deeplearning/","excerpt":"","text":"一、常用神经网络分类1.Perceptron(感知机,神经网络的前身)2.前向传播算法3.BP(反向传播算法)4.DNN(深度神经网络)5.CNN(卷积神经网络)6.RNN(循环神经网络)7.LSTM(..神经网络)二、学习资料 Michael Nielsen的《Neural Network and Deep Learning》中文翻译《神经网络与深度学习》","categories":[{"name":"DeepLearning","slug":"DeepLearning","permalink":"http://yoursite.com/categories/DeepLearning/"}],"tags":[]},{"title":"RNN算法","slug":"DeepLearning/RNN/RNN","date":"2017-01-09T08:02:27.000Z","updated":"2018-09-29T06:30:25.521Z","comments":true,"path":"2017/01/09/DeepLearning/RNN/RNN/","link":"","permalink":"http://yoursite.com/2017/01/09/DeepLearning/RNN/RNN/","excerpt":"","text":"","categories":[{"name":"DeepLearning","slug":"DeepLearning","permalink":"http://yoursite.com/categories/DeepLearning/"},{"name":"RNN","slug":"DeepLearning/RNN","permalink":"http://yoursite.com/categories/DeepLearning/RNN/"}],"tags":[]},{"title":"0-文本聚类算法总结","slug":"NLP/8-文本聚类/0-文本聚类算法总结 - 副本","date":"2016-12-18T09:17:35.000Z","updated":"2018-09-25T10:45:35.496Z","comments":true,"path":"2016/12/18/NLP/8-文本聚类/0-文本聚类算法总结 - 副本/","link":"","permalink":"http://yoursite.com/2016/12/18/NLP/8-文本聚类/0-文本聚类算法总结 - 副本/","excerpt":"","text":"一、文本聚类算法总结1.划分法(partitioning methods)：给定一个有N个元组或者纪录的数据集，分裂法将构造K个分组，每一个分组就代表一个聚类，K&lt;N。而且这K个分组满足下列条件：（1） 每一个分组至少包含一个数据纪录；（2）每一个数据纪录属于且仅属于一个分组（注意：这个要求在某些模糊聚类算法中可以放宽）；对于给定的K，算法首先给出一个初始的分组方法，以后通过反复迭代的方法改变分组，使得每一次改进之后的分组方案都较前一次好，而所谓好的标准就是：同一分组中的记录越近越好，而不同分组中的纪录越远越好。使用这个基本思想的算法有：K-MEANS算法、K-MEDOIDS算法、CLARANS算法； 2.层次法2.1层次发的总结(hierarchical methods)：这种方法对给定的数据集进行层次似的分解，直到某种条件满足为止。具体又可分为“自底向上”和“自顶向下”两种方案。例如在“自底向上”方案中，初始时每一个数据纪录都组成一个单独的组，在接下来的迭代中，它把那些相互邻近的组合并成一个组，直到所有的记录组成一个分组或者某个条件满足为止。代表算法有：CANOPY算法、BIRCH算法、CURE算法、CHAMELEON算法等； 2.2层次法的优点(1).距离和规则的相似度容易定义，限制少(2).不需要预先定制聚类书(3).可以发现类的层次关系(4).可以聚类成其他形状 2.3层次法的缺点(1).计算复杂度太高(2).奇异值也能产生很大影响(3).算法可能聚成链状 3.基于密度的方法（density-based methods):基于密度的方法与其它方法的一个根本区别是：它不是基于各种各样的距离的，而是基于密度的。这样就能克服基于距离的算法只能发现“类圆形”的聚类的缺点。这个方法的指导思想就是，只要一个区域中的点的密度大过某个阀值，就把它加到与之相近的聚类中去。代表算法有：DBSCAN算法、OPTICS算法、DENCLUE算法等； 4.基于网格的方法(grid-based methods):这种方法首先将数据空间划分成为有限个单元（cell）的网格结构,所有的处理都是以单个的单元为对象的。这么处理的一个突出的优点就是处理速度很快，通常这是与目标数据库中记录的个数无关的，它只与把数据空间分为多少个单元有关。代表算法有：STING算法、CLIQUE算法、WAVE-CLUSTER算法； 5.基于模型的方法(model-based methods):基于模型的方法给每一个聚类假定一个模型，然后去寻找一个能很好的满足这个模型的数据集。这样一个模型可能是数据点在空间中的密度分布函数或者其它。它的一个潜在的假定就是：目标数据集是由一系列的概率分布所决定的。通常有两种尝试方向：统计的方案和神经网络的方案 参考：[1].https://wenku.baidu.com/view/8713cffc581b6bd97e19eaea.html","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"},{"name":"8-文本聚类","slug":"NLP/8-文本聚类","permalink":"http://yoursite.com/categories/NLP/8-文本聚类/"}],"tags":[]},{"title":"0.文本特征提取算法总结","slug":"NLP/9-特征提取/0.文本特征提取算法总结 - 副本","date":"2016-01-18T05:17:35.000Z","updated":"2018-09-18T10:10:34.298Z","comments":true,"path":"2016/01/18/NLP/9-特征提取/0.文本特征提取算法总结 - 副本/","link":"","permalink":"http://yoursite.com/2016/01/18/NLP/9-特征提取/0.文本特征提取算法总结 - 副本/","excerpt":"","text":"为分类文本作处理的特征提取算法也对最终效果有巨大影响，而特征提取算法又分为特征选择和特征抽取两大类，其中特征选择算法有互信息，文档频率，信息增益，开方检验等等十数种","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"},{"name":"9-特征提取","slug":"NLP/9-特征提取","permalink":"http://yoursite.com/categories/NLP/9-特征提取/"}],"tags":[]},{"title":"1.TF-IDF算法","slug":"NLP/9-特征提取/1.TF-IDF","date":"2016-01-18T05:17:35.000Z","updated":"2018-09-28T08:59:27.402Z","comments":true,"path":"2016/01/18/NLP/9-特征提取/1.TF-IDF/","link":"","permalink":"http://yoursite.com/2016/01/18/NLP/9-特征提取/1.TF-IDF/","excerpt":"","text":"TF-IDF算法可以分解为两部分：TF和IDF 一、TF算法1.词频（term frequency，TF）2.公式：$TF =\\frac {N{i,j}}{ \\sum{k}N_{k,j}}$3.解释：以上式子中分子是该词在文件中的出现次数，而分母则是在文件中所有字词的出现次数之和。 二、IDF算法1.逆向文件频率（inverse document frequency，IDF）2.公式：$IDF =lg \\frac {|D|}{|j:t_i ∈d_j|}$3.解释：逆向文件频率（inverse document frequency，IDF）是一个词语普遍重要性的度量。某一特定词语的IDF，可以由总文件数目除以包含该词语之文件的数目，再将得到的商取以10为底的对数得到","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"},{"name":"9-特征提取","slug":"NLP/9-特征提取","permalink":"http://yoursite.com/categories/NLP/9-特征提取/"}],"tags":[]}]}