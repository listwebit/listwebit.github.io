{"meta":{"title":"帅的掉渣的博客","subtitle":null,"description":null,"author":"listwebit","url":"http://yoursite.com"},"pages":[{"title":"about","date":"2018-08-10T06:22:54.000Z","updated":"2018-08-10T06:22:54.035Z","comments":true,"path":"about/index-1.html","permalink":"http://yoursite.com/about/index-1.html","excerpt":"","text":""},{"title":"'about'","date":"2018-08-10T06:10:04.000Z","updated":"2018-08-10T06:10:38.230Z","comments":true,"path":"about/index.html","permalink":"http://yoursite.com/about/index.html","excerpt":"","text":"这里是关于页面"}],"posts":[{"title":"","slug":"人工智能机器学习NLP算法分类总结","date":"2018-08-10T02:27:01.714Z","updated":"2019-02-18T10:28:49.751Z","comments":true,"path":"2018/08/10/人工智能机器学习NLP算法分类总结/","link":"","permalink":"http://yoursite.com/2018/08/10/人工智能机器学习NLP算法分类总结/","excerpt":"","text":"第一章、目录 人工智能算法大体上来说可以分类两类：基于统计的机器学习算法(Machine Learning)和深度学习算法(Deep Learning) 1. 机器学习算法 (1).回归算法 (2).分类算法 (3).聚类算法 (4)降维算法 (5)概率图模型算法 (6)文本挖掘算法 (7)优化算法2.深度学习算法 3.分布式计算4.数据结构5.其他方面 5.1模型方面 (1).模型优化 (2).数据预处理 5.2应用方面 对话系统 推线系统 知识图谱 第二章、机器学习算法1.分类算法 (1).LR (Logistic Regression，逻辑回归又叫逻辑分类) (2).SVM (Support Vector Machine，支持向量机) (3).NB (Naive Bayes，朴素贝叶斯) (4).DT (Decision Tree，决策树) 1).C4.5 2).ID3 3).CART (5).集成算法 1).Bagging 2).Random Forest (随机森林) 3).GB(梯度提升,Gradient boosting) 4).GBDT (Gradient Boosting Decision Tree) 5).AdaBoost 6).Xgboost (6).最大熵模型 2.回归算法 (1).LR (Linear Regression，线性回归) (2).SVR (支持向量机回归) (3). RR (Ridge Regression，岭回归)3.聚类算法 (1).Knn (2).Kmeans 算法 (3).层次聚类 (4).密度聚类4.降维算法 (1).SGD (随机梯度下降) (2). 5.概率图模型算法 (1).贝叶斯网络 (2).HMM (3).CRF (条件随机场)6.文本挖掘算法(1).模型 1).LDA (主题生成模型，Latent Dirichlet Allocation) 4).最大熵模型(2).关键词提取 1).tf-idf 2).bm25 3).textrank 4).pagerank 5).左右熵 :左右熵高的作为关键词 6).互信息： (3).词法分析 1).分词 ①HMM (因马尔科夫) ②CRF (条件随机场) 2).词性标注 3).命名实体识别(4).句法分析 1).句法结构分析 2).依存句法分析(5).文本向量化 1).tf-idf 2).word2vec 3).doc2vec 4).cw2vec(6).距离计算 1).欧氏距离 2).相似度计算 7.优化算法 (1).正则化 1).L1正则化 2).L2正则化 第三章、深度学习算法 (1).BP (2).CNN (3).DNN (3).RNN (4).LSTM 第四章、分布式计算4.1.spark4.2.hadoop第五章、数据结构第六章、其他方面6.1模型方面6.1.1.模型优化 (1).特征选择 (2).梯度下降 (3).交叉验证 (4).参数调优 (5).模型评估：准确率、召回率、F1、AUC、ROC、损失函数 6.1.2.数据预处理 (1).标准化 (2).异常值处理 (3).二值化 (4).缺失值填充： 支持均值、中位数、特定值补差、多重插补","categories":[],"tags":[]},{"title":"文章观点抽取.md","slug":"NLP/3-短语分析/观点抽取/文章观点抽取/文章观点抽取","date":"2018-05-18T05:17:35.000Z","updated":"2018-09-26T12:13:05.016Z","comments":true,"path":"2018/05/18/NLP/3-短语分析/观点抽取/文章观点抽取/文章观点抽取/","link":"","permalink":"http://yoursite.com/2018/05/18/NLP/3-短语分析/观点抽取/文章观点抽取/文章观点抽取/","excerpt":"","text":"","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"},{"name":"3-短语分析","slug":"NLP/3-短语分析","permalink":"http://yoursite.com/categories/NLP/3-短语分析/"},{"name":"观点抽取","slug":"NLP/3-短语分析/观点抽取","permalink":"http://yoursite.com/categories/NLP/3-短语分析/观点抽取/"},{"name":"文章观点抽取","slug":"NLP/3-短语分析/观点抽取/文章观点抽取","permalink":"http://yoursite.com/categories/NLP/3-短语分析/观点抽取/文章观点抽取/"}],"tags":[]},{"title":"评论观点抽取","slug":"NLP/3-短语分析/观点抽取/评论观点抽取/评论观点抽取","date":"2018-05-16T05:17:35.000Z","updated":"2018-09-26T12:12:36.848Z","comments":true,"path":"2018/05/16/NLP/3-短语分析/观点抽取/评论观点抽取/评论观点抽取/","link":"","permalink":"http://yoursite.com/2018/05/16/NLP/3-短语分析/观点抽取/评论观点抽取/评论观点抽取/","excerpt":"","text":"","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"},{"name":"3-短语分析","slug":"NLP/3-短语分析","permalink":"http://yoursite.com/categories/NLP/3-短语分析/"},{"name":"观点抽取","slug":"NLP/3-短语分析/观点抽取","permalink":"http://yoursite.com/categories/NLP/3-短语分析/观点抽取/"},{"name":"评论观点抽取","slug":"NLP/3-短语分析/观点抽取/评论观点抽取","permalink":"http://yoursite.com/categories/NLP/3-短语分析/观点抽取/评论观点抽取/"}],"tags":[]},{"title":"观点抽取分类","slug":"NLP/3-短语分析/观点抽取/观点抽取分类","date":"2018-05-16T05:17:35.000Z","updated":"2018-09-26T12:11:59.135Z","comments":true,"path":"2018/05/16/NLP/3-短语分析/观点抽取/观点抽取分类/","link":"","permalink":"http://yoursite.com/2018/05/16/NLP/3-短语分析/观点抽取/观点抽取分类/","excerpt":"","text":"一、观点抽取的分类我认为按照观点抽取的对象可以分为两类，一种是对文章进行观点抽取，得到文章的类似摘要、分类、关键词等性质的句子、短语或者关键词。一种是对评论进行观点抽取，得到一个短语，然后在对短语进行分类。 1.文章观点抽取文章的观点抽取结果可以是一句话（类似标题），一段话（类似摘要），一个短语，一个关键词（打标签，其实是文本分类）。 2.评论观点抽取评论的观点抽取结果一般是一个短语。类似淘宝评论的观点抽取。例如可以将某一件衣服评论观点分为：质量好，超划算，性价比高，穿上好看。","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"},{"name":"3-短语分析","slug":"NLP/3-短语分析","permalink":"http://yoursite.com/categories/NLP/3-短语分析/"},{"name":"观点抽取","slug":"NLP/3-短语分析/观点抽取","permalink":"http://yoursite.com/categories/NLP/3-短语分析/观点抽取/"}],"tags":[]},{"title":"0-新词发现.md","slug":"NLP/1-词法分析/2-新词发现/0-新词发现","date":"2018-02-18T09:17:35.000Z","updated":"2018-12-19T07:58:41.582Z","comments":true,"path":"2018/02/18/NLP/1-词法分析/2-新词发现/0-新词发现/","link":"","permalink":"http://yoursite.com/2018/02/18/NLP/1-词法分析/2-新词发现/0-新词发现/","excerpt":"","text":"一、新词 新词发现又叫未登录词识别，我认为可以包括两种形式:1.命名实体识别 2.普通新词发现命名实体识别我们在另一章中专门讨论，本节只讨论普通新词发现。 目前新词发现可以分为两类算法：无监督，有监督。无监督主要通过互信息，左右熵来发现，有监督主要通过标注，例如CRF+深度学习 二、基本概念 现在基于统计方法的新词发现主要通过词内互信息(凝聚度)，词间的左右信息熵来确定，因此我们向明确一下基本 概念。 1.互信息：凝聚度(Pointwise mutual information) 公式：$PMI = \\frac{P(x,y)}{P(x)P(y)}$ 如果x，y独立 ，那么pmi等于1。如果不独立，则PMI大于1，当PMI足够大的时候就表示凝固度足够高。拿 “知”、“乎” 这两个字来说，假设在 5000 万字的样本中, “知” 出现了 150 万次， “乎” 出现了 4 万次。那 “知” 出现的概率为 0.03, “乎” 出现的概率为 0.0008。如果两个字符出现是个独立事件的话，”知”、“乎” 一起出现的期望概率是 0.03 0.0008 = 2.4e-05. 如果实际上 “知乎” 出现了 3 万次, 则实际上”知”、“乎” 一起出现的概率是 6e-03, 是期望概率的 250 倍。也就是说两个字越相关，点间互信息越大。 2.信息熵公式：$H = \\sum_{i=1}^{n}{P(i)*logP(i)}$熵描述的是信息的不确定性，熵越大，不确定性越强，例如“萝卜”，左边可以有“吃萝卜”、“腌萝卜”、“炒萝卜”、“炖萝卜”，左边的词越多，代表萝卜的左熵越丰富，那“萝卜”成词的可能性就越大。 3.左右熵左右熵是根据信息熵来算的。我们用信息熵来衡量一个文本片段的左邻字集合和右邻字集合有多随机。考虑这么一句话“吃葡萄不吐葡萄皮不吃葡萄倒吐葡萄皮”，“葡萄”一词出现了四次，其中左邻字分别为 {吃, 吐, 吃, 吐} ，右邻字分别为 {不, 皮, 倒, 皮} 。根据公式，“葡萄”一词的左邻字的信息熵为 – (1/2) · log(1/2) – (1/2) · log(1/2) ≈ 0.693 ，它的右邻字的信息熵则为 – (1/2) · log(1/2) – (1/4) · log(1/4) – (1/4) · log(1/4) ≈ 1.04 。可见，在这个句子中，“葡萄”一词的右邻字更加丰富一些。","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"},{"name":"1-词法分析","slug":"NLP/1-词法分析","permalink":"http://yoursite.com/categories/NLP/1-词法分析/"},{"name":"2-新词发现","slug":"NLP/1-词法分析/2-新词发现","permalink":"http://yoursite.com/categories/NLP/1-词法分析/2-新词发现/"}],"tags":[]},{"title":"代价函数，损失函数，目标函数区别","slug":"MachineLearning/0-基本概念/代价函数，损失函数，目标函数区别","date":"2018-01-09T08:02:27.000Z","updated":"2018-11-01T09:46:59.187Z","comments":true,"path":"2018/01/09/MachineLearning/0-基本概念/代价函数，损失函数，目标函数区别/","link":"","permalink":"http://yoursite.com/2018/01/09/MachineLearning/0-基本概念/代价函数，损失函数，目标函数区别/","excerpt":"","text":"首先给出结论： 损失函数（Loss Function ）是定义在单个样本上的，算的是一个样本的误差。 代价函数（Cost Function ）是定义在整个训练集上的，是所有样本误差的平均，也就是损失函数的平均。 目标函数（Object Function）定义为：最终需要优化的函数。等于经验风险+结构风险（也就是Cost Function + 正则化项）。 参考：https://www.zhihu.com/question/52398145","categories":[{"name":"MachineLearning","slug":"MachineLearning","permalink":"http://yoursite.com/categories/MachineLearning/"},{"name":"0-基本概念","slug":"MachineLearning/0-基本概念","permalink":"http://yoursite.com/categories/MachineLearning/0-基本概念/"}],"tags":[]},{"title":"时序","slug":"MachineLearning/4-时序/时序","date":"2018-01-09T08:02:27.000Z","updated":"2019-02-20T04:10:20.059Z","comments":true,"path":"2018/01/09/MachineLearning/4-时序/时序/","link":"","permalink":"http://yoursite.com/2018/01/09/MachineLearning/4-时序/时序/","excerpt":"","text":"","categories":[{"name":"MachineLearning","slug":"MachineLearning","permalink":"http://yoursite.com/categories/MachineLearning/"},{"name":"4-时序","slug":"MachineLearning/4-时序","permalink":"http://yoursite.com/categories/MachineLearning/4-时序/"}],"tags":[]},{"title":"回归的分类","slug":"MachineLearning/2-回归/回归的分类","date":"2018-01-09T08:02:27.000Z","updated":"2018-01-09T08:31:26.000Z","comments":true,"path":"2018/01/09/MachineLearning/2-回归/回归的分类/","link":"","permalink":"http://yoursite.com/2018/01/09/MachineLearning/2-回归/回归的分类/","excerpt":"一、回归可以分为以下几类 1.线性回归 2.非线性回归 3.逻辑回归","text":"一、回归可以分为以下几类 1.线性回归 2.非线性回归 3.逻辑回归 二、回归的概念1.线性回归可以简单理解为线性就是每个变量的指数都是1。 2.非线性回归而非线性就是至少有一个变量的指数不是1。 3.逻辑回归回归和分类的区别分类和回归的区别在于输出变量的类型。 定量输出称为回归，或者说是连续变量预测；定性输出称为分类，或者说是离散变量预测。 举个例子：预测明天的气温是多少度，这是一个回归任务；预测明天是阴、晴还是雨，就是一个分类任务。","categories":[{"name":"MachineLearning","slug":"MachineLearning","permalink":"http://yoursite.com/categories/MachineLearning/"},{"name":"2-回归","slug":"MachineLearning/2-回归","permalink":"http://yoursite.com/categories/MachineLearning/2-回归/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/机器学习/"}]},{"title":"回归","slug":"MachineLearning/2-回归/回归","date":"2018-01-09T08:02:27.000Z","updated":"2018-01-09T08:02:27.288Z","comments":true,"path":"2018/01/09/MachineLearning/2-回归/回归/","link":"","permalink":"http://yoursite.com/2018/01/09/MachineLearning/2-回归/回归/","excerpt":"","text":"","categories":[{"name":"MachineLearning","slug":"MachineLearning","permalink":"http://yoursite.com/categories/MachineLearning/"},{"name":"2-回归","slug":"MachineLearning/2-回归","permalink":"http://yoursite.com/categories/MachineLearning/2-回归/"}],"tags":[]},{"title":"0-吴恩达-机器学习","slug":"学习资料/机器学习/0-吴恩达—机器学习","date":"2018-01-09T08:02:27.000Z","updated":"2018-11-28T12:24:16.356Z","comments":true,"path":"2018/01/09/学习资料/机器学习/0-吴恩达—机器学习/","link":"","permalink":"http://yoursite.com/2018/01/09/学习资料/机器学习/0-吴恩达—机器学习/","excerpt":"","text":"机器学习课程地址：https://www.coursera.org/course/ml 笔记地址：http://www.ai-start.com/ml2014/ 深度学习课程地址： https://mooc.study.163.com/university/deeplearning_ai#/c 笔记地址： http://www.ai-start.com/dl2017/","categories":[{"name":"学习资料","slug":"学习资料","permalink":"http://yoursite.com/categories/学习资料/"},{"name":"机器学习","slug":"学习资料/机器学习","permalink":"http://yoursite.com/categories/学习资料/机器学习/"}],"tags":[]},{"title":"2017年总结","slug":"总结/2017年总结","date":"2018-01-09T06:55:28.000Z","updated":"2018-08-11T01:06:32.802Z","comments":true,"path":"2018/01/09/总结/2017年总结/","link":"","permalink":"http://yoursite.com/2018/01/09/总结/2017年总结/","excerpt":"","text":"","categories":[{"name":"总结","slug":"总结","permalink":"http://yoursite.com/categories/总结/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/tags/深度学习/"}]},{"title":"2017年","slug":"总结/2017年","date":"2018-01-09T06:55:03.000Z","updated":"2018-01-09T06:55:03.415Z","comments":true,"path":"2018/01/09/总结/2017年/","link":"","permalink":"http://yoursite.com/2018/01/09/总结/2017年/","excerpt":"","text":"","categories":[{"name":"总结","slug":"总结","permalink":"http://yoursite.com/categories/总结/"}],"tags":[]},{"title":"3-模型假设函数,损失函数,代价函数,目标函数","slug":"MachineLearning/1-模型/3-模型假设函数,损失函数,代价函数,目标函数","date":"2017-12-19T09:17:35.000Z","updated":"2018-11-28T09:32:28.927Z","comments":true,"path":"2017/12/19/MachineLearning/1-模型/3-模型假设函数,损失函数,代价函数,目标函数/","link":"","permalink":"http://yoursite.com/2017/12/19/MachineLearning/1-模型/3-模型假设函数,损失函数,代价函数,目标函数/","excerpt":"","text":"假设函数(hypothesis function):预测函数损失函数(loss function,error function):计算的是一个样本的误差代价函数(cost function,成本函数):是整个训练集上所有样本误差的平均目标函数(objective function):代价函数 + 正则化项","categories":[{"name":"MachineLearning","slug":"MachineLearning","permalink":"http://yoursite.com/categories/MachineLearning/"},{"name":"1-模型","slug":"MachineLearning/1-模型","permalink":"http://yoursite.com/categories/MachineLearning/1-模型/"}],"tags":[]},{"title":"1-PageRank算法的前世今生","slug":"NLP/1-词法分析/4-关键词提取/1-PageRank的前世今生","date":"2017-12-19T09:17:35.000Z","updated":"2018-11-28T04:06:07.710Z","comments":true,"path":"2017/12/19/NLP/1-词法分析/4-关键词提取/1-PageRank的前世今生/","link":"","permalink":"http://yoursite.com/2017/12/19/NLP/1-词法分析/4-关键词提取/1-PageRank的前世今生/","excerpt":"","text":"一、算法定义PageRank，网页排名，又称网页级别、Google左侧排名或佩奇排名，是一种由 [1] 根据网页之间相互的超链接计算的技术，而作为网页排名的要素之一，以Google公司创办人拉里·佩奇（Larry Page）之姓来命名。Google用它来体现网页的相关性和重要性，在搜索引擎优化操作中是经常被用来评估网页优化的成效因素之一。Google的创始人拉里·佩奇和谢尔盖·布林于1998年在斯坦福大学发明了这项技术。 PageRank通过网络浩瀚的超链接关系来确定一个页面的等级。Google把从A页面到B页面的链接解释为A页面给B页面投票，Google根据投票来源（甚至来源的来源，即链接到A页面的页面）和投票目标的等级来决定新的等级。简单的说，一个高等级的页面可以使其他低等级页面的等级提升。 二、算法来源这个要从搜索引擎的发展讲起。最早的搜索引擎采用的是 分类目录[^ref_1] 的方法，即通过人工进行网页分类并整理出高质量的网站。那时 Yahoo 和国内的 hao123 就是使用的这种方法。 后来网页越来越多，人工分类已经不现实了。搜索引擎进入了 文本检索 的时代，即计算用户查询关键词与网页内容的相关程度来返回搜索结果。这种方法突破了数量的限制，但是搜索结果不是很好。因为总有某些网页来回地倒腾某些关键词使自己的搜索排名靠前。 于是我们的主角要登场了。没错，谷歌的两位创始人，当时还是美国斯坦福大学 (Stanford University) 研究生的佩奇 (Larry Page) 和布林 (Sergey Brin) 开始了对网页排序问题的研究。他们的借鉴了学术界评判学术论文重要性的通用方法， 那就是看论文的引用次数。由此想到网页的重要性也可以根据这种方法来评价。于是PageRank的核心思想就诞生了[^ref_2]，非常简单：121.如果一个网页被很多其他网页链接到的话说明这个网页比较重要，也就是PageRank值会相对较高2.如果一个PageRank值很高的网页链接到一个其他的网页，那么被链接到的网页的PageRank值会相应地因此而提高 三、搜索引擎搜索引擎的功能主要是：根据用户输入的关键字，返回文档的链接结果。搜索引擎主要解决的三大问题：(1)如何获取文档资料 (2)如何根据关键词检索到相关文档 (3)如何对文档进行排序，返回给用户满意的页面。 1.获取文档资料利用爬虫程序，获取互联网的页面资料。爬虫技术先从一个网页出发，将该网页的内容记录下来，保存到资料库，接着分析页面中的超链接，分别递归得去获取超链接页面。 2.如何根据关键词检索到相关文档采用倒排索引算法。简单的说，倒排索引是一对key-value对，key代表关键词，value代表拥有这些关键词的文档编号或者url。 3.文档排序这是搜索引擎最核心的问题，也是google发家致富的法宝 – PageRank算法。 四、算法原理4.1PageRank的两个假设 数量假设：1.如果一个网页被很多其他网页链接到的话说明这个网页比较重要，也就是PageRank值会相对较高。质量假设：2.如果一个PageRank值很高的网页链接到一个其他的网页，那么被链接到的网页的PageRank值会相应地因此而提高。 4.2PageRank的算法原理PageRank算法总的来说就是预先给每个网页一个PR值（下面用PR值指代PageRank值），由于PR值物理意义上为一个网页被访问概率，所以一般是1/N，其中N为网页总数。预先给定PR值后，通过下面的算法不断迭代，直至达到平稳分布为止。 PageRank值主要是的是节点的入链值。 4.3PageRank的简单计算4.4PageRank的修正计算","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"},{"name":"1-词法分析","slug":"NLP/1-词法分析","permalink":"http://yoursite.com/categories/NLP/1-词法分析/"},{"name":"4-关键词提取","slug":"NLP/1-词法分析/4-关键词提取","permalink":"http://yoursite.com/categories/NLP/1-词法分析/4-关键词提取/"}],"tags":[]},{"title":"2-模型评估的指标","slug":"MachineLearning/1-模型/2-模型评估的指标","date":"2017-12-19T09:17:35.000Z","updated":"2018-10-08T06:03:43.875Z","comments":true,"path":"2017/12/19/MachineLearning/1-模型/2-模型评估的指标/","link":"","permalink":"http://yoursite.com/2017/12/19/MachineLearning/1-模型/2-模型评估的指标/","excerpt":"","text":"多模型评估的指标可以分为以下几个类别 一.Accuracy，Precision，Recall要计算这几个指标先要了解几个概念：FN：False Negative,被判定为负样本，但事实上是正样本。FP：False Positive,被判定为正样本，但事实上是负样本。TN：True Negative,被判定为负样本，事实上也是负样本。TP：True Positive,被判定为正样本，事实上也是证样本。1.Accuracy (正确率)$ac=\\frac {TP+TN}{TP+TN+FP+FN}$ 2.Precision精确率，准确率，查准率$P = \\frac {TP}{TP+FP}$解释：正样本占分类器所分的正样本的比例 3.Recall(召回率，查全率)$R = \\frac {TP}{TP + FN}$解释：正样本占真正的正样本的比例","categories":[{"name":"MachineLearning","slug":"MachineLearning","permalink":"http://yoursite.com/categories/MachineLearning/"},{"name":"1-模型","slug":"MachineLearning/1-模型","permalink":"http://yoursite.com/categories/MachineLearning/1-模型/"}],"tags":[]},{"title":"1-判别模型和生成模型.md","slug":"MachineLearning/1-模型/1-判别模型和生成模型","date":"2017-12-19T09:17:35.000Z","updated":"2018-10-08T06:02:20.795Z","comments":true,"path":"2017/12/19/MachineLearning/1-模型/1-判别模型和生成模型/","link":"","permalink":"http://yoursite.com/2017/12/19/MachineLearning/1-模型/1-判别模型和生成模型/","excerpt":"","text":"判别模型 生成模型 特点 寻找不同类别之间的最优分类面，反映的是异类数据之间的差异 对后验概率建模，从统计的角度表示数据的分布情况，能够反映同类数据本身的相似度 区别(假定输入x, 类别标签y) 估计的是条件概率分布(conditional distribution) : P(yx) 估计的是联合概率分布（joint probability distribution: P(x, y) 联系 由判别式模型得不到产生式模型 由产生式模型可以得到判别式模型 常见模型 – logistic regression – SVMs – traditional neural networks – Nearest neighbor –Gaussians, Naive Bayes –Mixtures of Gaussians, Mixtures of experts, HMMs–Sigmoidal belief networks, Bayesian networks– Markov random fields","categories":[{"name":"MachineLearning","slug":"MachineLearning","permalink":"http://yoursite.com/categories/MachineLearning/"},{"name":"1-模型","slug":"MachineLearning/1-模型","permalink":"http://yoursite.com/categories/MachineLearning/1-模型/"}],"tags":[]},{"title":"0-语言模型","slug":"NLP/11-语言模型/0-语言模型","date":"2017-12-18T09:17:35.000Z","updated":"2018-11-13T10:13:35.720Z","comments":true,"path":"2017/12/18/NLP/11-语言模型/0-语言模型/","link":"","permalink":"http://yoursite.com/2017/12/18/NLP/11-语言模型/0-语言模型/","excerpt":"","text":"语言模型就是用于评估文本符合语言使用习惯程度的模型。 一、定义我们目前所说的语言模型主要指的是统计语言模型。统计语言模型是一个单词序列上的概率分布，对于一个给定长度为m的序列，它可以为整个序列产生一个概率 P(w1,w2,…,wm)。其实就是想办法找到一个概率分布，它可以表示任意一个句子或序列出现的概率。 二、统计语言模型对于一个由T个词按顺序构成的句子$s=w_1w_2w_3…w_t$，p(s)实际上求解的是字符串$w_1w_2w_3…w_t$的联合概率，利用贝叶斯公式，链式分解如下：$P(s)=P(w_1w_2w_3…w_t) =P(w_1)P(w_2|w_1)P(w_3|w_1w_2)P(w_T|w1…w{T-1})$从上面可以看到，一个统计语言模型可以表示成，给定前面的的词，求后面一个词出现的条件概率。我们在求p(s)时实际上就已经建立了一个模型，这里的p(*)就是模型的参数，如果这些参数已经求解得到，那么很容易就能够得到字符串s的概率。 由于上式中的参数过多，因此需要近似的计算方法。常见的方法有n-gram模型方法、决策树方法、最大熵模型方法、最大熵马尔科夫模型方法、条件随机域方法、神经网络方法，等等。 三、","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"},{"name":"11-语言模型","slug":"NLP/11-语言模型","permalink":"http://yoursite.com/categories/NLP/11-语言模型/"}],"tags":[]},{"title":"0-文本向量化","slug":"NLP/6-文本向量化/0-文本向量化","date":"2017-12-18T09:17:35.000Z","updated":"2018-11-13T12:45:56.226Z","comments":true,"path":"2017/12/18/NLP/6-文本向量化/0-文本向量化/","link":"","permalink":"http://yoursite.com/2017/12/18/NLP/6-文本向量化/0-文本向量化/","excerpt":"","text":"一、简介二、分类1.one-hot(将词向量化)这最简单的一种想法，就是对一句话用one-hot编码:比如对于这句话：1234ohn likes to watch movies,Mary likes too.John likes to watch movies,Mary likes too.John also likes to watch football games.John also likes to watch football games. 可以通过以上语料构建一个字典：dic123456789101:&quot;John&quot;2:&quot;likes&quot;3:&quot;to&quot;4:&quot;watch&quot;5:&quot;movies&quot;6:&quot;also&quot;7:&quot;football&quot;8:&quot;games&quot;9:&quot;Mary&quot;10:&quot;too&quot; 用one-hot可以表示为：第一句话：John :[1,0,0,0,0,0,0,0,0,0]likes :[0,1,0,0,0,0,0,0,0,0]…too :[0,0,0,0,0,0,0,0,0,1] 第二句话：John :[1,0,0,0,0,0,0,0,0,0]likes :[0,1,0,0,0,0,0,0,0,0]…too :[0,0,0,0,0,0,0,0,0,1]但事实是，这样做耗费的资源太多，而且不能很好的表征每句话的特性。 2.Bag-of-words(将句子向量化)另一种方法则是词袋模型，它相当于一个词袋，不考虑词/句之间的相关性，只要出现了该词，就会记为1，再次出现就会+1。比如前面的那句话：1John likes to watch movies,Mary likes too. 可以表示为[1,2,1,1,1,0,0,0,1,1] 与其相似的是binary weighting,它就是看一下每个词是否出现，出现记为1，没出现记为0[1,1,1,1,1,0,0,0,1,1] 3.TF-IDF(将句子向量化：考虑词的重要性)因此，就有tf-idf解决这个问题，它的主要思路就是有两方面：A—第一就是如果这个词在我们当前文档出现的频率非常高，说明它在当前文档应该是比较重要的。B-但如果它在所有的文档中出现的频次都非常好，大家可能就会觉得这个词应该不是那么重要的。 比如中文的“的“，或者我们上面那两个句子中的to.因此，tf-idf就是一个在当前文档和所有文档中权衡他们的重要性，然后计算出每个词的重要度的方法。 4.word2vec(词的维度固定)","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"},{"name":"6-文本向量化","slug":"NLP/6-文本向量化","permalink":"http://yoursite.com/categories/NLP/6-文本向量化/"}],"tags":[]},{"title":"这里是句法分析","slug":"NLP/2-句法分析/这里是句法分析","date":"2017-12-18T09:17:35.000Z","updated":"2018-08-12T08:48:45.815Z","comments":true,"path":"2017/12/18/NLP/2-句法分析/这里是句法分析/","link":"","permalink":"http://yoursite.com/2017/12/18/NLP/2-句法分析/这里是句法分析/","excerpt":"","text":"","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"},{"name":"2-句法分析","slug":"NLP/2-句法分析","permalink":"http://yoursite.com/categories/NLP/2-句法分析/"}],"tags":[]},{"title":"1.朴素贝叶斯算法","slug":"NLP/7-文本分类/1.朴素贝叶斯算法","date":"2017-12-18T09:17:35.000Z","updated":"2018-09-18T10:06:14.531Z","comments":true,"path":"2017/12/18/NLP/7-文本分类/1.朴素贝叶斯算法/","link":"","permalink":"http://yoursite.com/2017/12/18/NLP/7-文本分类/1.朴素贝叶斯算法/","excerpt":"","text":"一、基本概念1.分类原理通过某对象的先验概率，利用贝叶斯公式，计算出其后验概率。即该对象属于某一类的概率，选择具有最大后验概率的类作为该对象所属的类。 2.贝叶斯公式$$P(A|B) = \\frac{P(B|A)*P(A)}{P(B)}$$ (1).其中P(A)为先验概率：先验概率（prior probability）是指根据以往经验和分析得到的概率，如全概率公式，它往往作为”由因求果”问题中的”因”出现的概率。； (2).其中P(B|A)为似然概率(likelihood)：是先前统计的事件中，A事件发生情况下B事件发生的概率 (3).其中P(B)为边界似然概率； (4).其中P(A|B)为后验概率；3.相关概念 (1).先验概率 (2).—-- ①后验概率 - ②.最大后验概率 (3).— ①.条件概率：指事件A在另外一个事件B已经发生条件下的发生概率。条件概率表示为：P(A|B)。若只有两个事件A，B，那么$P(A|B) = \\frac{P(AB)}{P(B)}$ ②.联合概率：表示两个事件共同发生的概率。A与B的联合概率表示为 P(AB) 或者P(A,B),或者P（A∩B） ③.全概率 (4).似然概率 联合概率的乘法公式为：P(AB) = P(A|B)*P(B),变形后可得到$P(A|B) = \\frac{P(AB)}{P(B)}$ 二、朴素贝叶斯分类器1.朴素贝叶斯分类的定义朴素贝叶斯分类的正式定义如下：(1).设 $x = \\left{ f{1},f{2},f{3},…,f{m} \\right}$为一个待分类项，而每个f为x的一个特征。(2).有类别集合$C = \\left{ y{1},y{2},y{3},…,y{n} \\right}$(3).计算$P(y{1}|x),P(y{2}|x),….,P(y{n}|x)$(4).如果$P(y{k}|x) = max \\left{ P(y{1}|x),P(y{2}|x),….,P(y{n}|x) \\right}，则x∈y{k}$。那么现在的关键就是如何计算第3步中的各个条件概率。我们可以这么做：(1)、找到一个已知分类的待分类项集合，这个集合叫做训练样本集。(2)、统计得到在各类别下各个特征属性的条件概率估计。即$P(x{1}|y{1}),P(x{2}|y{1}),…,P(x{3}|y{1})$;$P(x{1}|y{2}),P(x{2}|y{2}),…,P(x{3}|y{2})$;……$P(x{1}|y{3}),P(x{2}|y{3}),…,P(x{3}|y{3})$;(3)、如果各个特征属性是条件独立的，则根据贝叶斯定理有如下推导：$P(y{i}|x) = \\frac {P(x|y{i})P(y{i})}{P(x)}$ 因为分母对于所有类别为常数，因为我们只要将分子最大化皆可。又因为各特征属性是条件独立的，所以有： $P(x|y{i})P(y{i}) = P(a{1}|y{i})P(a{2}|y{i})…P(a{m}|y{i})P(y{i})=P(y(i))\\prod\\limits{j=1}^mP(a_j|yi)$ 2.朴素贝叶斯分类的举例我们知道朴素贝叶斯的公式为：$P(C|f) = \\frac{P(f|C)P(C)}{P(f)}$如果换个表达式就会明朗很多，如下：$P(类别|特征) = \\frac{P(特征|分类)P(分类)}{P(特征)}$其中P(分类)和P(特征)都是已知的，我们只需求P(特征|分类)即可。 例题分析给定数据如下：| 高否 | 富否 | 帅否 | 嫁否 || – | — | – | – || 高 | 富 | 帅 | 嫁 || 高 | 不富 | 帅 | 嫁 || 不高 | 富 | 不帅 | 嫁 || 高 | 富 | 不帅 | 不嫁 || 不高 | 不富 | 帅 | 不嫁 || 高 | 不富 | 不帅 | 不嫁 || 不高 | 不富 | 不帅 | 不嫁 |那我们现在的问题是，一个男生向一个女生求婚，这个男生具有以下三个特点：不高、富、帅，请你判断以下该女孩是否会嫁？ 这是一个典型的分类问题，转化为概率论问题就是$P(嫁|不高、富、帅)$ 与 $P(不嫁|不高、富、帅)那个概率更大？ 这里我们就使用朴素贝叶斯公式来分别求以下两种情况下的概率： ①$P(嫁|不高、富、帅) = \\frac {P(不高、富、帅|嫁)*P(嫁)}{P(不高、富、帅)}$ ②$P(不嫁|不高、富、帅) = \\frac {P(不高、富、帅|不嫁)*P(不嫁)}{P(不高、富、帅)}$对①求解我们先对要求①进行求解。要求$P(嫁|不高、富、帅)$的概率只需求$P(不高、富、帅|嫁)、P(嫁)、P(不高、富、帅)$即可。根据“朴素”一词也就是各个特征之间是独立的，可以得到如下公式和公式(只需求如下公式即可)：$P(不高、富、帅|嫁) = P(不高|嫁)P(富|嫁)P(帅|嫁)$$P(不高、富、帅) = P(不高)P(富)P(帅)$同时只需要再求出公式问题就得到解决$P(嫁)$ 我们从表格中统计所有嫁的样本共有3条，其中不高的样本有1条，所以$P(不高|嫁) = 1/3$，同理可以得到$P(富|嫁) = 2/3$， $P(帅|嫁) = 2/3$。 我们从表格中统计所有样本共有7条，其中嫁的样本有3条，所以 $P(嫁) = 3/7$。 我们从表格中统计所有样本共有7条，其中不高的样本有3条，所以 $P(不高) = 3/7$，其中富的样本有3条所以$P(富) = 3/7$，其中帅的样本有3条所以$P(帅) = 3/7$。 综上①$P(嫁|不高、富、帅) = \\frac {P(不高、富、帅|嫁)P(嫁)}{ \\; P(不高、富、帅) \\; } $ 此公式 $= \\frac {(\\frac {1}{3} \\frac {2}{3}\\frac {2}{3})\\frac {3}{7}} {\\frac {3}{7} \\frac {3}{7}\\frac {3}{7}} =\\frac {196}{243}$ 对②求解$P(不嫁|不高、富、帅) = \\frac {P(不高、富、帅|不嫁)P(不嫁)}{P(不高、富、帅)}$此公式 $= \\frac {(\\frac {1}{2} \\frac {1}{4}\\frac {1}{4})\\frac {4}{7}} {\\frac {3}{7} \\frac {3}{7}\\frac {3}{7}} =\\frac {49}{216}$ 所以最终的答案是“嫁”参考：1.https://blog.csdn.net/xueyingxue001/article/details/523961702.https://blog.csdn.net/amds123/article/details/70173402","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"},{"name":"7-文本分类","slug":"NLP/7-文本分类","permalink":"http://yoursite.com/categories/NLP/7-文本分类/"}],"tags":[]},{"title":"这里是词法分析","slug":"NLP/1-词法分析/这里是词法分析","date":"2017-12-18T09:17:35.000Z","updated":"2018-08-12T08:48:26.854Z","comments":true,"path":"2017/12/18/NLP/1-词法分析/这里是词法分析/","link":"","permalink":"http://yoursite.com/2017/12/18/NLP/1-词法分析/这里是词法分析/","excerpt":"","text":"","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"},{"name":"1-词法分析","slug":"NLP/1-词法分析","permalink":"http://yoursite.com/categories/NLP/1-词法分析/"}],"tags":[]},{"title":"0-关键词提取的分类","slug":"NLP/1-词法分析/4-关键词提取/0-关键词提取的分类","date":"2017-12-18T09:17:35.000Z","updated":"2018-11-28T04:05:53.430Z","comments":true,"path":"2017/12/18/NLP/1-词法分析/4-关键词提取/0-关键词提取的分类/","link":"","permalink":"http://yoursite.com/2017/12/18/NLP/1-词法分析/4-关键词提取/0-关键词提取的分类/","excerpt":"","text":"目前,关键词抽取主要有两种方式:(1)基于规则的方法：关键词分配。即预先定义一个关键词词库,对于一篇文章,从词库中选取若干词语作为文章的关键词;(2)基于机器学习方法：关键词抽取。即从文章的内容中寻找一些词语作为推荐关键词 一、基于规则的方法(关键词分配):一般要求词库是某个或某些领域的专业词汇,或者看作是与一个或多个领域相关的专业词典.这些词典一般都是由专家手工编纂的,有质量保证,但费时费力,而词典的大小和覆盖度决定了关键词分配的范围和效果.当切换到一个新的领域时,又需要重新构建词典,无法满足如今网络时代的大规模应用和推广需求. 二、基于机器学习方法(关键词抽取):对于关键词抽取,大致可分为无监督方法和有监督方法。 1.无监督方法无监督方法会利用 TFIDF等统计信息,选取topK 作为关键词.这些方法无需人工标注训练集合的过程,因此更加快捷,但无法有效地综合利用词法和语义信息对候选关键词进行排序. – (1).使用外部的知识库 1).TF-IDFTF-IDF关键词提取算法就是需要保存每个词的IDF值作为外部知识库 2).LDA模型 – (2).不适用外部知识库 1).TextRank 2).PageRank 2.有监督方法中将关键词抽取问题转换为判断每个候选关键词是否为关键词的二分类问题,它需要一个已经标注关键词的文档集合来训练分类模型,目标是在一个有标注的数据集上训练一个分类器,以便决定候选词中哪些是关键词.不同的机器学习算法可以训练出这样一个分类器,如 贝 叶 斯 算 法、决 策 树 算 法、bagging、boosting、最大熵算法、多层感知机和 SVM 算法.但是,把关键词抽取问题看作分类问题存在一些问题,最主要的问题是它对每个候选词进行单独处理,忽略了文本中句子结构的有效信息,造成模型分类的性能较差. 3.基于图模型(序列标注)基于分类思想解决此任务的不足,另外一种思路是将关键词自动抽取任务转化为序列标注问题来解决.传统的最常用的解决序列标注问题的方案是隐马尔可夫(Hidden MarkovModel,HMM)、最大熵(Maximum Entropy,ME)和条件随机场(ConditionalRandomFields,CRF)等模型.其中 CRF是目前解决序列标注问题最主流的做法,性能也最好,目前已被广泛应用于 NLP的各种任务中,如分词、词性标注、命名实体 识 别 等,并 且 取 得 了 非 常 好 的 效 果.基","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"},{"name":"1-词法分析","slug":"NLP/1-词法分析","permalink":"http://yoursite.com/categories/NLP/1-词法分析/"},{"name":"4-关键词提取","slug":"NLP/1-词法分析/4-关键词提取","permalink":"http://yoursite.com/categories/NLP/1-词法分析/4-关键词提取/"}],"tags":[]},{"title":"0文本分类算法总结","slug":"NLP/7-文本分类/0.文本分类算法总结","date":"2017-12-18T09:17:35.000Z","updated":"2018-09-18T10:05:46.154Z","comments":true,"path":"2017/12/18/NLP/7-文本分类/0.文本分类算法总结/","link":"","permalink":"http://yoursite.com/2017/12/18/NLP/7-文本分类/0.文本分类算法总结/","excerpt":"","text":"","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"},{"name":"7-文本分类","slug":"NLP/7-文本分类","permalink":"http://yoursite.com/categories/NLP/7-文本分类/"}],"tags":[]},{"title":"word2ve源码解析(python版gensim实现)","slug":"NLP/6-文本向量化/2.1-word2ve源码解析(python版gensim实现)","date":"2017-10-12T04:18:15.000Z","updated":"2018-11-27T03:13:23.258Z","comments":true,"path":"2017/10/12/NLP/6-文本向量化/2.1-word2ve源码解析(python版gensim实现)/","link":"","permalink":"http://yoursite.com/2017/10/12/NLP/6-文本向量化/2.1-word2ve源码解析(python版gensim实现)/","excerpt":"","text":"————————–base_any2vec.py——————- class BaseAny2VecModel() def init(): def _worker_loop(): def _job_producer(): def _log_epoch_progress(): def _train_epoch(): #在这里才是真正的训练 def train(): for cur_epoch in range(self.epochs): self._train_epoch() def load(): return super(BaseAny2VecModel, cls).load() def save(): super(BaseAny2VecModel, self).save() class BaseWordEmbeddingsModel(BaseAny2VecModel) def __init__(): super(BaseWordEmbeddingsModel, self).__init__() #开始建立词典1 self.build_vocab(sentences) #开始训练 self.train() def build_vocab(): total_words, corpus_count = self.vocabulary.scan_vocab() #开始构建词典2 report_values = self.vocabulary.prepare_vocab() #开始构建网络模型 self.trainables.prepare_weights() def build_vocab_from_freq(): def estimate_memory(): def train(): return super(BaseWordEmbeddingsModel, self).train() def load(): def similarity(): def most_similar(): ##———————–word2vec.py———————– #word2vec 共有三个类：Word2Vec 、Word2VecVocab、Word2VecTrainablesdef train_batch_sg()def train_batch_cbow()def score_sentence_sg()def score_sentence_cbow()def train_sg_pair()def train_cbow_pair()def score_sg_pair()def score_cbow_pair() class Word2Vec(BaseWordEmbeddingsModel) def __init__(): #wv: 是类 ~gensim.models.keyedvectors.Word2VecKeyedVectors生产的对象，在word2vec是一个属性 为了在不同的训练算法（Word2Vec，Fastext，WordRank，VarEmbed）之间共享单词向量查询代码，gensim将单词向量的存储和查询分离为一个单独的类 KeyedVectors包含单词和对应向量的映射。可以通过它进行词向量的查询 #This object essentially contains the mapping between words and embeddings. self.wv = Word2VecKeyedVectors(size) #词汇表 self.vocabulary = Word2VecVocab() #网络模型初始化 self.trainables = Word2VecTrainables() super(Word2Vec, self).__init__() def _do_train_job(): def _clear_post_train(self): def _set_train_params(): def train(): return super(Word2Vec, self).train() def score(): #非主要 def clear_sims(self): #非主要 def intersect_word2vec_format(): def predict_output_word(): def init_sims(): def reset_from(): #非主要 def log_accuracy() #非主要 def accuracy(): def delete_temporary_training_data(): #保存模型 def save(): def get_latest_training_loss(): def load_word2vec_format(): def save_word2vec_format(): #加载模型 def load(): #词库的构建：huffman树的构建class Word2VecVocab() def init(): #该方法是从句子序列中构建单词表 def scan_vocab(): def sort_vocab(): #开始构建词典3 def prepare_vocab(): #如果是层次softmax构建霍夫曼树 if hs: self.create_binary_tree(wv) #如果是负采样 if negative: self.make_cum_table(wv) #使用存储的词汇单词及其词频创建一个二叉霍夫曼树 def create_binary_tree(): def add_null_word(): def make_cum_table(): #训练词向量的内部浅层神经网络class Word2VecTrainables(): def init(): def prepare_weights(): def seeded_vector(): def reset_weights(): def update_weights(): 参考资料：[1]https://blog.csdn.net/zynash2/article/details/81636338[2]https://blog.csdn.net/u012416045/article/details/78237060[3]https://blog.csdn.net/google19890102/article/details/51887344","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"},{"name":"6-文本向量化","slug":"NLP/6-文本向量化","permalink":"http://yoursite.com/categories/NLP/6-文本向量化/"}],"tags":[]},{"title":"0-word2vec","slug":"NLP/6-文本向量化/1-NNLP(神经网络模型)训练的词向量","date":"2017-10-12T04:18:15.000Z","updated":"2018-11-13T12:50:14.912Z","comments":true,"path":"2017/10/12/NLP/6-文本向量化/1-NNLP(神经网络模型)训练的词向量/","link":"","permalink":"http://yoursite.com/2017/10/12/NLP/6-文本向量化/1-NNLP(神经网络模型)训练的词向量/","excerpt":"","text":"","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"},{"name":"6-文本向量化","slug":"NLP/6-文本向量化","permalink":"http://yoursite.com/categories/NLP/6-文本向量化/"}],"tags":[]},{"title":"0-word2vec","slug":"NLP/6-文本向量化/2.0-word2vec","date":"2017-10-12T04:18:15.000Z","updated":"2018-11-13T12:50:14.912Z","comments":true,"path":"2017/10/12/NLP/6-文本向量化/2.0-word2vec/","link":"","permalink":"http://yoursite.com/2017/10/12/NLP/6-文本向量化/2.0-word2vec/","excerpt":"","text":"","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"},{"name":"6-文本向量化","slug":"NLP/6-文本向量化","permalink":"http://yoursite.com/categories/NLP/6-文本向量化/"}],"tags":[]},{"title":"3-层次聚类法之CANOPY","slug":"NLP/8-文本聚类/3-层次聚类法之CANOPY","date":"2017-05-18T09:17:35.000Z","updated":"2018-09-25T10:45:58.395Z","comments":true,"path":"2017/05/18/NLP/8-文本聚类/3-层次聚类法之CANOPY/","link":"","permalink":"http://yoursite.com/2017/05/18/NLP/8-文本聚类/3-层次聚类法之CANOPY/","excerpt":"","text":"","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"},{"name":"8-文本聚类","slug":"NLP/8-文本聚类","permalink":"http://yoursite.com/categories/NLP/8-文本聚类/"}],"tags":[]},{"title":"2-密度聚类法之DBSCAN","slug":"NLP/8-文本聚类/2-密度聚类法之DBSCAN","date":"2017-03-18T09:17:35.000Z","updated":"2018-09-25T10:45:53.121Z","comments":true,"path":"2017/03/18/NLP/8-文本聚类/2-密度聚类法之DBSCAN/","link":"","permalink":"http://yoursite.com/2017/03/18/NLP/8-文本聚类/2-密度聚类法之DBSCAN/","excerpt":"","text":"","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"},{"name":"8-文本聚类","slug":"NLP/8-文本聚类","permalink":"http://yoursite.com/categories/NLP/8-文本聚类/"}],"tags":[]},{"title":"1-划分聚类法之k-means","slug":"NLP/8-文本聚类/1-划分聚类法之k-means","date":"2017-01-18T09:17:35.000Z","updated":"2018-09-25T10:45:46.632Z","comments":true,"path":"2017/01/18/NLP/8-文本聚类/1-划分聚类法之k-means/","link":"","permalink":"http://yoursite.com/2017/01/18/NLP/8-文本聚类/1-划分聚类法之k-means/","excerpt":"","text":"","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"},{"name":"8-文本聚类","slug":"NLP/8-文本聚类","permalink":"http://yoursite.com/categories/NLP/8-文本聚类/"}],"tags":[]},{"title":"deeplearning算法分类","slug":"DeepLearning/deeplearning","date":"2017-01-09T08:02:27.000Z","updated":"2019-04-28T08:03:34.607Z","comments":true,"path":"2017/01/09/DeepLearning/deeplearning/","link":"","permalink":"http://yoursite.com/2017/01/09/DeepLearning/deeplearning/","excerpt":"","text":"一、常用神经网络分类1.Perceptron(感知机,神经网络的前身)2.前向传播算法3.BP(反向传播算法)4.DNN(深度神经网络)5.CNN(卷积神经网络)6.RNN(循环神经网络)7.LSTM(..神经网络)1.深度学习模型分类概述深度神经网络常用的参数更新算法为：前向传播算法(fp)，反向传播算法(BP)。 1.1感知器1.2DNN1.3CNN1.4RNN1.5Seq2Seq2深度学习技术深度学习是机器学习的分支，是试图使用包含复杂结构或由多重非线性变换构成的多处理层计算模型对数据进行高层抽象的一类算法。深度学习技术已被广泛应用到图像处理、语音处理、自然语言处理等多个领域，取得了重大突破[26]。 2.1神经网络语言模型语言模型（LanguageModel，LM）把语料库当作一个随机变量，对给定前面的词语预测下一个词语的任务建模来计算句子概率。神经网络语言模型（NeuralNetworkLanguageModel，NNLM）最早由Bengio等人[27]提出，其核心思路是用一个K维的向量来表示词语，被称为词向量（WordEmbedding），使得语义相似的词在向量空间中处于相近的位置，并基于神经网络模型将输入的上下文词向量序列转换成成固定长度的上下文隐藏向量，使得语言模型不必存储所有不同词语的排列组合信息，从而改进传统语言模型受词典规模限制的不足。 2.2自编码器自编码器（Autoencoder,AE）是一种无监督的学习模型，由Rumelhart等人[28]最早提出。自编码器由编码器和解码器两部分组成，先用编码器对输入数据进行压缩，将高维数据映射到低维空间，再用解码器解压缩，对输入数据进行还原，从而来实现输入到输出的复现。如图2所示，自编码器的训练目标是，使得输出X尽可能地还原输入X。其中，编码器和解码器基于神经网络构建。为了改进基本模型中容易陷入局部最优的情 况，深度自编码器模型被提出[29, 30]。其中，变分自 编码器和条件变分自编码器被用到开放领域的对 话系统中，对回复生成的多样性进行控制，示意图 如图 3。变分自编码器[31]（Variational Auto-Encoder， VAE）是一种生成模型，它引入统计思想在基础的 自编码器模型基础上加入正则约束项，使得隐层 z 满足某个分布，并从 z 中自动生成数据。条件变分 自编码器[31]（Conditional Variational Auto-Encoders， CVAE）是在变分自编码器之上再加一些额外信息 为条件的一类模型。其模型训练和测试时候均以该 额外信息 c 为条件。 2.3 卷积神经网络卷积神经网络（Convolutional Neural Network， CNN）是人工神经网络的一种。其核心思想是设计 局部特征抽取器运用到全局，利用空间相对关系共 享参数，来提高训练性能。早期主要运用于图像处 理领域，后来被应用到自然语言处理中[32, 33]。 卷积层和池化层是卷积神经网络的重要组成部 分。其中，卷积层的作用是从固定大小的窗口中读 取输入层数据，经过卷积计算，实现特征提取。卷 积神经网络在同一层共享卷积计算模型来控制参 数规模，降低模型复杂度。池化层的作用是对特征 信号进行抽象，用于缩减输入数据的规模，按一定 方法将特征压缩。池化的方法包括加和池化、最大 池化、均值池化、最小值池化和随机池化。最后一 个池化层通常连接到全连接层，来计算最终的输 出。 图 4 卷积神经网络应用示意图 不同研究中的卷积网络模型会有细微差别。以 文本表示为例，卷积神经网络在能从文本中提取隐 藏特征，形成低维向量表示。如图 4 所示，模型利 用局部特征抽取器通过滑动窗口获取变长序列的 隐藏特征，并经过池化得到定长输出。 2.4 循环神经网络 循环神经网络（Recurrent Neural Network， RNN）是专门设计用于处理序列数据的神经网络架 构，它利用时间相对关系减少参数数目以提高训练 性能，已经成功的用于自然语言处理中[34]。 图 5 循环神经网络模型示意图 循环神经网络具有自环的网络结构。一个简单 的循环神经网络如图 5 所示，左边为压缩表示，右 边是按时间展开的表示。其中，自环的网络对前面 的信息进行记忆并应用于当前输出的计算中，即当 前时刻隐藏层的输入包括输入层变量和上一时刻 的隐藏层变量。由于可以无限循环，所以理论上循 环神经网络能够对任何长度的序列数据进行处理。 循环神经网络在实际应用时有梯度消失等问题。后 续研究针对该问题提出带存储单元的循环神经网 络长短时记忆网络（Long Short-Term Memory， LSTM）[35]和门控循环单元（Gated Recurrent Unit， GRU）[36]。 循环神经网络在开放领域对话系统中可用于文 本表示，即将词向量按词语在文本中的顺序逐个输 入到网络中，末节点的隐藏向量可以作为该话语的 陈晨等：基于深度学习的开放领域对话系统研究综述 5 语义向量表示。随着技术的发展，其扩展模型双向 循环神经网络（Bi-LSTM）、密集循环神经网络 （DenseRNN）等都相继被引入到开放领域对话系 统中。 2.5 序列到序列模型 序列到序列（Sequence to Sequence，Seq2Seq） 模型在 2014 年被 Cho 和 Sutskever 先后提出，前者 将该模型命名为编码器 - 解码器模型 （Encoder-Decoder Model）[37]，后者将其命名为序 列到序列模型[38]。两者有一些细节上的差异，但总 体思想基本相同。具体来说，序列到序列模型就是 输入一个序列，输出另一个序列，它是一个通用的 框架，适用于各种序列的生成任务。其基本模型利 用两个循环神经网络：一个循环神经网络作为编码 器，将输入序列转换成定长的向量，将向量视为输 入序列的语义表示；另一个循环神经网络作为解码 器，根据输入序列的语义表示生成输出序列，如图 6 所示。 图 6 序列到序列模型示意图 层次序列到序列模型在序列到序列模型基础上 定义了多层结构的编码器。首先，每个句子将其包 含的词序列向量表示输入循环神经网络得到该句 子的向量表示；然后，每个段落将其包含的句子序 列向量表示输入另一个循环神经网络得到该段落 向量表示。 2.6 注意力机制 通用的序列到序列模型，只使用到编码器的最 终状态来初始化解码器的初始状态，导致编码器无 法学习到句子内的长期依赖关系，同时解码器隐藏 变量会随着不断预测出的新词，稀释源输入句子的 影响。为了解决这个问题，Bahdanau 等人[39]提出 了注意力机制（Attention Mechanism）。注意力机制 可以理解为回溯策略。它在当前解码时刻，将解码 器 RNN 前一个时刻的隐藏向量与输入序列关联起 来，计算输入的每一步对当前解码的影响程度作为 权重，如图 7 所示。其中，前一时刻隐藏向量和输 入序列的关联方式有点乘[39]、向量级联方法[14]等。 最后，通过 softmax 函数归一化，得到概率分布权 重对输入序列做加权，重点考虑输入数据中对当前 解码影响最大的输入词。 图 7 注意力机制模型示意图 随着研究的深入，Vaswani 等人[40]将注意力机 制定义为一个查询到一组键值对的映射过程，并提 出了自注意力机制（Self-Attention），即其中的查询、 键、值是同一个句子，减少对外部信息的依赖，捕 捉数据内部的相关性。如图 8 所示，V、K、Q 分别 代表值、键、查询，若模型为自注意力则 V=K=Q。 Vaswani 等人[40]还提出了多头注意力机制，即分多 次计算注意力，在不同的表示子空间学习信息。多 头注意力机制先对输入做划分，依次经过线性变换 和点积后再拼接作为输出。 图 8 多头注意力机制模型示意图 2.7 记忆网络 记忆网络（Memory Network）[41]是指通过在外 部存储器模块中存储重要信息来增强神经网络的 一类模型。外部存储器模块具有内容可读写，信息 可检索和重用的特点。Sukhbaatar 等人[42]提出了一 个用于问答键值存储的端到端记忆网络架构 （End-to-End Memory Networks，MemN2N）。其中 外部存储器以键-值对结构存储问答知识，可以检索 与输入相关的信息，得到相关度权值，然后获取将 对应的值加权求和作为输出。如图 9 所示， 计算机学报 6 计 算 机 学 报 2019 年 图 9 端到端记忆网络模型示意图 相对于其他的神经网络模型，记忆网络的外部 存储器可以构建具有长期记忆的模块（如知识库、 历史对话信息等）来增强神经模型。 2.8 生成对抗网络 生 成 对 抗 网 络 （ Generative Adversarial Networks，GAN,）[43]是 Goodfellow Ian 于 2014 年 提出的一种深度学习模型。它包含两个模块：生成 模型和判别模型。生成模型的训练目标是生成与训 练集中真实数据相似的数据。判别模型是一个二分 类器，用来判断这个样本是真实训练样本，还是生 成模型生成的样本，其训练目标是尽可能地区分真 实数据和生成数据。如图 10 所示，G 代表生成模 型，D 代表判别模型。 图 10 生成对抗网络模型示意图 GAN 最早被用在图像处理领域，后来也被用到 自然语言处理领域中。与图灵测试的思想类似，在 开放领域对话系统中使用生成对抗网络的目标是 生成与人类回复无差别的回复。 2.9 强化学习 机器学习按学习范式可以分为有监督学习、无 监督学习和强化学习。强化学习(Reinforcement Learning)[44]是指智能体通过和环境交互，序列化地 做出决策和采取动作，并获得奖赏指导行为的学习 机制。 经典的强化学习建模框架如图 11 所示：在每 个时刻 t，智能体接收一个观察值 ot，收到一个奖 励值 rt，并执行一个动作 at；从环境的角度，它接 收智能体动作 at，给出下一个时刻观察值 ot+1 及对 应的奖赏 rt+1。由此，观察值、动作和奖赏一起构 成的序列就是智能体获得的经验数据，智能体的目 标则是依据经验获取最大累计奖励。 图 11 强化学习系统模型示意图 近年，深度强化学习的诞生打破早期强化学习 模型不稳定难收敛的瓶颈，在人机博弈、无人驾驶、 视频游戏等很多任务上取得很好的效果。深度强化 学习的发展主要有两种路线：一种是以 DQN（Deep Q-Leraning）[45]为代表的算法；另一种是策略梯度 方法（Policy Gradient Methods）[46]。策略梯度方法 通过梯度下降来学习预期奖励的策略参数，将策略 搜索转化成优化问题，并根据目标函数最优值确定 最优策略。相比而言，策略梯度方法更适合在自然 语言处理领域应用。","categories":[{"name":"DeepLearning","slug":"DeepLearning","permalink":"http://yoursite.com/categories/DeepLearning/"}],"tags":[]},{"title":"RNN算法","slug":"DeepLearning/3-RNN/RNN","date":"2017-01-09T08:02:27.000Z","updated":"2018-09-29T06:30:25.521Z","comments":true,"path":"2017/01/09/DeepLearning/3-RNN/RNN/","link":"","permalink":"http://yoursite.com/2017/01/09/DeepLearning/3-RNN/RNN/","excerpt":"","text":"","categories":[{"name":"DeepLearning","slug":"DeepLearning","permalink":"http://yoursite.com/categories/DeepLearning/"},{"name":"3-RNN","slug":"DeepLearning/3-RNN","permalink":"http://yoursite.com/categories/DeepLearning/3-RNN/"}],"tags":[]},{"title":"CNN算法","slug":"DeepLearning/2-CNN/CNN","date":"2017-01-09T08:02:27.000Z","updated":"2018-09-20T06:12:27.399Z","comments":true,"path":"2017/01/09/DeepLearning/2-CNN/CNN/","link":"","permalink":"http://yoursite.com/2017/01/09/DeepLearning/2-CNN/CNN/","excerpt":"","text":"二、学习资料 Michael Nielsen的《Neural Network and Deep Learning》中文翻译《神经网络与深度学习》","categories":[{"name":"DeepLearning","slug":"DeepLearning","permalink":"http://yoursite.com/categories/DeepLearning/"},{"name":"2-CNN","slug":"DeepLearning/2-CNN","permalink":"http://yoursite.com/categories/DeepLearning/2-CNN/"}],"tags":[]},{"title":"CNN算法","slug":"DeepLearning/2-CNN/CNN算法之文本分类","date":"2017-01-09T08:02:27.000Z","updated":"2018-09-20T06:12:27.399Z","comments":true,"path":"2017/01/09/DeepLearning/2-CNN/CNN算法之文本分类/","link":"","permalink":"http://yoursite.com/2017/01/09/DeepLearning/2-CNN/CNN算法之文本分类/","excerpt":"","text":"二、学习资料 Michael Nielsen的《Neural Network and Deep Learning》中文翻译《神经网络与深度学习》","categories":[{"name":"DeepLearning","slug":"DeepLearning","permalink":"http://yoursite.com/categories/DeepLearning/"},{"name":"2-CNN","slug":"DeepLearning/2-CNN","permalink":"http://yoursite.com/categories/DeepLearning/2-CNN/"}],"tags":[]},{"title":"感知机(感知器)算法","slug":"DeepLearning/1-Perceptron/多层感知机(MLP)","date":"2017-01-09T08:02:27.000Z","updated":"2019-04-28T08:09:30.873Z","comments":true,"path":"2017/01/09/DeepLearning/1-Perceptron/多层感知机(MLP)/","link":"","permalink":"http://yoursite.com/2017/01/09/DeepLearning/1-Perceptron/多层感知机(MLP)/","excerpt":"","text":"一、深度学习1.基本定义在现在的人工智能领域，数据挖掘类的算法大致可以分为两类，第一种是机器学习算法，第二种是深度学习算法。深度学习算法，可以叫做深度神经网络算法。神经网络的结构可以如下图所示：上图中每个圆圈都是一个神经元，每条线叫神经元的链接。上图中神经元可以分为三层，层与层直接有神经元连接，层内无神经元连接。最左边的层叫输入层。最右边的层叫输出层。输入层和输出层直接的叫隐藏层。深度神经网络：隐藏层大于2的神经网络。 深度网络和宽度网络的区别：(1)一个仅有一个隐藏层的神经网络就能拟合任何一个函数，但是它需要很多很多的神经元。(2)而深层网络用少得多的神经元就能拟合同样的函数。也就是为了拟合一个函数，要么使用一个浅而宽的网络，要么使用一个深而窄的网络。而后者往往更节约资源。 二、感知器(也叫感知机)1.感知器的基本定义感知器： 也叫感知机，也叫神经元，神经网络的基本组成单元。感知器的定义：可以看到，一个感知器有如下组成部分： 输入权值： 一个感知器可以接收多个输入$(x_1,x_2,…,x_n)$，每个输入上有一个权值，此外还有一个偏置项$b$，就是上图中的$w_0$。 激活函数： 感知器的激活函数可以有很多选择，比如我们可以选择下面这个阶跃函数来作为激活函数$f(x)=\\begin{cases}1&amp; \\text{z&gt;0}\\0&amp; \\text{otherwise}\\end{cases}$ 输出： 感知器的输出由下面的公式来计算$y =f(w*x +b)$ 2.感知器的训练前面的权重项和偏置项的值是如何获得的呢？这就要用到感知器训练算法：将权重项和偏置项初始化为0，然后，利用下面的感知器规则迭代的修改$w_i$和$b$，直到训练完成。$w_i = w_i + \\nabla w_i$ （公式1）$b = b+\\nabla b$ （公式2） 其中：$\\nabla w_i = \\alpha(t-y)x_i$$\\nabla b= \\alpha(t-y)$ $w_i$是输入$x_i$对应的权重项。$b$是偏置项。$t$是训练样本的实际值，也就是label。$y$是感知器的输出值。$\\alpha$是学习率的常数。","categories":[{"name":"DeepLearning","slug":"DeepLearning","permalink":"http://yoursite.com/categories/DeepLearning/"},{"name":"1-Perceptron","slug":"DeepLearning/1-Perceptron","permalink":"http://yoursite.com/categories/DeepLearning/1-Perceptron/"}],"tags":[]},{"title":"1-联合概率","slug":"数学/统计学/1-联合概率","date":"2016-12-18T09:17:35.000Z","updated":"2018-11-01T08:49:14.375Z","comments":true,"path":"2016/12/18/数学/统计学/1-联合概率/","link":"","permalink":"http://yoursite.com/2016/12/18/数学/统计学/1-联合概率/","excerpt":"","text":"一、定义联合概率是指在多元的概率分布中多个随机变量分别满足各自条件的概率。假设X和Y都服从正态分布，那么P{X&lt;4,Y&lt;0}就是一个联合概率，表示X&lt;4,Y&lt;0两个条件同时成立的概率。表示两个事件共同发生的概率。A与B的联合概率表示为 P(AB) 或者P(A,B),或者P（A∩B）。 二、公式$P(AB) = P(A)*P(B|A)$ $P(AB) = P(B)*P(A|B)$以上公式还可以变形为： $P(B|A) = \\frac {P(A|B)*P(B)}{P(A)}$ $P(A|B) = \\frac {P(B|A)*P(A)}{P(B)}$","categories":[{"name":"数学","slug":"数学","permalink":"http://yoursite.com/categories/数学/"},{"name":"统计学","slug":"数学/统计学","permalink":"http://yoursite.com/categories/数学/统计学/"}],"tags":[]},{"title":"0-条件概率","slug":"数学/统计学/0-条件概率","date":"2016-12-18T09:17:35.000Z","updated":"2018-10-08T06:07:42.683Z","comments":true,"path":"2016/12/18/数学/统计学/0-条件概率/","link":"","permalink":"http://yoursite.com/2016/12/18/数学/统计学/0-条件概率/","excerpt":"","text":"一、定义条件概率是指事件A在另外一个事件B已经发生条件下的发生概率。条件概率表示为：P（A|B），读作“在B的条件下A的概率”。若只有两个事件A，B，那么， $P(A|B) = \\frac {P(AB)}{P(B)}$。 二、公式$P(A|B) = \\frac {P(AB)}{P(B)}$根据贝叶斯公式条件概率还可以是$P(A|B) = \\frac {P(B|A)*P(A)}{P(B)}$","categories":[{"name":"数学","slug":"数学","permalink":"http://yoursite.com/categories/数学/"},{"name":"统计学","slug":"数学/统计学","permalink":"http://yoursite.com/categories/数学/统计学/"}],"tags":[]},{"title":"0-文本聚类算法总结","slug":"NLP/8-文本聚类/0-文本聚类算法总结","date":"2016-12-18T09:17:35.000Z","updated":"2018-09-25T10:45:35.496Z","comments":true,"path":"2016/12/18/NLP/8-文本聚类/0-文本聚类算法总结/","link":"","permalink":"http://yoursite.com/2016/12/18/NLP/8-文本聚类/0-文本聚类算法总结/","excerpt":"","text":"一、文本聚类算法总结1.划分法(partitioning methods)：给定一个有N个元组或者纪录的数据集，分裂法将构造K个分组，每一个分组就代表一个聚类，K&lt;N。而且这K个分组满足下列条件：（1） 每一个分组至少包含一个数据纪录；（2）每一个数据纪录属于且仅属于一个分组（注意：这个要求在某些模糊聚类算法中可以放宽）；对于给定的K，算法首先给出一个初始的分组方法，以后通过反复迭代的方法改变分组，使得每一次改进之后的分组方案都较前一次好，而所谓好的标准就是：同一分组中的记录越近越好，而不同分组中的纪录越远越好。使用这个基本思想的算法有：K-MEANS算法、K-MEDOIDS算法、CLARANS算法； 2.层次法2.1层次发的总结(hierarchical methods)：这种方法对给定的数据集进行层次似的分解，直到某种条件满足为止。具体又可分为“自底向上”和“自顶向下”两种方案。例如在“自底向上”方案中，初始时每一个数据纪录都组成一个单独的组，在接下来的迭代中，它把那些相互邻近的组合并成一个组，直到所有的记录组成一个分组或者某个条件满足为止。代表算法有：CANOPY算法、BIRCH算法、CURE算法、CHAMELEON算法等； 2.2层次法的优点(1).距离和规则的相似度容易定义，限制少(2).不需要预先定制聚类书(3).可以发现类的层次关系(4).可以聚类成其他形状 2.3层次法的缺点(1).计算复杂度太高(2).奇异值也能产生很大影响(3).算法可能聚成链状 3.基于密度的方法（density-based methods):基于密度的方法与其它方法的一个根本区别是：它不是基于各种各样的距离的，而是基于密度的。这样就能克服基于距离的算法只能发现“类圆形”的聚类的缺点。这个方法的指导思想就是，只要一个区域中的点的密度大过某个阀值，就把它加到与之相近的聚类中去。代表算法有：DBSCAN算法、OPTICS算法、DENCLUE算法等； 4.基于网格的方法(grid-based methods):这种方法首先将数据空间划分成为有限个单元（cell）的网格结构,所有的处理都是以单个的单元为对象的。这么处理的一个突出的优点就是处理速度很快，通常这是与目标数据库中记录的个数无关的，它只与把数据空间分为多少个单元有关。代表算法有：STING算法、CLIQUE算法、WAVE-CLUSTER算法； 5.基于模型的方法(model-based methods):基于模型的方法给每一个聚类假定一个模型，然后去寻找一个能很好的满足这个模型的数据集。这样一个模型可能是数据点在空间中的密度分布函数或者其它。它的一个潜在的假定就是：目标数据集是由一系列的概率分布所决定的。通常有两种尝试方向：统计的方案和神经网络的方案 参考：[1].https://wenku.baidu.com/view/8713cffc581b6bd97e19eaea.html","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"},{"name":"8-文本聚类","slug":"NLP/8-文本聚类","permalink":"http://yoursite.com/categories/NLP/8-文本聚类/"}],"tags":[]},{"title":"0-文本特征提取算法总结","slug":"NLP/9-特征提取/0-文本特征提取算法总结","date":"2016-01-18T05:17:35.000Z","updated":"2018-10-08T06:09:59.092Z","comments":true,"path":"2016/01/18/NLP/9-特征提取/0-文本特征提取算法总结/","link":"","permalink":"http://yoursite.com/2016/01/18/NLP/9-特征提取/0-文本特征提取算法总结/","excerpt":"","text":"为分类文本作处理的特征提取算法也对最终效果有巨大影响，而特征提取算法又分为特征选择和特征抽取两大类，其中特征选择算法有互信息，文档频率，信息增益，开方检验等等十数种","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"},{"name":"9-特征提取","slug":"NLP/9-特征提取","permalink":"http://yoursite.com/categories/NLP/9-特征提取/"}],"tags":[]},{"title":"1-TF-IDF算法","slug":"NLP/9-特征提取/1-TF-IDF","date":"2016-01-18T05:17:35.000Z","updated":"2018-10-08T06:10:07.347Z","comments":true,"path":"2016/01/18/NLP/9-特征提取/1-TF-IDF/","link":"","permalink":"http://yoursite.com/2016/01/18/NLP/9-特征提取/1-TF-IDF/","excerpt":"","text":"TF-IDF算法可以分解为两部分：TF和IDF 一、TF算法1.词频（term frequency，TF）2.公式：$TF =\\frac {N{i,j}}{ \\sum{k}N_{k,j}}$3.解释：以上式子中分子是该词在文件中的出现次数，而分母则是在文件中所有字词的出现次数之和。 二、IDF算法1.逆向文件频率（inverse document frequency，IDF）2.公式：$IDF =lg \\frac {|D|}{|j:t_i ∈d_j|}$3.解释：逆向文件频率（inverse document frequency，IDF）是一个词语普遍重要性的度量。某一特定词语的IDF，可以由总文件数目除以包含该词语之文件的数目，再将得到的商取以10为底的对数得到","categories":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/categories/NLP/"},{"name":"9-特征提取","slug":"NLP/9-特征提取","permalink":"http://yoursite.com/categories/NLP/9-特征提取/"}],"tags":[]}]}