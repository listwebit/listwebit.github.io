<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>帅的掉渣的博客</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2019-02-18T10:28:49.751Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>listwebit</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title></title>
    <link href="http://yoursite.com/2018/08/10/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0NLP%E7%AE%97%E6%B3%95%E5%88%86%E7%B1%BB%E6%80%BB%E7%BB%93/"/>
    <id>http://yoursite.com/2018/08/10/人工智能机器学习NLP算法分类总结/</id>
    <published>2018-08-10T02:27:01.714Z</published>
    <updated>2019-02-18T10:28:49.751Z</updated>
    
    <content type="html"><![CDATA[<h2 id="第一章、目录"><a href="#第一章、目录" class="headerlink" title="第一章、目录"></a>第一章、目录</h2><p> 人工智能算法大体上来说可以分类两类：基于统计的机器学习算法(Machine Learning)和深度学习算法(Deep Learning)</p><h3 id="1-机器学习算法"><a href="#1-机器学习算法" class="headerlink" title="1. 机器学习算法"></a>1. 机器学习算法</h3><ul><li>(1).回归算法</li><li>(2).分类算法</li><li>(3).聚类算法</li><li>(4)降维算法</li><li>(5)概率图模型算法</li><li>(6)文本挖掘算法</li><li>(7)优化算法<h3 id="2-深度学习算法"><a href="#2-深度学习算法" class="headerlink" title="2.深度学习算法"></a>2.深度学习算法</h3></li></ul><h3 id="3-分布式计算"><a href="#3-分布式计算" class="headerlink" title="3.分布式计算"></a>3.分布式计算</h3><h3 id="4-数据结构"><a href="#4-数据结构" class="headerlink" title="4.数据结构"></a>4.数据结构</h3><h3 id="5-其他方面"><a href="#5-其他方面" class="headerlink" title="5.其他方面"></a>5.其他方面</h3><ul><li>5.1模型方面<ul><li>(1).模型优化</li><li>(2).数据预处理</li><li>5.2应用方面</li><li>对话系统</li><li>推线系统</li><li>知识图谱 </li></ul></li></ul><h2 id="第二章、机器学习算法"><a href="#第二章、机器学习算法" class="headerlink" title="第二章、机器学习算法"></a>第二章、机器学习算法</h2><h4 id="1-分类算法"><a href="#1-分类算法" class="headerlink" title="1.分类算法"></a>1.分类算法</h4><ul><li>(1).LR (Logistic Regression，逻辑回归又叫逻辑分类)</li><li>(2).SVM (Support Vector Machine，支持向量机)</li><li>(3).NB  (Naive Bayes，朴素贝叶斯)</li><li>(4).DT  (Decision Tree，决策树)<ul><li>1).C4.5</li><li>2).ID3</li><li>3).CART</li></ul></li><li>(5).集成算法<ul><li>1).Bagging</li><li>2).Random Forest (随机森林)</li><li>3).GB(梯度提升,Gradient boosting)</li><li>4).GBDT (Gradient Boosting Decision Tree)</li><li>5).AdaBoost  </li><li>6).Xgboost</li></ul></li><li><p>(6).最大熵模型</p><h4 id="2-回归算法"><a href="#2-回归算法" class="headerlink" title="2.回归算法"></a>2.回归算法</h4><ul><li>(1).LR (Linear Regression，线性回归)</li><li>(2).SVR (支持向量机回归)</li><li>(3). RR (Ridge Regression，岭回归)<h4 id="3-聚类算法"><a href="#3-聚类算法" class="headerlink" title="3.聚类算法"></a>3.聚类算法</h4></li><li>(1).Knn </li><li>(2).Kmeans 算法</li><li>(3).层次聚类</li><li>(4).密度聚类<h4 id="4-降维算法"><a href="#4-降维算法" class="headerlink" title="4.降维算法"></a>4.降维算法</h4></li><li>(1).SGD  (随机梯度下降)</li><li>(2). <h4 id="5-概率图模型算法"><a href="#5-概率图模型算法" class="headerlink" title="5.概率图模型算法"></a>5.概率图模型算法</h4><ul><li>(1).贝叶斯网络</li><li>(2).HMM</li><li>(3).CRF (条件随机场)<h4 id="6-文本挖掘算法"><a href="#6-文本挖掘算法" class="headerlink" title="6.文本挖掘算法"></a>6.文本挖掘算法</h4><h6 id="1-模型"><a href="#1-模型" class="headerlink" title="(1).模型"></a>(1).模型</h6><ul><li>1).LDA (主题生成模型，Latent Dirichlet Allocation)</li><li>4).最大熵模型<h6 id="2-关键词提取"><a href="#2-关键词提取" class="headerlink" title="(2).关键词提取"></a>(2).关键词提取</h6></li></ul></li><li>1).tf-idf</li><li>2).bm25</li><li>3).textrank</li><li>4).pagerank</li><li>5).左右熵 :左右熵高的作为关键词</li><li>6).互信息： <h6 id="3-词法分析"><a href="#3-词法分析" class="headerlink" title="(3).词法分析"></a>(3).词法分析</h6><ul><li>1).分词<ul><li>①HMM (因马尔科夫)</li><li>②CRF (条件随机场)</li></ul></li><li>2).词性标注</li><li>3).命名实体识别<h6 id="4-句法分析"><a href="#4-句法分析" class="headerlink" title="(4).句法分析"></a>(4).句法分析</h6></li></ul></li><li>1).句法结构分析</li><li>2).依存句法分析<h6 id="5-文本向量化"><a href="#5-文本向量化" class="headerlink" title="(5).文本向量化"></a>(5).文本向量化</h6></li><li>1).tf-idf</li><li>2).word2vec</li><li>3).doc2vec</li><li>4).cw2vec<h6 id="6-距离计算"><a href="#6-距离计算" class="headerlink" title="(6).距离计算"></a>(6).距离计算</h6></li><li>1).欧氏距离</li><li>2).相似度计算</li></ul></li></ul><h4 id="7-优化算法"><a href="#7-优化算法" class="headerlink" title="7.优化算法"></a>7.优化算法</h4><ul><li>(1).正则化<ul><li>1).L1正则化</li><li>2).L2正则化</li></ul></li></ul></li></ul><h2 id="第三章、深度学习算法"><a href="#第三章、深度学习算法" class="headerlink" title="第三章、深度学习算法"></a>第三章、深度学习算法</h2><ul><li>(1).BP </li><li>(2).CNN</li><li>(3).DNN</li><li>(3).RNN</li><li>(4).LSTM</li></ul><h2 id="第四章、分布式计算"><a href="#第四章、分布式计算" class="headerlink" title="第四章、分布式计算"></a>第四章、分布式计算</h2><h4 id="4-1-spark"><a href="#4-1-spark" class="headerlink" title="4.1.spark"></a>4.1.spark</h4><h4 id="4-2-hadoop"><a href="#4-2-hadoop" class="headerlink" title="4.2.hadoop"></a>4.2.hadoop</h4><h2 id="第五章、数据结构"><a href="#第五章、数据结构" class="headerlink" title="第五章、数据结构"></a>第五章、数据结构</h2><h2 id="第六章、其他方面"><a href="#第六章、其他方面" class="headerlink" title="第六章、其他方面"></a>第六章、其他方面</h2><h3 id="6-1模型方面"><a href="#6-1模型方面" class="headerlink" title="6.1模型方面"></a>6.1模型方面</h3><h4 id="6-1-1-模型优化"><a href="#6-1-1-模型优化" class="headerlink" title="6.1.1.模型优化"></a>6.1.1.模型优化</h4><ul><li>(1).特征选择</li><li>(2).梯度下降</li><li>(3).交叉验证</li><li>(4).参数调优</li><li>(5).模型评估：准确率、召回率、F1、AUC、ROC、损失函数 <h4 id="6-1-2-数据预处理"><a href="#6-1-2-数据预处理" class="headerlink" title="6.1.2.数据预处理"></a>6.1.2.数据预处理</h4></li><li>(1).标准化</li><li>(2).异常值处理</li><li>(3).二值化</li><li>(4).缺失值填充： 支持均值、中位数、特定值补差、多重插补</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;第一章、目录&quot;&gt;&lt;a href=&quot;#第一章、目录&quot; class=&quot;headerlink&quot; title=&quot;第一章、目录&quot;&gt;&lt;/a&gt;第一章、目录&lt;/h2&gt;&lt;p&gt; 人工智能算法大体上来说可以分类两类：基于统计的机器学习算法(Machine Learning)和深度学习算
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>文章观点抽取.md</title>
    <link href="http://yoursite.com/2018/05/18/NLP/3-%E7%9F%AD%E8%AF%AD%E5%88%86%E6%9E%90/%E8%A7%82%E7%82%B9%E6%8A%BD%E5%8F%96/%E6%96%87%E7%AB%A0%E8%A7%82%E7%82%B9%E6%8A%BD%E5%8F%96/%E6%96%87%E7%AB%A0%E8%A7%82%E7%82%B9%E6%8A%BD%E5%8F%96/"/>
    <id>http://yoursite.com/2018/05/18/NLP/3-短语分析/观点抽取/文章观点抽取/文章观点抽取/</id>
    <published>2018-05-18T05:17:35.000Z</published>
    <updated>2018-09-26T12:13:05.016Z</updated>
    
    <summary type="html">
    
    </summary>
    
      <category term="NLP" scheme="http://yoursite.com/categories/NLP/"/>
    
      <category term="3-短语分析" scheme="http://yoursite.com/categories/NLP/3-%E7%9F%AD%E8%AF%AD%E5%88%86%E6%9E%90/"/>
    
      <category term="观点抽取" scheme="http://yoursite.com/categories/NLP/3-%E7%9F%AD%E8%AF%AD%E5%88%86%E6%9E%90/%E8%A7%82%E7%82%B9%E6%8A%BD%E5%8F%96/"/>
    
      <category term="文章观点抽取" scheme="http://yoursite.com/categories/NLP/3-%E7%9F%AD%E8%AF%AD%E5%88%86%E6%9E%90/%E8%A7%82%E7%82%B9%E6%8A%BD%E5%8F%96/%E6%96%87%E7%AB%A0%E8%A7%82%E7%82%B9%E6%8A%BD%E5%8F%96/"/>
    
    
  </entry>
  
  <entry>
    <title>评论观点抽取</title>
    <link href="http://yoursite.com/2018/05/16/NLP/3-%E7%9F%AD%E8%AF%AD%E5%88%86%E6%9E%90/%E8%A7%82%E7%82%B9%E6%8A%BD%E5%8F%96/%E8%AF%84%E8%AE%BA%E8%A7%82%E7%82%B9%E6%8A%BD%E5%8F%96/%E8%AF%84%E8%AE%BA%E8%A7%82%E7%82%B9%E6%8A%BD%E5%8F%96/"/>
    <id>http://yoursite.com/2018/05/16/NLP/3-短语分析/观点抽取/评论观点抽取/评论观点抽取/</id>
    <published>2018-05-16T05:17:35.000Z</published>
    <updated>2018-09-26T12:12:36.848Z</updated>
    
    <summary type="html">
    
    </summary>
    
      <category term="NLP" scheme="http://yoursite.com/categories/NLP/"/>
    
      <category term="3-短语分析" scheme="http://yoursite.com/categories/NLP/3-%E7%9F%AD%E8%AF%AD%E5%88%86%E6%9E%90/"/>
    
      <category term="观点抽取" scheme="http://yoursite.com/categories/NLP/3-%E7%9F%AD%E8%AF%AD%E5%88%86%E6%9E%90/%E8%A7%82%E7%82%B9%E6%8A%BD%E5%8F%96/"/>
    
      <category term="评论观点抽取" scheme="http://yoursite.com/categories/NLP/3-%E7%9F%AD%E8%AF%AD%E5%88%86%E6%9E%90/%E8%A7%82%E7%82%B9%E6%8A%BD%E5%8F%96/%E8%AF%84%E8%AE%BA%E8%A7%82%E7%82%B9%E6%8A%BD%E5%8F%96/"/>
    
    
  </entry>
  
  <entry>
    <title>观点抽取分类</title>
    <link href="http://yoursite.com/2018/05/16/NLP/3-%E7%9F%AD%E8%AF%AD%E5%88%86%E6%9E%90/%E8%A7%82%E7%82%B9%E6%8A%BD%E5%8F%96/%E8%A7%82%E7%82%B9%E6%8A%BD%E5%8F%96%E5%88%86%E7%B1%BB/"/>
    <id>http://yoursite.com/2018/05/16/NLP/3-短语分析/观点抽取/观点抽取分类/</id>
    <published>2018-05-16T05:17:35.000Z</published>
    <updated>2018-09-26T12:11:59.135Z</updated>
    
    <content type="html"><![CDATA[<h4 id="一、观点抽取的分类"><a href="#一、观点抽取的分类" class="headerlink" title="一、观点抽取的分类"></a>一、观点抽取的分类</h4><p>我认为按照观点抽取的对象可以分为两类，一种是对文章进行观点抽取，得到文章的类似摘要、分类、关键词等性质的句子、短语或者关键词。一种是对评论进行观点抽取，得到一个短语，然后在对短语进行分类。</p><h6 id="1-文章观点抽取"><a href="#1-文章观点抽取" class="headerlink" title="1.文章观点抽取"></a>1.文章观点抽取</h6><p>文章的观点抽取结果可以是一句话（类似标题），一段话（类似摘要），一个短语，一个关键词（打标签，其实是文本分类）。</p><h6 id="2-评论观点抽取"><a href="#2-评论观点抽取" class="headerlink" title="2.评论观点抽取"></a>2.评论观点抽取</h6><p>评论的观点抽取结果一般是一个短语。类似淘宝评论的观点抽取。例如可以将某一件衣服评论观点分为：质量好，超划算，性价比高，穿上好看。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;一、观点抽取的分类&quot;&gt;&lt;a href=&quot;#一、观点抽取的分类&quot; class=&quot;headerlink&quot; title=&quot;一、观点抽取的分类&quot;&gt;&lt;/a&gt;一、观点抽取的分类&lt;/h4&gt;&lt;p&gt;我认为按照观点抽取的对象可以分为两类，一种是对文章进行观点抽取，得到文章的类似摘要、
      
    
    </summary>
    
      <category term="NLP" scheme="http://yoursite.com/categories/NLP/"/>
    
      <category term="3-短语分析" scheme="http://yoursite.com/categories/NLP/3-%E7%9F%AD%E8%AF%AD%E5%88%86%E6%9E%90/"/>
    
      <category term="观点抽取" scheme="http://yoursite.com/categories/NLP/3-%E7%9F%AD%E8%AF%AD%E5%88%86%E6%9E%90/%E8%A7%82%E7%82%B9%E6%8A%BD%E5%8F%96/"/>
    
    
  </entry>
  
  <entry>
    <title>0-新词发现.md</title>
    <link href="http://yoursite.com/2018/02/18/NLP/1-%E8%AF%8D%E6%B3%95%E5%88%86%E6%9E%90/2-%E6%96%B0%E8%AF%8D%E5%8F%91%E7%8E%B0/0-%E6%96%B0%E8%AF%8D%E5%8F%91%E7%8E%B0/"/>
    <id>http://yoursite.com/2018/02/18/NLP/1-词法分析/2-新词发现/0-新词发现/</id>
    <published>2018-02-18T09:17:35.000Z</published>
    <updated>2018-12-19T07:58:41.582Z</updated>
    
    <content type="html"><![CDATA[<h4 id="一、新词"><a href="#一、新词" class="headerlink" title="一、新词"></a>一、新词</h4><p>  新词发现又叫未登录词识别，我认为可以包括两种形式:1.命名实体识别 2.普通新词发现<br>命名实体识别我们在另一章中专门讨论，本节只讨论普通新词发现。<br>  目前新词发现可以分为两类算法：无监督，有监督。无监督主要通过互信息，左右熵来发现，有监督主要通过<br>标注，例如CRF+深度学习  </p><h4 id="二、基本概念"><a href="#二、基本概念" class="headerlink" title="二、基本概念"></a>二、基本概念</h4><p>   现在基于统计方法的新词发现主要通过词内互信息(凝聚度)，词间的左右信息熵来确定，因此我们向明确一下基本<br>   概念。</p><h6 id="1-互信息：凝聚度-Pointwise-mutual-information"><a href="#1-互信息：凝聚度-Pointwise-mutual-information" class="headerlink" title="1.互信息：凝聚度(Pointwise mutual information)"></a>1.互信息：凝聚度(Pointwise mutual information)</h6><p>   公式：$PMI = \frac{P(x,y)}{P(x)<em>P(y)}$<br>   如果x，y独立 ，那么pmi等于1。如果不独立，则PMI大于1，当PMI足够大的时候就表示凝固度足够高。拿 “知”、“乎” 这两个字来说，假设在 5000 万字的样本中, “知” 出现了 150 万次， “乎” 出现了 4 万次。那 “知” 出现的概率为 0.03, “乎” 出现的概率为 0.0008。如果两个字符出现是个独立事件的话，”知”、“乎” 一起出现的期望概率是 0.03 </em> 0.0008 = 2.4e-05. 如果实际上 “知乎” 出现了 3 万次, 则实际上”知”、“乎” 一起出现的概率是 6e-03, 是期望概率的 250 倍。也就是说两个字越相关，点间互信息越大。</p><h6 id="2-信息熵"><a href="#2-信息熵" class="headerlink" title="2.信息熵"></a>2.信息熵</h6><p>公式：$H = \sum_{i=1}^{n}{P(i)*logP(i)}$<br>熵描述的是信息的不确定性，熵越大，不确定性越强，例如“萝卜”，左边可以有“吃萝卜”、“腌萝卜”、“炒萝卜”、“炖萝卜”，左边的词越多，代表萝卜的左熵越丰富，那“萝卜”成词的可能性就越大。</p><h6 id="3-左右熵"><a href="#3-左右熵" class="headerlink" title="3.左右熵"></a>3.左右熵</h6><p>左右熵是根据信息熵来算的。<br>我们用信息熵来衡量一个文本片段的左邻字集合和右邻字集合有多随机。考虑这么一句话“吃葡萄不吐葡萄皮不吃葡萄倒吐葡萄皮”，“葡萄”一词出现了四次，其中左邻字分别为 {吃, 吐, 吃, 吐} ，右邻字分别为 {不, 皮, 倒, 皮} 。根据公式，“葡萄”一词的左邻字的信息熵为 – (1/2) · log(1/2) – (1/2) · log(1/2) ≈ 0.693 ，它的右邻字的信息熵则为 – (1/2) · log(1/2) – (1/4) · log(1/4) – (1/4) · log(1/4) ≈ 1.04 。可见，在这个句子中，“葡萄”一词的右邻字更加丰富一些。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;一、新词&quot;&gt;&lt;a href=&quot;#一、新词&quot; class=&quot;headerlink&quot; title=&quot;一、新词&quot;&gt;&lt;/a&gt;一、新词&lt;/h4&gt;&lt;p&gt;  新词发现又叫未登录词识别，我认为可以包括两种形式:1.命名实体识别 2.普通新词发现&lt;br&gt;命名实体识别我们在另一章中专
      
    
    </summary>
    
      <category term="NLP" scheme="http://yoursite.com/categories/NLP/"/>
    
      <category term="1-词法分析" scheme="http://yoursite.com/categories/NLP/1-%E8%AF%8D%E6%B3%95%E5%88%86%E6%9E%90/"/>
    
      <category term="2-新词发现" scheme="http://yoursite.com/categories/NLP/1-%E8%AF%8D%E6%B3%95%E5%88%86%E6%9E%90/2-%E6%96%B0%E8%AF%8D%E5%8F%91%E7%8E%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>代价函数，损失函数，目标函数区别</title>
    <link href="http://yoursite.com/2018/01/09/MachineLearning/0-%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0%EF%BC%8C%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%EF%BC%8C%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0%E5%8C%BA%E5%88%AB/"/>
    <id>http://yoursite.com/2018/01/09/MachineLearning/0-基本概念/代价函数，损失函数，目标函数区别/</id>
    <published>2018-01-09T08:02:27.000Z</published>
    <updated>2018-11-01T09:46:59.187Z</updated>
    
    <content type="html"><![CDATA[<p>首先给出结论：</p><p>损失函数（Loss Function ）是定义在单个样本上的，算的是一个样本的误差。</p><p>代价函数（Cost Function ）是定义在整个训练集上的，是所有样本误差的平均，也就是损失函数的平均。</p><p>目标函数（Object Function）定义为：最终需要优化的函数。等于经验风险+结构风险（也就是Cost Function + 正则化项）。</p><p>参考：<a href="https://www.zhihu.com/question/52398145" target="_blank" rel="noopener">https://www.zhihu.com/question/52398145</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;首先给出结论：&lt;/p&gt;
&lt;p&gt;损失函数（Loss Function ）是定义在单个样本上的，算的是一个样本的误差。&lt;/p&gt;
&lt;p&gt;代价函数（Cost Function ）是定义在整个训练集上的，是所有样本误差的平均，也就是损失函数的平均。&lt;/p&gt;
&lt;p&gt;目标函数（Objec
      
    
    </summary>
    
      <category term="MachineLearning" scheme="http://yoursite.com/categories/MachineLearning/"/>
    
      <category term="0-基本概念" scheme="http://yoursite.com/categories/MachineLearning/0-%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/"/>
    
    
  </entry>
  
  <entry>
    <title>时序</title>
    <link href="http://yoursite.com/2018/01/09/MachineLearning/4-%E6%97%B6%E5%BA%8F/%E6%97%B6%E5%BA%8F/"/>
    <id>http://yoursite.com/2018/01/09/MachineLearning/4-时序/时序/</id>
    <published>2018-01-09T08:02:27.000Z</published>
    <updated>2019-02-20T04:10:20.059Z</updated>
    
    <summary type="html">
    
    </summary>
    
      <category term="MachineLearning" scheme="http://yoursite.com/categories/MachineLearning/"/>
    
      <category term="4-时序" scheme="http://yoursite.com/categories/MachineLearning/4-%E6%97%B6%E5%BA%8F/"/>
    
    
  </entry>
  
  <entry>
    <title>回归的分类</title>
    <link href="http://yoursite.com/2018/01/09/MachineLearning/2-%E5%9B%9E%E5%BD%92/%E5%9B%9E%E5%BD%92%E7%9A%84%E5%88%86%E7%B1%BB/"/>
    <id>http://yoursite.com/2018/01/09/MachineLearning/2-回归/回归的分类/</id>
    <published>2018-01-09T08:02:27.000Z</published>
    <updated>2018-01-09T08:31:26.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、回归可以分为以下几类"><a href="#一、回归可以分为以下几类" class="headerlink" title="一、回归可以分为以下几类"></a>一、回归可以分为以下几类</h2><ul><li>1.线性回归</li><li>2.非线性回归</li><li>3.逻辑回归<a id="more"></a><h2 id="二、回归的概念"><a href="#二、回归的概念" class="headerlink" title="二、回归的概念"></a>二、回归的概念</h2><h4 id="1-线性回归"><a href="#1-线性回归" class="headerlink" title="1.线性回归"></a>1.线性回归</h4>可以简单理解为线性就是每个变量的指数都是1。</li></ul><h4 id="2-非线性回归"><a href="#2-非线性回归" class="headerlink" title="2.非线性回归"></a>2.非线性回归</h4><p>而非线性就是至少有一个变量的指数不是1。</p><h4 id="3-逻辑回归"><a href="#3-逻辑回归" class="headerlink" title="3.逻辑回归"></a>3.逻辑回归</h4><h1 id="回归和分类的区别"><a href="#回归和分类的区别" class="headerlink" title="回归和分类的区别"></a>回归和分类的区别</h1><p>分类和回归的区别在于输出变量的类型。</p><p>定量输出称为回归，或者说是连续变量预测；<br>定性输出称为分类，或者说是离散变量预测。</p><p>举个例子：<br>预测明天的气温是多少度，这是一个回归任务；<br>预测明天是阴、晴还是雨，就是一个分类任务。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;一、回归可以分为以下几类&quot;&gt;&lt;a href=&quot;#一、回归可以分为以下几类&quot; class=&quot;headerlink&quot; title=&quot;一、回归可以分为以下几类&quot;&gt;&lt;/a&gt;一、回归可以分为以下几类&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;1.线性回归&lt;/li&gt;
&lt;li&gt;2.非线性回归&lt;/li&gt;
&lt;li&gt;3.逻辑回归
    
    </summary>
    
      <category term="MachineLearning" scheme="http://yoursite.com/categories/MachineLearning/"/>
    
      <category term="2-回归" scheme="http://yoursite.com/categories/MachineLearning/2-%E5%9B%9E%E5%BD%92/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>回归</title>
    <link href="http://yoursite.com/2018/01/09/MachineLearning/2-%E5%9B%9E%E5%BD%92/%E5%9B%9E%E5%BD%92/"/>
    <id>http://yoursite.com/2018/01/09/MachineLearning/2-回归/回归/</id>
    <published>2018-01-09T08:02:27.000Z</published>
    <updated>2018-01-09T08:02:27.288Z</updated>
    
    <summary type="html">
    
    </summary>
    
      <category term="MachineLearning" scheme="http://yoursite.com/categories/MachineLearning/"/>
    
      <category term="2-回归" scheme="http://yoursite.com/categories/MachineLearning/2-%E5%9B%9E%E5%BD%92/"/>
    
    
  </entry>
  
  <entry>
    <title>0-吴恩达-机器学习</title>
    <link href="http://yoursite.com/2018/01/09/%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%96%99/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/0-%E5%90%B4%E6%81%A9%E8%BE%BE%E2%80%94%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    <id>http://yoursite.com/2018/01/09/学习资料/机器学习/0-吴恩达—机器学习/</id>
    <published>2018-01-09T08:02:27.000Z</published>
    <updated>2018-11-28T12:24:16.356Z</updated>
    
    <content type="html"><![CDATA[<p>机器学习课程地址：<br><a href="https://www.coursera.org/course/ml" target="_blank" rel="noopener">https://www.coursera.org/course/ml</a></p><p>笔记地址：<br><a href="http://www.ai-start.com/ml2014/" target="_blank" rel="noopener">http://www.ai-start.com/ml2014/</a></p><p>深度学习课程地址：</p><p><a href="https://mooc.study.163.com/university/deeplearning_ai#/c" target="_blank" rel="noopener">https://mooc.study.163.com/university/deeplearning_ai#/c</a></p><p>笔记地址：</p><p><a href="http://www.ai-start.com/dl2017/" target="_blank" rel="noopener">http://www.ai-start.com/dl2017/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;机器学习课程地址：&lt;br&gt;&lt;a href=&quot;https://www.coursera.org/course/ml&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.coursera.org/course/ml&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;笔记
      
    
    </summary>
    
      <category term="学习资料" scheme="http://yoursite.com/categories/%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%96%99/"/>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%96%99/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>2017年总结</title>
    <link href="http://yoursite.com/2018/01/09/%E6%80%BB%E7%BB%93/2017%E5%B9%B4%E6%80%BB%E7%BB%93/"/>
    <id>http://yoursite.com/2018/01/09/总结/2017年总结/</id>
    <published>2018-01-09T06:55:28.000Z</published>
    <updated>2018-08-11T01:06:32.802Z</updated>
    
    <summary type="html">
    
    </summary>
    
      <category term="总结" scheme="http://yoursite.com/categories/%E6%80%BB%E7%BB%93/"/>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>2017年</title>
    <link href="http://yoursite.com/2018/01/09/%E6%80%BB%E7%BB%93/2017%E5%B9%B4/"/>
    <id>http://yoursite.com/2018/01/09/总结/2017年/</id>
    <published>2018-01-09T06:55:03.000Z</published>
    <updated>2018-01-09T06:55:03.415Z</updated>
    
    <summary type="html">
    
    </summary>
    
      <category term="总结" scheme="http://yoursite.com/categories/%E6%80%BB%E7%BB%93/"/>
    
    
  </entry>
  
  <entry>
    <title>3-模型假设函数,损失函数,代价函数,目标函数</title>
    <link href="http://yoursite.com/2017/12/19/MachineLearning/1-%E6%A8%A1%E5%9E%8B/3-%E6%A8%A1%E5%9E%8B%E5%81%87%E8%AE%BE%E5%87%BD%E6%95%B0,%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0,%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0,%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0/"/>
    <id>http://yoursite.com/2017/12/19/MachineLearning/1-模型/3-模型假设函数,损失函数,代价函数,目标函数/</id>
    <published>2017-12-19T09:17:35.000Z</published>
    <updated>2018-11-28T09:32:28.927Z</updated>
    
    <content type="html"><![CDATA[<p>假设函数(hypothesis function):预测函数<br>损失函数(loss function,error function):计算的是一个样本的误差<br>代价函数(cost function,成本函数):是整个训练集上所有样本误差的平均<br>目标函数(objective function):代价函数 + 正则化项</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;假设函数(hypothesis function):预测函数&lt;br&gt;损失函数(loss function,error function):计算的是一个样本的误差&lt;br&gt;代价函数(cost function,成本函数):是整个训练集上所有样本误差的平均&lt;br&gt;目标函数(obj
      
    
    </summary>
    
      <category term="MachineLearning" scheme="http://yoursite.com/categories/MachineLearning/"/>
    
      <category term="1-模型" scheme="http://yoursite.com/categories/MachineLearning/1-%E6%A8%A1%E5%9E%8B/"/>
    
    
  </entry>
  
  <entry>
    <title>1-PageRank算法的前世今生</title>
    <link href="http://yoursite.com/2017/12/19/NLP/1-%E8%AF%8D%E6%B3%95%E5%88%86%E6%9E%90/4-%E5%85%B3%E9%94%AE%E8%AF%8D%E6%8F%90%E5%8F%96/1-PageRank%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F/"/>
    <id>http://yoursite.com/2017/12/19/NLP/1-词法分析/4-关键词提取/1-PageRank的前世今生/</id>
    <published>2017-12-19T09:17:35.000Z</published>
    <updated>2018-11-28T04:06:07.710Z</updated>
    
    <content type="html"><![CDATA[<h4 id="一、算法定义"><a href="#一、算法定义" class="headerlink" title="一、算法定义"></a>一、算法定义</h4><p>PageRank，网页排名，又称网页级别、Google左侧排名或佩奇排名，是一种由 [1]  根据网页之间相互的超链接计算的技术，而作为网页排名的要素之一，以Google公司创办人拉里·佩奇（Larry Page）之姓来命名。Google用它来体现网页的相关性和重要性，在搜索引擎优化操作中是经常被用来评估网页优化的成效因素之一。Google的创始人拉里·佩奇和谢尔盖·布林于1998年在斯坦福大学发明了这项技术。</p><p>PageRank通过网络浩瀚的超链接关系来确定一个页面的等级。Google把从A页面到B页面的链接解释为A页面给B页面投票，Google根据投票来源（甚至来源的来源，即链接到A页面的页面）和投票目标的等级来决定新的等级。简单的说，一个高等级的页面可以使其他低等级页面的等级提升。</p><h4 id="二、算法来源"><a href="#二、算法来源" class="headerlink" title="二、算法来源"></a>二、算法来源</h4><p>这个要从搜索引擎的发展讲起。最早的搜索引擎采用的是 分类目录[^ref_1] 的方法，即通过人工进行网页分类并整理出高质量的网站。那时 Yahoo 和国内的 hao123 就是使用的这种方法。</p><p>后来网页越来越多，人工分类已经不现实了。搜索引擎进入了 文本检索 的时代，即计算用户查询关键词与网页内容的相关程度来返回搜索结果。这种方法突破了数量的限制，但是搜索结果不是很好。因为总有某些网页来回地倒腾某些关键词使自己的搜索排名靠前。</p><p>于是我们的主角要登场了。没错，谷歌的两位创始人，当时还是美国斯坦福大学 (Stanford University) 研究生的佩奇 (Larry Page) 和布林 (Sergey Brin) 开始了对网页排序问题的研究。他们的借鉴了学术界评判学术论文重要性的通用方法， 那就是看论文的引用次数。由此想到网页的重要性也可以根据这种方法来评价。于是PageRank的核心思想就诞生了[^ref_2]，非常简单：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1.如果一个网页被很多其他网页链接到的话说明这个网页比较重要，也就是PageRank值会相对较高</span><br><span class="line">2.如果一个PageRank值很高的网页链接到一个其他的网页，那么被链接到的网页的PageRank值会相应地因此而提高</span><br></pre></td></tr></table></figure></p><h4 id="三、搜索引擎"><a href="#三、搜索引擎" class="headerlink" title="三、搜索引擎"></a>三、搜索引擎</h4><p>搜索引擎的功能主要是：根据用户输入的关键字，返回文档的链接结果。<br>搜索引擎主要解决的三大问题：(1)如何获取文档资料  (2)如何根据关键词检索到相关文档  (3)如何对文档进行排序，返回给用户满意的页面。</p><h6 id="1-获取文档资料"><a href="#1-获取文档资料" class="headerlink" title="1.获取文档资料"></a>1.获取文档资料</h6><p>利用爬虫程序，获取互联网的页面资料。爬虫技术先从一个网页出发，将该网页的内容记录下来，保存到资料库，接着分析页面中的超链接，分别递归得去获取超链接页面。</p><h6 id="2-如何根据关键词检索到相关文档"><a href="#2-如何根据关键词检索到相关文档" class="headerlink" title="2.如何根据关键词检索到相关文档"></a>2.如何根据关键词检索到相关文档</h6><p>采用倒排索引算法。简单的说，倒排索引是一对key-value对，key代表关键词，value代表拥有这些关键词的文档编号或者url。</p><h6 id="3-文档排序"><a href="#3-文档排序" class="headerlink" title="3.文档排序"></a>3.文档排序</h6><p>这是搜索引擎最核心的问题，也是google发家致富的法宝 – PageRank算法。</p><h4 id="四、算法原理"><a href="#四、算法原理" class="headerlink" title="四、算法原理"></a>四、算法原理</h4><h6 id="4-1PageRank的两个假设"><a href="#4-1PageRank的两个假设" class="headerlink" title="4.1PageRank的两个假设"></a>4.1PageRank的两个假设</h6><blockquote><p>数量假设：1.如果一个网页被很多其他网页链接到的话说明这个网页比较重要，也就是PageRank值会相对较高。<br>质量假设：2.如果一个PageRank值很高的网页链接到一个其他的网页，那么被链接到的网页的PageRank值会相应地因此而提高。</p></blockquote><h6 id="4-2PageRank的算法原理"><a href="#4-2PageRank的算法原理" class="headerlink" title="4.2PageRank的算法原理"></a>4.2PageRank的算法原理</h6><p>PageRank算法总的来说就是预先给每个网页一个PR值（下面用PR值指代PageRank值），由于PR值物理意义上为一个网页被访问概率，所以一般是1/N，其中N为网页总数。<br>预先给定PR值后，通过下面的算法不断迭代，直至达到平稳分布为止。</p><p><strong><em>PageRank值主要是的是节点的入链值。</em></strong></p><h6 id="4-3PageRank的简单计算"><a href="#4-3PageRank的简单计算" class="headerlink" title="4.3PageRank的简单计算"></a>4.3PageRank的简单计算</h6><h6 id="4-4PageRank的修正计算"><a href="#4-4PageRank的修正计算" class="headerlink" title="4.4PageRank的修正计算"></a>4.4PageRank的修正计算</h6>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;一、算法定义&quot;&gt;&lt;a href=&quot;#一、算法定义&quot; class=&quot;headerlink&quot; title=&quot;一、算法定义&quot;&gt;&lt;/a&gt;一、算法定义&lt;/h4&gt;&lt;p&gt;PageRank，网页排名，又称网页级别、Google左侧排名或佩奇排名，是一种由 [1]  根据网页之间相
      
    
    </summary>
    
      <category term="NLP" scheme="http://yoursite.com/categories/NLP/"/>
    
      <category term="1-词法分析" scheme="http://yoursite.com/categories/NLP/1-%E8%AF%8D%E6%B3%95%E5%88%86%E6%9E%90/"/>
    
      <category term="4-关键词提取" scheme="http://yoursite.com/categories/NLP/1-%E8%AF%8D%E6%B3%95%E5%88%86%E6%9E%90/4-%E5%85%B3%E9%94%AE%E8%AF%8D%E6%8F%90%E5%8F%96/"/>
    
    
  </entry>
  
  <entry>
    <title>2-模型评估的指标</title>
    <link href="http://yoursite.com/2017/12/19/MachineLearning/1-%E6%A8%A1%E5%9E%8B/2-%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E7%9A%84%E6%8C%87%E6%A0%87/"/>
    <id>http://yoursite.com/2017/12/19/MachineLearning/1-模型/2-模型评估的指标/</id>
    <published>2017-12-19T09:17:35.000Z</published>
    <updated>2018-10-08T06:03:43.875Z</updated>
    
    <content type="html"><![CDATA[<p>多模型评估的指标可以分为以下几个类别</p><h4 id="一-Accuracy，Precision，Recall"><a href="#一-Accuracy，Precision，Recall" class="headerlink" title="一.Accuracy，Precision，Recall"></a>一.Accuracy，Precision，Recall</h4><p>要计算这几个指标先要了解几个概念：<br>FN：False Negative,被判定为负样本，但事实上是正样本。<br>FP：False Positive,被判定为正样本，但事实上是负样本。<br>TN：True Negative,被判定为负样本，事实上也是负样本。<br>TP：True Positive,被判定为正样本，事实上也是证样本。<br>1.Accuracy (正确率)<br>$ac=\frac {TP+TN}{TP+TN+FP+FN}$</p><p>2.Precision精确率，准确率，查准率<br>$P = \frac {TP}{TP+FP}$<br>解释：正样本占分类器所分的正样本的比例</p><p>3.Recall(召回率，查全率)<br>$R = \frac {TP}{TP + FN}$<br>解释：正样本占真正的正样本的比例</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;多模型评估的指标可以分为以下几个类别&lt;/p&gt;
&lt;h4 id=&quot;一-Accuracy，Precision，Recall&quot;&gt;&lt;a href=&quot;#一-Accuracy，Precision，Recall&quot; class=&quot;headerlink&quot; title=&quot;一.Accuracy，P
      
    
    </summary>
    
      <category term="MachineLearning" scheme="http://yoursite.com/categories/MachineLearning/"/>
    
      <category term="1-模型" scheme="http://yoursite.com/categories/MachineLearning/1-%E6%A8%A1%E5%9E%8B/"/>
    
    
  </entry>
  
  <entry>
    <title>1-判别模型和生成模型.md</title>
    <link href="http://yoursite.com/2017/12/19/MachineLearning/1-%E6%A8%A1%E5%9E%8B/1-%E5%88%A4%E5%88%AB%E6%A8%A1%E5%9E%8B%E5%92%8C%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/"/>
    <id>http://yoursite.com/2017/12/19/MachineLearning/1-模型/1-判别模型和生成模型/</id>
    <published>2017-12-19T09:17:35.000Z</published>
    <updated>2018-10-08T06:02:20.795Z</updated>
    
    <content type="html"><![CDATA[<table><thead><tr><th></th><th>判别模型</th><th>生成模型</th></tr></thead><tbody><tr><td>特点</td><td>寻找不同类别之间的最优分类面，反映的是异类数据之间的差异</td><td>对后验概率建模，从统计的角度表示数据的分布情况，能够反映同类数据本身的相似度</td></tr><tr><td>区别(假定输入x, 类别标签y)</td><td>估计的是条件概率分布(conditional distribution) : P(yx)</td><td>估计的是联合概率分布（joint probability distribution: P(x, y)</td></tr><tr><td>联系</td><td>由判别式模型得不到产生式模型</td><td>由产生式模型可以得到判别式模型</td></tr><tr><td>常见模型</td><td>– logistic regression  – SVMs  – traditional neural networks  – Nearest neighbor</td><td>–Gaussians, Naive Bayes –Mixtures of Gaussians, Mixtures of experts, HMMs–Sigmoidal belief networks, Bayesian networks– Markov random fields</td></tr></tbody></table>]]></content>
    
    <summary type="html">
    
      
      
        &lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;判别模型&lt;/th&gt;
&lt;th&gt;生成模型&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;特点&lt;/td&gt;
&lt;td&gt;寻找不同类别之间的最优分类面，反映的是异类数据之间的差异&lt;/td&gt;
&lt;td
      
    
    </summary>
    
      <category term="MachineLearning" scheme="http://yoursite.com/categories/MachineLearning/"/>
    
      <category term="1-模型" scheme="http://yoursite.com/categories/MachineLearning/1-%E6%A8%A1%E5%9E%8B/"/>
    
    
  </entry>
  
  <entry>
    <title>0-语言模型</title>
    <link href="http://yoursite.com/2017/12/18/NLP/11-%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/0-%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/"/>
    <id>http://yoursite.com/2017/12/18/NLP/11-语言模型/0-语言模型/</id>
    <published>2017-12-18T09:17:35.000Z</published>
    <updated>2018-11-13T10:13:35.720Z</updated>
    
    <content type="html"><![CDATA[<p><code>语言模型就是用于评估文本符合语言使用习惯程度的模型。</code></p><h4 id="一、定义"><a href="#一、定义" class="headerlink" title="一、定义"></a>一、定义</h4><p>我们目前所说的语言模型主要指的是统计语言模型。<br>统计语言模型是一个单词序列上的概率分布，对于一个给定长度为m的序列，它可以为整个序列产生一个概率 P(w1,w2,…,wm)。其实就是想办法找到一个概率分布，它可以表示任意一个句子或序列出现的概率。 </p><h4 id="二、统计语言模型"><a href="#二、统计语言模型" class="headerlink" title="二、统计语言模型"></a>二、统计语言模型</h4><p>对于一个由T个词按顺序构成的句子$s=w_1w_2w_3…w_t$，p(s)实际上求解的是字符串$w_1w_2w_3…w_t$的联合概率，利用贝叶斯公式，链式分解如下：<br>$P(s)=P(w_1w_2w_3…w_t) =P(w_1)P(w_2|w_1)P(w_3|w_1w_2)P(w_T|w<em>1…w</em>{T-1})$<br>从上面可以看到，一个统计语言模型可以表示成，给定前面的的词，求后面一个词出现的条件概率。我们在求p(s)时实际上就已经建立了一个模型，这里的p(*)就是模型的参数，如果这些参数已经求解得到，那么很容易就能够得到字符串s的概率。</p><p>由于上式中的参数过多，因此需要近似的计算方法。常见的方法有n-gram模型方法、决策树方法、最大熵模型方法、最大熵马尔科夫模型方法、条件随机域方法、神经网络方法，等等。</p><h4 id="三、"><a href="#三、" class="headerlink" title="三、"></a>三、</h4>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;code&gt;语言模型就是用于评估文本符合语言使用习惯程度的模型。&lt;/code&gt;&lt;/p&gt;
&lt;h4 id=&quot;一、定义&quot;&gt;&lt;a href=&quot;#一、定义&quot; class=&quot;headerlink&quot; title=&quot;一、定义&quot;&gt;&lt;/a&gt;一、定义&lt;/h4&gt;&lt;p&gt;我们目前所说的语言模型主要指的
      
    
    </summary>
    
      <category term="NLP" scheme="http://yoursite.com/categories/NLP/"/>
    
      <category term="11-语言模型" scheme="http://yoursite.com/categories/NLP/11-%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/"/>
    
    
  </entry>
  
  <entry>
    <title>0-文本向量化</title>
    <link href="http://yoursite.com/2017/12/18/NLP/6-%E6%96%87%E6%9C%AC%E5%90%91%E9%87%8F%E5%8C%96/0-%E6%96%87%E6%9C%AC%E5%90%91%E9%87%8F%E5%8C%96/"/>
    <id>http://yoursite.com/2017/12/18/NLP/6-文本向量化/0-文本向量化/</id>
    <published>2017-12-18T09:17:35.000Z</published>
    <updated>2018-11-13T12:45:56.226Z</updated>
    
    <content type="html"><![CDATA[<h4 id="一、简介"><a href="#一、简介" class="headerlink" title="一、简介"></a>一、简介</h4><h4 id="二、分类"><a href="#二、分类" class="headerlink" title="二、分类"></a>二、分类</h4><h5 id="1-one-hot-将词向量化"><a href="#1-one-hot-将词向量化" class="headerlink" title="1.one-hot(将词向量化)"></a>1.one-hot(将词向量化)</h5><p>这最简单的一种想法，就是对一句话用one-hot编码:比如对于这句话：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ohn likes to watch movies,Mary likes too.</span><br><span class="line">John likes to watch movies,Mary likes too.</span><br><span class="line">John also likes to watch football games.</span><br><span class="line">John also likes to watch football games.</span><br></pre></td></tr></table></figure></p><p>可以通过以上语料构建一个字典：dic<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">1:&quot;John&quot;</span><br><span class="line">2:&quot;likes&quot;</span><br><span class="line">3:&quot;to&quot;</span><br><span class="line">4:&quot;watch&quot;</span><br><span class="line">5:&quot;movies&quot;</span><br><span class="line">6:&quot;also&quot;</span><br><span class="line">7:&quot;football&quot;</span><br><span class="line">8:&quot;games&quot;</span><br><span class="line">9:&quot;Mary&quot;</span><br><span class="line">10:&quot;too&quot;</span><br></pre></td></tr></table></figure></p><p>用one-hot可以表示为：<br>第一句话：<br>John :[1,0,0,0,0,0,0,0,0,0]<br>likes  :[0,1,0,0,0,0,0,0,0,0]<br>…<br>too   :[0,0,0,0,0,0,0,0,0,1]</p><p>第二句话：<br>John :[1,0,0,0,0,0,0,0,0,0]<br>likes  :[0,1,0,0,0,0,0,0,0,0]<br>…<br>too   :[0,0,0,0,0,0,0,0,0,1]<br>但事实是，这样做耗费的资源太多，而且不能很好的表征每句话的特性。</p><h5 id="2-Bag-of-words-将句子向量化"><a href="#2-Bag-of-words-将句子向量化" class="headerlink" title="2.Bag-of-words(将句子向量化)"></a>2.Bag-of-words(将句子向量化)</h5><p>另一种方法则是词袋模型，它相当于一个词袋，不考虑词/句之间的相关性，只要出现了该词，就会记为1，再次出现就会+1。比如前面的那句话：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">John likes to watch movies,Mary likes too.</span><br></pre></td></tr></table></figure></p><p>可以表示为<br>[1,2,1,1,1,0,0,0,1,1]</p><p>与其相似的是binary weighting,它就是看一下每个词是否出现，出现记为1，没出现记为0<br>[1,1,1,1,1,0,0,0,1,1]</p><h5 id="3-TF-IDF-将句子向量化：考虑词的重要性"><a href="#3-TF-IDF-将句子向量化：考虑词的重要性" class="headerlink" title="3.TF-IDF(将句子向量化：考虑词的重要性)"></a>3.TF-IDF(将句子向量化：考虑词的重要性)</h5><p>因此，就有tf-idf解决这个问题，它的主要思路就是有两方面：<br>A—第一就是如果这个词在我们当前文档出现的频率非常高，说明它在当前文档应该是比较重要的。<br>B-但如果它在所有的文档中出现的频次都非常好，大家可能就会觉得这个词应该不是那么重要的。</p><p>比如中文的“的“，或者我们上面那两个句子中的to.<br>因此，tf-idf就是一个在当前文档和所有文档中权衡他们的重要性，然后计算出每个词的重要度的方法。</p><h5 id="4-word2vec-词的维度固定"><a href="#4-word2vec-词的维度固定" class="headerlink" title="4.word2vec(词的维度固定)"></a>4.word2vec(词的维度固定)</h5>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;一、简介&quot;&gt;&lt;a href=&quot;#一、简介&quot; class=&quot;headerlink&quot; title=&quot;一、简介&quot;&gt;&lt;/a&gt;一、简介&lt;/h4&gt;&lt;h4 id=&quot;二、分类&quot;&gt;&lt;a href=&quot;#二、分类&quot; class=&quot;headerlink&quot; title=&quot;二、分类&quot;&gt;&lt;/a
      
    
    </summary>
    
      <category term="NLP" scheme="http://yoursite.com/categories/NLP/"/>
    
      <category term="6-文本向量化" scheme="http://yoursite.com/categories/NLP/6-%E6%96%87%E6%9C%AC%E5%90%91%E9%87%8F%E5%8C%96/"/>
    
    
  </entry>
  
  <entry>
    <title>这里是句法分析</title>
    <link href="http://yoursite.com/2017/12/18/NLP/2-%E5%8F%A5%E6%B3%95%E5%88%86%E6%9E%90/%E8%BF%99%E9%87%8C%E6%98%AF%E5%8F%A5%E6%B3%95%E5%88%86%E6%9E%90/"/>
    <id>http://yoursite.com/2017/12/18/NLP/2-句法分析/这里是句法分析/</id>
    <published>2017-12-18T09:17:35.000Z</published>
    <updated>2018-08-12T08:48:45.815Z</updated>
    
    <summary type="html">
    
    </summary>
    
      <category term="NLP" scheme="http://yoursite.com/categories/NLP/"/>
    
      <category term="2-句法分析" scheme="http://yoursite.com/categories/NLP/2-%E5%8F%A5%E6%B3%95%E5%88%86%E6%9E%90/"/>
    
    
  </entry>
  
  <entry>
    <title>1.朴素贝叶斯算法</title>
    <link href="http://yoursite.com/2017/12/18/NLP/7-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/1.%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95/"/>
    <id>http://yoursite.com/2017/12/18/NLP/7-文本分类/1.朴素贝叶斯算法/</id>
    <published>2017-12-18T09:17:35.000Z</published>
    <updated>2018-09-18T10:06:14.531Z</updated>
    
    <content type="html"><![CDATA[<h4 id="一、基本概念"><a href="#一、基本概念" class="headerlink" title="一、基本概念"></a>一、基本概念</h4><h5 id="1-分类原理"><a href="#1-分类原理" class="headerlink" title="1.分类原理"></a>1.分类原理</h5><p>通过某对象的先验概率，利用贝叶斯公式，计算出其后验概率。即该对象属于某一类的概率，选择具有最大后验概率的类作为该对象所属的类。</p><h5 id="2-贝叶斯公式"><a href="#2-贝叶斯公式" class="headerlink" title="2.贝叶斯公式"></a>2.贝叶斯公式</h5><p>$$P(A|B) = \frac{P(B|A)*P(A)}{P(B)}$$</p><ul><li>(1).其中P(A)为先验概率：先验概率（prior probability）是指根据以往经验和分析得到的概率，如全概率公式，它往往作为”由因求果”问题中的”因”出现的概率。；</li><li>(2).其中P(B|A)为似然概率(likelihood)：是先前统计的事件中，A事件发生情况下B事件发生的概率</li><li>(3).其中P(B)为边界似然概率；</li><li>(4).其中P(A|B)为后验概率；<h5 id="3-相关概念"><a href="#3-相关概念" class="headerlink" title="3.相关概念"></a>3.相关概念</h5></li><li>(1).先验概率</li><li>(2).—-<pre><code>- ①后验概率- ②.最大后验概率</code></pre></li><li>(3).—<ul><li>①.条件概率：指事件A在另外一个事件B已经发生条件下的发生概率。条件概率表示为：P(A|B)。若只有两个事件A，B，那么$P(A|B) = \frac{P(AB)}{P(B)}$</li><li>②.联合概率：表示两个事件共同发生的概率。A与B的联合概率表示为 P(AB) 或者P(A,B),或者P（A∩B）</li><li>③.全概率</li></ul></li><li>(4).似然概率 </li></ul><p>联合概率的乘法公式为：P(AB) = P(A|B)*P(B),变形后可得到$P(A|B) = \frac{P(AB)}{P(B)}$</p><h4 id="二、朴素贝叶斯分类器"><a href="#二、朴素贝叶斯分类器" class="headerlink" title="二、朴素贝叶斯分类器"></a>二、朴素贝叶斯分类器</h4><h5 id="1-朴素贝叶斯分类的定义"><a href="#1-朴素贝叶斯分类的定义" class="headerlink" title="1.朴素贝叶斯分类的定义"></a>1.朴素贝叶斯分类的定义</h5><p>朴素贝叶斯分类的正式定义如下：<br>(1).设 $x = \left{  f<em>{1},f</em>{2},f<em>{3},…,f</em>{m} \right}$为一个待分类项，而每个f为x的一个特征。<br>(2).有类别集合$C = \left{  y<em>{1},y</em>{2},y<em>{3},…,y</em>{n} \right}$<br>(3).计算$P(y<em>{1}|x),P(y</em>{2}|x),….,P(y<em>{n}|x)$<br>(4).如果$P(y</em>{k}|x) = max \left{ P(y<em>{1}|x),P(y</em>{2}|x),….,P(y<em>{n}|x) \right}，则x∈y</em>{k}$。<br>那么现在的关键就是如何计算第3步中的各个条件概率。我们可以这么做：<br>(1)、找到一个已知分类的待分类项集合，这个集合叫做训练样本集。<br>(2)、统计得到在各类别下各个特征属性的条件概率估计。即<br>$P(x<em>{1}|y</em>{1}),P(x<em>{2}|y</em>{1}),…,P(x<em>{3}|y</em>{1})$;<br>$P(x<em>{1}|y</em>{2}),P(x<em>{2}|y</em>{2}),…,P(x<em>{3}|y</em>{2})$;<br>……<br>$P(x<em>{1}|y</em>{3}),P(x<em>{2}|y</em>{3}),…,P(x<em>{3}|y</em>{3})$;<br>(3)、如果各个特征属性是条件独立的，则根据贝叶斯定理有如下推导：<br>$P(y<em>{i}|x) = \frac {P(x|y</em>{i})P(y<em>{i})}{P(x)}$<br> 因为分母对于所有类别为常数，因为我们只要将分子最大化皆可。又因为各特征属性是条件独立的，所以有：<br> $P(x|y</em>{i})P(y<em>{i}) = P(a</em>{1}|y<em>{i})P(a</em>{2}|y<em>{i})…P(a</em>{m}|y<em>{i})P(y</em>{i})=P(y<em>(i))\prod\limits</em>{j=1}^mP(a_j|yi)$</p><h5 id="2-朴素贝叶斯分类的举例"><a href="#2-朴素贝叶斯分类的举例" class="headerlink" title="2.朴素贝叶斯分类的举例"></a>2.朴素贝叶斯分类的举例</h5><p>我们知道朴素贝叶斯的公式为：<br>$P(C|f) = \frac{P(f|C)<em>P(C)}{P(f)}$<br>如果换个表达式就会明朗很多，如下：<br>$P(类别|特征) = \frac{P(特征|分类)</em>P(分类)}{P(特征)}$<br>其中P(分类)和P(特征)都是已知的，我们只需求P(特征|分类)即可。</p><h5 id="例题分析"><a href="#例题分析" class="headerlink" title="例题分析"></a>例题分析</h5><p>给定数据如下：<br>| 高否 | 富否 | 帅否 | 嫁否 |<br>| – | — | – | – |<br>| 高 | 富 | 帅 | 嫁 |<br>| 高 | 不富 | 帅 | 嫁 |<br>| 不高 | 富 | 不帅 | 嫁 |<br>| 高 | 富 | 不帅 | 不嫁 |<br>| 不高 | 不富 | 帅 | 不嫁 |<br>| 高 | 不富 | 不帅 | 不嫁 |<br>| 不高 | 不富 | 不帅 | 不嫁 |<br>那我们现在的问题是，一个男生向一个女生求婚，这个男生具有以下三个特点：不高、富、帅，请你判断以下该女孩是否会嫁？</p><p>这是一个典型的分类问题，转化为概率论问题就是$P(嫁|不高、富、帅)$ 与 $P(不嫁|不高、富、帅)那个概率更大？</p><p>这里我们就使用朴素贝叶斯公式来分别求以下两种情况下的概率：</p><ul><li>①$P(嫁|不高、富、帅) = \frac {P(不高、富、帅|嫁)*P(嫁)}{P(不高、富、帅)}$</li><li>②$P(不嫁|不高、富、帅) = \frac {P(不高、富、帅|不嫁)*P(不嫁)}{P(不高、富、帅)}$<h6 id="对①求解"><a href="#对①求解" class="headerlink" title="对①求解"></a>对①求解</h6>我们先对要求①进行求解。要求$P(嫁|不高、富、帅)$的概率只需求$P(不高、富、帅|嫁)、P(嫁)、P(不高、富、帅)$即可。根据“朴素”一词也就是各个特征之间是独立的，可以得到如下<1>公式和<2>公式(只需求如下公式即可)：<br><1>$P(不高、富、帅|嫁) = P(不高|嫁)<em>P(富|嫁)</em>P(帅|嫁)$<br><2>$P(不高、富、帅) = P(不高)<em>P(富)</em>P(帅)$<br>同时只需要再求出公式<3>问题就得到解决<br><3>$P(嫁)$</3></3></2></1></2></1></li></ul><p>我们从表格中统计所有嫁的样本共有3条，其中不高的样本有1条，所以$P(不高|嫁) = 1/3$，同理可以得到$P(富|嫁) = 2/3$， $P(帅|嫁) = 2/3$。</p><p>我们从表格中统计所有样本共有7条，其中嫁的样本有3条，所以 $P(嫁) = 3/7$。</p><p>我们从表格中统计所有样本共有7条，其中不高的样本有3条，所以 $P(不高) = 3/7$，其中富的样本有3条所以$P(富) = 3/7$，其中帅的样本有3条所以$P(帅) = 3/7$。</p><p>综上①$P(嫁|不高、富、帅) = \frac {P(不高、富、帅|嫁)<em>P(嫁)}{ \; P(不高、富、帅) \; } $<br> 此公式 $= \frac {(\frac {1}{3} </em>\frac {2}{3}<em>\frac {2}{3})</em>\frac {3}{7}}  {\frac {3}{7} <em>\frac {3}{7}</em>\frac {3}{7}} =\frac {196}{243}$</p><h6 id="对②求解"><a href="#对②求解" class="headerlink" title="对②求解"></a>对②求解</h6><p>$P(不嫁|不高、富、帅) = \frac {P(不高、富、帅|不嫁)<em>P(不嫁)}{P(不高、富、帅)}$<br>此公式  $= \frac {(\frac {1}{2} </em>\frac {1}{4}<em>\frac {1}{4})</em>\frac {4}{7}}  {\frac {3}{7} <em>\frac {3}{7}</em>\frac {3}{7}} =\frac {49}{216}$</p><p>所以最终的答案是“嫁”<br>参考：<br>1.<a href="https://blog.csdn.net/xueyingxue001/article/details/52396170" target="_blank" rel="noopener">https://blog.csdn.net/xueyingxue001/article/details/52396170</a><br>2.<a href="https://blog.csdn.net/amds123/article/details/70173402" target="_blank" rel="noopener">https://blog.csdn.net/amds123/article/details/70173402</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;一、基本概念&quot;&gt;&lt;a href=&quot;#一、基本概念&quot; class=&quot;headerlink&quot; title=&quot;一、基本概念&quot;&gt;&lt;/a&gt;一、基本概念&lt;/h4&gt;&lt;h5 id=&quot;1-分类原理&quot;&gt;&lt;a href=&quot;#1-分类原理&quot; class=&quot;headerlink&quot; titl
      
    
    </summary>
    
      <category term="NLP" scheme="http://yoursite.com/categories/NLP/"/>
    
      <category term="7-文本分类" scheme="http://yoursite.com/categories/NLP/7-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/"/>
    
    
  </entry>
  
</feed>
