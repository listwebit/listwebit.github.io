<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>帅的掉渣的博客</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2018-01-09T08:31:26.000Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>listwebit</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>回归的分类</title>
    <link href="http://yoursite.com/2018/01/09/MachineLearning/%E5%9B%9E%E5%BD%92%E7%9A%84%E5%88%86%E7%B1%BB/"/>
    <id>http://yoursite.com/2018/01/09/MachineLearning/回归的分类/</id>
    <published>2018-01-09T08:02:27.000Z</published>
    <updated>2018-01-09T08:31:26.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、回归可以分为以下几类"><a href="#一、回归可以分为以下几类" class="headerlink" title="一、回归可以分为以下几类"></a>一、回归可以分为以下几类</h2><ul><li>1.线性回归</li><li>2.非线性回归</li><li>3.逻辑回归<a id="more"></a><h2 id="二、回归的概念"><a href="#二、回归的概念" class="headerlink" title="二、回归的概念"></a>二、回归的概念</h2><h4 id="1-线性回归"><a href="#1-线性回归" class="headerlink" title="1.线性回归"></a>1.线性回归</h4>可以简单理解为线性就是每个变量的指数都是1。</li></ul><h4 id="2-非线性回归"><a href="#2-非线性回归" class="headerlink" title="2.非线性回归"></a>2.非线性回归</h4><p>而非线性就是至少有一个变量的指数不是1。</p><h4 id="3-逻辑回归"><a href="#3-逻辑回归" class="headerlink" title="3.逻辑回归"></a>3.逻辑回归</h4><h1 id="回归和分类的区别"><a href="#回归和分类的区别" class="headerlink" title="回归和分类的区别"></a>回归和分类的区别</h1><p>分类和回归的区别在于输出变量的类型。</p><p>定量输出称为回归，或者说是连续变量预测；<br>定性输出称为分类，或者说是离散变量预测。</p><p>举个例子：<br>预测明天的气温是多少度，这是一个回归任务；<br>预测明天是阴、晴还是雨，就是一个分类任务。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;一、回归可以分为以下几类&quot;&gt;&lt;a href=&quot;#一、回归可以分为以下几类&quot; class=&quot;headerlink&quot; title=&quot;一、回归可以分为以下几类&quot;&gt;&lt;/a&gt;一、回归可以分为以下几类&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;1.线性回归&lt;/li&gt;
&lt;li&gt;2.非线性回归&lt;/li&gt;
&lt;li&gt;3.逻辑回归
    
    </summary>
    
      <category term="MachineLearning" scheme="http://yoursite.com/categories/MachineLearning/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>deeplearning算法分类</title>
    <link href="http://yoursite.com/2018/01/09/DeepLearning/deeplearning/"/>
    <id>http://yoursite.com/2018/01/09/DeepLearning/deeplearning/</id>
    <published>2018-01-09T08:02:27.000Z</published>
    <updated>2018-09-12T05:13:33.101Z</updated>
    
    <content type="html"><![CDATA[<h4 id="一、常用神经网络分类"><a href="#一、常用神经网络分类" class="headerlink" title="一、常用神经网络分类"></a>一、常用神经网络分类</h4><h6 id="1-感知机-神经网络的前身"><a href="#1-感知机-神经网络的前身" class="headerlink" title="1.感知机(神经网络的前身)"></a>1.感知机(神经网络的前身)</h6><h6 id="2-前向传播算法"><a href="#2-前向传播算法" class="headerlink" title="2.前向传播算法"></a>2.前向传播算法</h6><h6 id="3-BP-反向传播算法"><a href="#3-BP-反向传播算法" class="headerlink" title="3.BP(反向传播算法)"></a>3.BP(反向传播算法)</h6><h6 id="4-DNN-深度神经网络"><a href="#4-DNN-深度神经网络" class="headerlink" title="4.DNN(深度神经网络)"></a>4.DNN(深度神经网络)</h6><h6 id="5-CNN-卷积神经网络"><a href="#5-CNN-卷积神经网络" class="headerlink" title="5.CNN(卷积神经网络)"></a>5.CNN(卷积神经网络)</h6><p>二、学习资料</p><ol><li>Michael Nielsen的《Neural Network and Deep Learning》<br>中文翻译《神经网络与深度学习》  </li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;一、常用神经网络分类&quot;&gt;&lt;a href=&quot;#一、常用神经网络分类&quot; class=&quot;headerlink&quot; title=&quot;一、常用神经网络分类&quot;&gt;&lt;/a&gt;一、常用神经网络分类&lt;/h4&gt;&lt;h6 id=&quot;1-感知机-神经网络的前身&quot;&gt;&lt;a href=&quot;#1-感知机-神经
      
    
    </summary>
    
      <category term="DeepLearning" scheme="http://yoursite.com/categories/DeepLearning/"/>
    
    
  </entry>
  
  <entry>
    <title>回归</title>
    <link href="http://yoursite.com/2018/01/09/MachineLearning/%E5%9B%9E%E5%BD%92/"/>
    <id>http://yoursite.com/2018/01/09/MachineLearning/回归/</id>
    <published>2018-01-09T08:02:27.000Z</published>
    <updated>2018-01-09T08:02:27.288Z</updated>
    
    <summary type="html">
    
    </summary>
    
      <category term="MachineLearning" scheme="http://yoursite.com/categories/MachineLearning/"/>
    
    
  </entry>
  
  <entry>
    <title>2017年总结</title>
    <link href="http://yoursite.com/2018/01/09/%E6%80%BB%E7%BB%93/2017%E5%B9%B4%E6%80%BB%E7%BB%93/"/>
    <id>http://yoursite.com/2018/01/09/总结/2017年总结/</id>
    <published>2018-01-09T06:55:28.000Z</published>
    <updated>2018-08-11T01:06:32.802Z</updated>
    
    <summary type="html">
    
    </summary>
    
      <category term="总结" scheme="http://yoursite.com/categories/%E6%80%BB%E7%BB%93/"/>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>2017年</title>
    <link href="http://yoursite.com/2018/01/09/%E6%80%BB%E7%BB%93/2017%E5%B9%B4/"/>
    <id>http://yoursite.com/2018/01/09/总结/2017年/</id>
    <published>2018-01-09T06:55:03.000Z</published>
    <updated>2018-01-09T06:55:03.415Z</updated>
    
    <summary type="html">
    
    </summary>
    
      <category term="总结" scheme="http://yoursite.com/categories/%E6%80%BB%E7%BB%93/"/>
    
    
  </entry>
  
  <entry>
    <title>0文本分类算法总结</title>
    <link href="http://yoursite.com/2017/12/18/NLP/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/0.%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/"/>
    <id>http://yoursite.com/2017/12/18/NLP/文本分类/0.文本分类算法总结/</id>
    <published>2017-12-18T09:17:35.000Z</published>
    <updated>2018-09-18T10:05:46.154Z</updated>
    
    <summary type="html">
    
    </summary>
    
      <category term="NLP" scheme="http://yoursite.com/categories/NLP/"/>
    
      <category term="文本分类" scheme="http://yoursite.com/categories/NLP/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/"/>
    
    
  </entry>
  
  <entry>
    <title>1.朴素贝叶斯算法</title>
    <link href="http://yoursite.com/2017/12/18/NLP/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/1.%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95/"/>
    <id>http://yoursite.com/2017/12/18/NLP/文本分类/1.朴素贝叶斯算法/</id>
    <published>2017-12-18T09:17:35.000Z</published>
    <updated>2018-09-18T10:06:14.531Z</updated>
    
    <content type="html"><![CDATA[<h4 id="一、基本概念"><a href="#一、基本概念" class="headerlink" title="一、基本概念"></a>一、基本概念</h4><h5 id="1-分类原理"><a href="#1-分类原理" class="headerlink" title="1.分类原理"></a>1.分类原理</h5><p>通过某对象的先验概率，利用贝叶斯公式，计算出其后验概率。即该对象属于某一类的概率，选择具有最大后验概率的类作为该对象所属的类。</p><h5 id="2-贝叶斯公式"><a href="#2-贝叶斯公式" class="headerlink" title="2.贝叶斯公式"></a>2.贝叶斯公式</h5><p>$$P(A|B) = \frac{P(B|A)*P(A)}{P(B)}$$</p><ul><li>(1).其中P(A)为先验概率：先验概率（prior probability）是指根据以往经验和分析得到的概率，如全概率公式，它往往作为”由因求果”问题中的”因”出现的概率。；</li><li>(2).其中P(B|A)为似然概率(likelihood)：是先前统计的事件中，A事件发生情况下B事件发生的概率</li><li>(3).其中P(B)为边界似然概率；</li><li>(4).其中P(A|B)为后验概率；<h5 id="3-相关概念"><a href="#3-相关概念" class="headerlink" title="3.相关概念"></a>3.相关概念</h5></li><li>(1).先验概率</li><li>(2).—-<pre><code>- ①后验概率- ②.最大后验概率</code></pre></li><li>(3).—<ul><li>①.条件概率：指事件A在另外一个事件B已经发生条件下的发生概率。条件概率表示为：P(A|B)。若只有两个事件A，B，那么$P(A|B) = \frac{P(AB)}{P(B)}$</li><li>②.联合概率：表示两个事件共同发生的概率。A与B的联合概率表示为 P(AB) 或者P(A,B),或者P（A∩B）</li><li>③.全概率</li></ul></li><li>(4).似然概率 </li></ul><p>联合概率的乘法公式为：P(AB) = P(A|B)*P(B),变形后可得到$P(A|B) = \frac{P(AB)}{P(B)}$</p><h4 id="二、朴素贝叶斯分类器"><a href="#二、朴素贝叶斯分类器" class="headerlink" title="二、朴素贝叶斯分类器"></a>二、朴素贝叶斯分类器</h4><h5 id="1-朴素贝叶斯分类的定义"><a href="#1-朴素贝叶斯分类的定义" class="headerlink" title="1.朴素贝叶斯分类的定义"></a>1.朴素贝叶斯分类的定义</h5><p>朴素贝叶斯分类的正式定义如下：<br>(1).设 $x = \left{  f<em>{1},f</em>{2},f<em>{3},…,f</em>{m} \right}$为一个待分类项，而每个f为x的一个特征。<br>(2).有类别集合$C = \left{  y<em>{1},y</em>{2},y<em>{3},…,y</em>{n} \right}$<br>(3).计算$P(y<em>{1}|x),P(y</em>{2}|x),….,P(y<em>{n}|x)$<br>(4).如果$P(y</em>{k}|x) = max \left{ P(y<em>{1}|x),P(y</em>{2}|x),….,P(y<em>{n}|x) \right}，则x∈y</em>{k}$。<br>那么现在的关键就是如何计算第3步中的各个条件概率。我们可以这么做：<br>(1)、找到一个已知分类的待分类项集合，这个集合叫做训练样本集。<br>(2)、统计得到在各类别下各个特征属性的条件概率估计。即<br>$P(x<em>{1}|y</em>{1}),P(x<em>{2}|y</em>{1}),…,P(x<em>{3}|y</em>{1})$;<br>$P(x<em>{1}|y</em>{2}),P(x<em>{2}|y</em>{2}),…,P(x<em>{3}|y</em>{2})$;<br>……<br>$P(x<em>{1}|y</em>{3}),P(x<em>{2}|y</em>{3}),…,P(x<em>{3}|y</em>{3})$;<br>(3)、如果各个特征属性是条件独立的，则根据贝叶斯定理有如下推导：<br>$P(y<em>{i}|x) = \frac {P(x|y</em>{i})P(y<em>{i})}{P(x)}$<br> 因为分母对于所有类别为常数，因为我们只要将分子最大化皆可。又因为各特征属性是条件独立的，所以有：<br> $P(x|y</em>{i})P(y<em>{i}) = P(a</em>{1}|y<em>{i})P(a</em>{2}|y<em>{i})…P(a</em>{m}|y<em>{i})P(y</em>{i})=P(y<em>(i))\prod\limits</em>{j=1}^mP(a_j|yi)$</p><h5 id="2-朴素贝叶斯分类的举例"><a href="#2-朴素贝叶斯分类的举例" class="headerlink" title="2.朴素贝叶斯分类的举例"></a>2.朴素贝叶斯分类的举例</h5><p>我们知道朴素贝叶斯的公式为：<br>$P(C|f) = \frac{P(f|C)<em>P(C)}{P(f)}$<br>如果换个表达式就会明朗很多，如下：<br>$P(类别|特征) = \frac{P(特征|分类)</em>P(分类)}{P(特征)}$<br>其中P(分类)和P(特征)都是已知的，我们只需求P(特征|分类)即可。</p><h5 id="例题分析"><a href="#例题分析" class="headerlink" title="例题分析"></a>例题分析</h5><p>给定数据如下：<br>| 高否 | 富否 | 帅否 | 嫁否 |<br>| – | — | – | – |<br>| 高 | 富 | 帅 | 嫁 |<br>| 高 | 不富 | 帅 | 嫁 |<br>| 不高 | 富 | 不帅 | 嫁 |<br>| 高 | 富 | 不帅 | 不嫁 |<br>| 不高 | 不富 | 帅 | 不嫁 |<br>| 高 | 不富 | 不帅 | 不嫁 |<br>| 不高 | 不富 | 不帅 | 不嫁 |<br>那我们现在的问题是，一个男生向一个女生求婚，这个男生具有以下三个特点：不高、富、帅，请你判断以下该女孩是否会嫁？</p><p>这是一个典型的分类问题，转化为概率论问题就是$P(嫁|不高、富、帅)$ 与 $P(不嫁|不高、富、帅)那个概率更大？</p><p>这里我们就使用朴素贝叶斯公式来分别求以下两种情况下的概率：</p><ul><li>①$P(嫁|不高、富、帅) = \frac {P(不高、富、帅|嫁)*P(嫁)}{P(不高、富、帅)}$</li><li>②$P(不嫁|不高、富、帅) = \frac {P(不高、富、帅|不嫁)*P(不嫁)}{P(不高、富、帅)}$<h6 id="对①求解"><a href="#对①求解" class="headerlink" title="对①求解"></a>对①求解</h6>我们先对要求①进行求解。要求$P(嫁|不高、富、帅)$的概率只需求$P(不高、富、帅|嫁)、P(嫁)、P(不高、富、帅)$即可。根据“朴素”一词也就是各个特征之间是独立的，可以得到如下<1>公式和<2>公式(只需求如下公式即可)：<br><1>$P(不高、富、帅|嫁) = P(不高|嫁)<em>P(富|嫁)</em>P(帅|嫁)$<br><2>$P(不高、富、帅) = P(不高)<em>P(富)</em>P(帅)$<br>同时只需要再求出公式<3>问题就得到解决<br><3>$P(嫁)$</3></3></2></1></2></1></li></ul><p>我们从表格中统计所有嫁的样本共有3条，其中不高的样本有1条，所以$P(不高|嫁) = 1/3$，同理可以得到$P(富|嫁) = 2/3$， $P(帅|嫁) = 2/3$。</p><p>我们从表格中统计所有样本共有7条，其中嫁的样本有3条，所以 $P(嫁) = 3/7$。</p><p>我们从表格中统计所有样本共有7条，其中不高的样本有3条，所以 $P(不高) = 3/7$，其中富的样本有3条所以$P(富) = 3/7$，其中帅的样本有3条所以$P(帅) = 3/7$。</p><p>综上①$P(嫁|不高、富、帅) = \frac {P(不高、富、帅|嫁)<em>P(嫁)}{ \; P(不高、富、帅) \; } $<br> 此公式 $= \frac {(\frac {1}{3} </em>\frac {2}{3}<em>\frac {2}{3})</em>\frac {3}{7}}  {\frac {3}{7} <em>\frac {3}{7}</em>\frac {3}{7}} =\frac {196}{243}$</p><h6 id="对②求解"><a href="#对②求解" class="headerlink" title="对②求解"></a>对②求解</h6><p>$P(不嫁|不高、富、帅) = \frac {P(不高、富、帅|不嫁)<em>P(不嫁)}{P(不高、富、帅)}$<br>此公式  $= \frac {(\frac {1}{2} </em>\frac {1}{4}<em>\frac {1}{4})</em>\frac {4}{7}}  {\frac {3}{7} <em>\frac {3}{7}</em>\frac {3}{7}} =\frac {49}{216}$</p><p>所以最终的答案是“嫁”<br>参考：<br>1.<a href="https://blog.csdn.net/xueyingxue001/article/details/52396170" target="_blank" rel="noopener">https://blog.csdn.net/xueyingxue001/article/details/52396170</a><br>2.<a href="https://blog.csdn.net/amds123/article/details/70173402" target="_blank" rel="noopener">https://blog.csdn.net/amds123/article/details/70173402</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;一、基本概念&quot;&gt;&lt;a href=&quot;#一、基本概念&quot; class=&quot;headerlink&quot; title=&quot;一、基本概念&quot;&gt;&lt;/a&gt;一、基本概念&lt;/h4&gt;&lt;h5 id=&quot;1-分类原理&quot;&gt;&lt;a href=&quot;#1-分类原理&quot; class=&quot;headerlink&quot; titl
      
    
    </summary>
    
      <category term="NLP" scheme="http://yoursite.com/categories/NLP/"/>
    
      <category term="文本分类" scheme="http://yoursite.com/categories/NLP/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/"/>
    
    
  </entry>
  
  <entry>
    <title>PageRank算法的前世今生</title>
    <link href="http://yoursite.com/2017/12/18/NLP/%E8%AF%8D%E6%B3%95%E5%88%86%E6%9E%90/%E5%85%B3%E9%94%AE%E8%AF%8D%E6%8F%90%E5%8F%96/1.PageRank%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F/"/>
    <id>http://yoursite.com/2017/12/18/NLP/词法分析/关键词提取/1.PageRank的前世今生/</id>
    <published>2017-12-18T09:17:35.000Z</published>
    <updated>2018-08-13T00:08:23.140Z</updated>
    
    <content type="html"><![CDATA[<h4 id="一、算法定义"><a href="#一、算法定义" class="headerlink" title="一、算法定义"></a>一、算法定义</h4><p>PageRank，网页排名，又称网页级别、Google左侧排名或佩奇排名，是一种由 [1]  根据网页之间相互的超链接计算的技术，而作为网页排名的要素之一，以Google公司创办人拉里·佩奇（Larry Page）之姓来命名。Google用它来体现网页的相关性和重要性，在搜索引擎优化操作中是经常被用来评估网页优化的成效因素之一。Google的创始人拉里·佩奇和谢尔盖·布林于1998年在斯坦福大学发明了这项技术。</p><p>PageRank通过网络浩瀚的超链接关系来确定一个页面的等级。Google把从A页面到B页面的链接解释为A页面给B页面投票，Google根据投票来源（甚至来源的来源，即链接到A页面的页面）和投票目标的等级来决定新的等级。简单的说，一个高等级的页面可以使其他低等级页面的等级提升。</p><h4 id="二、算法来源"><a href="#二、算法来源" class="headerlink" title="二、算法来源"></a>二、算法来源</h4><p>这个要从搜索引擎的发展讲起。最早的搜索引擎采用的是 分类目录[^ref_1] 的方法，即通过人工进行网页分类并整理出高质量的网站。那时 Yahoo 和国内的 hao123 就是使用的这种方法。</p><p>后来网页越来越多，人工分类已经不现实了。搜索引擎进入了 文本检索 的时代，即计算用户查询关键词与网页内容的相关程度来返回搜索结果。这种方法突破了数量的限制，但是搜索结果不是很好。因为总有某些网页来回地倒腾某些关键词使自己的搜索排名靠前。</p><p>于是我们的主角要登场了。没错，谷歌的两位创始人，当时还是美国斯坦福大学 (Stanford University) 研究生的佩奇 (Larry Page) 和布林 (Sergey Brin) 开始了对网页排序问题的研究。他们的借鉴了学术界评判学术论文重要性的通用方法， 那就是看论文的引用次数。由此想到网页的重要性也可以根据这种方法来评价。于是PageRank的核心思想就诞生了[^ref_2]，非常简单：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1.如果一个网页被很多其他网页链接到的话说明这个网页比较重要，也就是PageRank值会相对较高</span><br><span class="line">2.如果一个PageRank值很高的网页链接到一个其他的网页，那么被链接到的网页的PageRank值会相应地因此而提高</span><br></pre></td></tr></table></figure></p><h4 id="三、搜索引擎"><a href="#三、搜索引擎" class="headerlink" title="三、搜索引擎"></a>三、搜索引擎</h4><p>搜索引擎的功能主要是：根据用户输入的关键字，返回文档的链接结果。<br>搜索引擎主要解决的三大问题：(1)如何获取文档资料  (2)如何根据关键词检索到相关文档  (3)如何对文档进行排序，返回给用户满意的页面。</p><h6 id="1-获取文档资料"><a href="#1-获取文档资料" class="headerlink" title="1.获取文档资料"></a>1.获取文档资料</h6><p>利用爬虫程序，获取互联网的页面资料。爬虫技术先从一个网页出发，将该网页的内容记录下来，保存到资料库，接着分析页面中的超链接，分别递归得去获取超链接页面。</p><h6 id="2-如何根据关键词检索到相关文档"><a href="#2-如何根据关键词检索到相关文档" class="headerlink" title="2.如何根据关键词检索到相关文档"></a>2.如何根据关键词检索到相关文档</h6><p>采用倒排索引算法。简单的说，倒排索引是一对key-value对，key代表关键词，value代表拥有这些关键词的文档编号或者url。</p><h6 id="3-文档排序"><a href="#3-文档排序" class="headerlink" title="3.文档排序"></a>3.文档排序</h6><p>这是搜索引擎最核心的问题，也是google发家致富的法宝 – PageRank算法。</p><h4 id="四、算法原理"><a href="#四、算法原理" class="headerlink" title="四、算法原理"></a>四、算法原理</h4><h6 id="4-1PageRank的两个假设"><a href="#4-1PageRank的两个假设" class="headerlink" title="4.1PageRank的两个假设"></a>4.1PageRank的两个假设</h6><blockquote><p>数量假设：1.如果一个网页被很多其他网页链接到的话说明这个网页比较重要，也就是PageRank值会相对较高。<br>质量假设：2.如果一个PageRank值很高的网页链接到一个其他的网页，那么被链接到的网页的PageRank值会相应地因此而提高。</p></blockquote><h6 id="4-2PageRank的算法原理"><a href="#4-2PageRank的算法原理" class="headerlink" title="4.2PageRank的算法原理"></a>4.2PageRank的算法原理</h6><p>PageRank算法总的来说就是预先给每个网页一个PR值（下面用PR值指代PageRank值），由于PR值物理意义上为一个网页被访问概率，所以一般是1/N，其中N为网页总数。<br>预先给定PR值后，通过下面的算法不断迭代，直至达到平稳分布为止。</p><p><strong><em>PageRank值主要是的是节点的入链值。</em></strong></p><h6 id="4-3PageRank的简单计算"><a href="#4-3PageRank的简单计算" class="headerlink" title="4.3PageRank的简单计算"></a>4.3PageRank的简单计算</h6><h6 id="4-4PageRank的修正计算"><a href="#4-4PageRank的修正计算" class="headerlink" title="4.4PageRank的修正计算"></a>4.4PageRank的修正计算</h6>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;一、算法定义&quot;&gt;&lt;a href=&quot;#一、算法定义&quot; class=&quot;headerlink&quot; title=&quot;一、算法定义&quot;&gt;&lt;/a&gt;一、算法定义&lt;/h4&gt;&lt;p&gt;PageRank，网页排名，又称网页级别、Google左侧排名或佩奇排名，是一种由 [1]  根据网页之间相
      
    
    </summary>
    
      <category term="NLP" scheme="http://yoursite.com/categories/NLP/"/>
    
      <category term="词法分析" scheme="http://yoursite.com/categories/NLP/%E8%AF%8D%E6%B3%95%E5%88%86%E6%9E%90/"/>
    
      <category term="关键词提取" scheme="http://yoursite.com/categories/NLP/%E8%AF%8D%E6%B3%95%E5%88%86%E6%9E%90/%E5%85%B3%E9%94%AE%E8%AF%8D%E6%8F%90%E5%8F%96/"/>
    
    
  </entry>
  
  <entry>
    <title>关键词提取的分类</title>
    <link href="http://yoursite.com/2017/12/18/NLP/%E8%AF%8D%E6%B3%95%E5%88%86%E6%9E%90/%E5%85%B3%E9%94%AE%E8%AF%8D%E6%8F%90%E5%8F%96/0.%E5%85%B3%E9%94%AE%E8%AF%8D%E6%8F%90%E5%8F%96%E7%9A%84%E5%88%86%E7%B1%BB/"/>
    <id>http://yoursite.com/2017/12/18/NLP/词法分析/关键词提取/0.关键词提取的分类/</id>
    <published>2017-12-18T09:17:35.000Z</published>
    <updated>2018-08-13T00:11:22.877Z</updated>
    
    <content type="html"><![CDATA[<h4 id="一、关键词抽取分类"><a href="#一、关键词抽取分类" class="headerlink" title="一、关键词抽取分类"></a>一、关键词抽取分类</h4><p>关键词抽取算法可以分为两类</p><h6 id="1-使用外部的知识库"><a href="#1-使用外部的知识库" class="headerlink" title="1.使用外部的知识库"></a>1.使用外部的知识库</h6><p>(1).TF-IDF<br>TF-IDF关键词提取算法就是需要保存每个词的IDF值作为外部知识库<br>(2).LDA模型</p><h6 id="2-不适用外部知识库"><a href="#2-不适用外部知识库" class="headerlink" title="2.不适用外部知识库"></a>2.不适用外部知识库</h6><p>(1).TextRank<br>(2).PageRank</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;一、关键词抽取分类&quot;&gt;&lt;a href=&quot;#一、关键词抽取分类&quot; class=&quot;headerlink&quot; title=&quot;一、关键词抽取分类&quot;&gt;&lt;/a&gt;一、关键词抽取分类&lt;/h4&gt;&lt;p&gt;关键词抽取算法可以分为两类&lt;/p&gt;
&lt;h6 id=&quot;1-使用外部的知识库&quot;&gt;&lt;a h
      
    
    </summary>
    
      <category term="NLP" scheme="http://yoursite.com/categories/NLP/"/>
    
      <category term="词法分析" scheme="http://yoursite.com/categories/NLP/%E8%AF%8D%E6%B3%95%E5%88%86%E6%9E%90/"/>
    
      <category term="关键词提取" scheme="http://yoursite.com/categories/NLP/%E8%AF%8D%E6%B3%95%E5%88%86%E6%9E%90/%E5%85%B3%E9%94%AE%E8%AF%8D%E6%8F%90%E5%8F%96/"/>
    
    
  </entry>
  
  <entry>
    <title>这里是词法分析</title>
    <link href="http://yoursite.com/2017/12/18/NLP/%E8%AF%8D%E6%B3%95%E5%88%86%E6%9E%90/%E8%BF%99%E9%87%8C%E6%98%AF%E8%AF%8D%E6%B3%95%E5%88%86%E6%9E%90/"/>
    <id>http://yoursite.com/2017/12/18/NLP/词法分析/这里是词法分析/</id>
    <published>2017-12-18T09:17:35.000Z</published>
    <updated>2018-08-12T08:48:26.854Z</updated>
    
    <summary type="html">
    
    </summary>
    
      <category term="NLP" scheme="http://yoursite.com/categories/NLP/"/>
    
      <category term="词法分析" scheme="http://yoursite.com/categories/NLP/%E8%AF%8D%E6%B3%95%E5%88%86%E6%9E%90/"/>
    
    
  </entry>
  
  <entry>
    <title>文本聚类算法总结</title>
    <link href="http://yoursite.com/2017/12/18/NLP/%E6%96%87%E6%9C%AC%E8%81%9A%E7%B1%BB/0.%E6%96%87%E6%9C%AC%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/"/>
    <id>http://yoursite.com/2017/12/18/NLP/文本聚类/0.文本聚类算法总结/</id>
    <published>2017-12-18T09:17:35.000Z</published>
    <updated>2018-09-19T09:16:29.300Z</updated>
    
    <content type="html"><![CDATA[<h4 id="一、文本聚类算法总结"><a href="#一、文本聚类算法总结" class="headerlink" title="一、文本聚类算法总结"></a>一、文本聚类算法总结</h4><h5 id="1-划分法"><a href="#1-划分法" class="headerlink" title="1.划分法"></a>1.划分法</h5><p>(partitioning methods)：给定一个有N个元组或者纪录的数据集，分裂法将构造K个分组，每一个分组就代表一个聚类，K&lt;N。而且这K个分组满足下列条件：（1） 每一个分组至少包含一个数据纪录；（2）每一个数据纪录属于且仅属于一个分组（注意：这个要求在某些模糊聚类算法中可以放宽）；对于给定的K，算法首先给出一个初始的分组方法，以后通过反复迭代的方法改变分组，使得每一次改进之后的分组方案都较前一次好，而所谓好的标准就是：同一分组中的记录越近越好，而不同分组中的纪录越远越好。使用这个基本思想的算法有：K-MEANS算法、K-MEDOIDS算法、CLARANS算法；</p><h5 id="2-层次法"><a href="#2-层次法" class="headerlink" title="2.层次法"></a>2.层次法</h5><p>(hierarchical methods)：这种方法对给定的数据集进行层次似的分解，直到某种条件满足为止。具体又可分为“自底向上”和“自顶向下”两种方案。例如在“自底向上”方案中，初始时每一个数据纪录都组成一个单独的组，在接下来的迭代中，它把那些相互邻近的组合并成一个组，直到所有的记录组成一个分组或者某个条件满足为止。代表算法有：BIRCH算法、CURE算法、CHAMELEON算法等；</p><h5 id="3-基于密度的方法"><a href="#3-基于密度的方法" class="headerlink" title="3.基于密度的方法"></a>3.基于密度的方法</h5><p>（density-based methods):基于密度的方法与其它方法的一个根本区别是：它不是基于各种各样的距离的，而是基于密度的。这样就能克服基于距离的算法只能发现“类圆形”的聚类的缺点。这个方法的指导思想就是，只要一个区域中的点的密度大过某个阀值，就把它加到与之相近的聚类中去。代表算法有：DBSCAN算法、OPTICS算法、DENCLUE算法等；</p><h5 id="4-基于网格的方法"><a href="#4-基于网格的方法" class="headerlink" title="4.基于网格的方法"></a>4.基于网格的方法</h5><p>(grid-based methods):这种方法首先将数据空间划分成为有限个单元（cell）的网格结构,所有的处理都是以单个的单元为对象的。这么处理的一个突出的优点就是处理速度很快，通常这是与目标数据库中记录的个数无关的，它只与把数据空间分为多少个单元有关。代表算法有：STING算法、CLIQUE算法、WAVE-CLUSTER算法；</p><h5 id="5-基于模型的方法"><a href="#5-基于模型的方法" class="headerlink" title="5.基于模型的方法"></a>5.基于模型的方法</h5><p>(model-based methods):基于模型的方法给每一个聚类假定一个模型，然后去寻找一个能很好的满足这个模型的数据集。这样一个模型可能是数据点在空间中的密度分布函数或者其它。它的一个潜在的假定就是：目标数据集是由一系列的概率分布所决定的。通常有两种尝试方向：统计的方案和神经网络的方案</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;一、文本聚类算法总结&quot;&gt;&lt;a href=&quot;#一、文本聚类算法总结&quot; class=&quot;headerlink&quot; title=&quot;一、文本聚类算法总结&quot;&gt;&lt;/a&gt;一、文本聚类算法总结&lt;/h4&gt;&lt;h5 id=&quot;1-划分法&quot;&gt;&lt;a href=&quot;#1-划分法&quot; class=&quot;he
      
    
    </summary>
    
      <category term="NLP" scheme="http://yoursite.com/categories/NLP/"/>
    
      <category term="文本聚类" scheme="http://yoursite.com/categories/NLP/%E6%96%87%E6%9C%AC%E8%81%9A%E7%B1%BB/"/>
    
    
  </entry>
  
  <entry>
    <title>这里是句法分析</title>
    <link href="http://yoursite.com/2017/12/18/NLP/%E5%8F%A5%E6%B3%95%E5%88%86%E6%9E%90/%E8%BF%99%E9%87%8C%E6%98%AF%E5%8F%A5%E6%B3%95%E5%88%86%E6%9E%90/"/>
    <id>http://yoursite.com/2017/12/18/NLP/句法分析/这里是句法分析/</id>
    <published>2017-12-18T09:17:35.000Z</published>
    <updated>2018-08-12T08:48:45.815Z</updated>
    
    <summary type="html">
    
    </summary>
    
      <category term="NLP" scheme="http://yoursite.com/categories/NLP/"/>
    
      <category term="句法分析" scheme="http://yoursite.com/categories/NLP/%E5%8F%A5%E6%B3%95%E5%88%86%E6%9E%90/"/>
    
    
  </entry>
  
  <entry>
    <title>人工智能机器学习NLP算法分类总结</title>
    <link href="http://yoursite.com/2017/03/10/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0NLP%E7%AE%97%E6%B3%95%E5%88%86%E7%B1%BB%E6%80%BB%E7%BB%93/"/>
    <id>http://yoursite.com/2017/03/10/人工智能机器学习NLP算法分类总结/</id>
    <published>2017-03-10T02:27:01.000Z</published>
    <updated>2018-09-18T09:49:24.191Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、人工智能学习算法分类"><a href="#一、人工智能学习算法分类" class="headerlink" title="一、人工智能学习算法分类"></a>一、人工智能学习算法分类</h2><p> 人工智能算法大体上来说可以分类两类：基于统计的机器学习算法(Machine Learning)和深度学习算法(Deep Learning)</p><h4 id="总的来说，在sklearn中机器学习算法大概的分类如下："><a href="#总的来说，在sklearn中机器学习算法大概的分类如下：" class="headerlink" title="总的来说，在sklearn中机器学习算法大概的分类如下："></a>总的来说，在sklearn中机器学习算法大概的分类如下：</h4><h5 id="1-纯算法类"><a href="#1-纯算法类" class="headerlink" title="1. 纯算法类"></a>1. 纯算法类</h5><ul><li>(1).回归算法</li><li>(2).分类算法</li><li>(3).聚类算法</li><li>(4)降维算法</li><li>(5)概率图模型算法</li><li>(6)文本挖掘算法</li><li>(7)优化算法</li><li>(8)深度学习算法<h5 id="2-建模方面"><a href="#2-建模方面" class="headerlink" title="2.建模方面"></a>2.建模方面</h5></li><li>(1).模型优化</li><li>(2).数据预处理</li></ul><h2 id="二、详细算法"><a href="#二、详细算法" class="headerlink" title="二、详细算法"></a>二、详细算法</h2><h4 id="1-分类算法"><a href="#1-分类算法" class="headerlink" title="1.分类算法"></a>1.分类算法</h4><ul><li>(1).LR (Logistic Regression，逻辑回归又叫逻辑分类)</li><li>(2).SVM (Support Vector Machine，支持向量机)</li><li>(3).NB  (Naive Bayes，朴素贝叶斯)</li><li>(4).DT  (Decision Tree，决策树)<ul><li>1).C4.5</li><li>2).ID3</li><li>3).CART</li></ul></li><li>(5).集成算法<ul><li>1).Bagging</li><li>2).Random Forest (随机森林)</li><li>3).GB(梯度提升,Gradient boosting)</li><li>4).GBDT (Gradient Boosting Decision Tree)</li><li>5).AdaBoost  </li><li>6).Xgboost</li></ul></li><li><p>(6).最大熵模型</p><h4 id="2-回归算法"><a href="#2-回归算法" class="headerlink" title="2.回归算法"></a>2.回归算法</h4><ul><li>(1).LR (Linear Regression，线性回归)</li><li>(2).SVR (支持向量机回归)</li><li>(3). RR (Ridge Regression，岭回归)<h4 id="3-聚类算法"><a href="#3-聚类算法" class="headerlink" title="3.聚类算法"></a>3.聚类算法</h4></li><li>(1).Knn </li><li>(2).Kmeans 算法</li><li>(3).层次聚类</li><li>(4).密度聚类<h4 id="4-降维算法"><a href="#4-降维算法" class="headerlink" title="4.降维算法"></a>4.降维算法</h4></li><li>(1).SGD  (随机梯度下降)</li><li>(2). <h4 id="5-概率图模型算法"><a href="#5-概率图模型算法" class="headerlink" title="5.概率图模型算法"></a>5.概率图模型算法</h4><ul><li>(1).贝叶斯网络</li><li>(2).HMM</li><li>(3).CRF (条件随机场)<h4 id="6-文本挖掘算法"><a href="#6-文本挖掘算法" class="headerlink" title="6.文本挖掘算法"></a>6.文本挖掘算法</h4><h6 id="1-模型"><a href="#1-模型" class="headerlink" title="(1).模型"></a>(1).模型</h6><ul><li>1).LDA (主题生成模型，Latent Dirichlet Allocation)</li><li>4).最大熵模型<h6 id="2-关键词提取"><a href="#2-关键词提取" class="headerlink" title="(2).关键词提取"></a>(2).关键词提取</h6></li></ul></li><li>1).tf-idf</li><li>2).bm25</li><li>3).textrank</li><li>4).pagerank</li><li>5).左右熵 :左右熵高的作为关键词</li><li>6).互信息： <h6 id="3-词法分析"><a href="#3-词法分析" class="headerlink" title="(3).词法分析"></a>(3).词法分析</h6><ul><li>1).分词<ul><li>①HMM (因马尔科夫)</li><li>②CRF (条件随机场)</li></ul></li><li>2).词性标注</li><li>3).命名实体识别<h6 id="4-句法分析"><a href="#4-句法分析" class="headerlink" title="(4).句法分析"></a>(4).句法分析</h6></li></ul></li><li>1).句法结构分析</li><li>2).依存句法分析<h6 id="5-文本向量化"><a href="#5-文本向量化" class="headerlink" title="(5).文本向量化"></a>(5).文本向量化</h6></li><li>1).tf-idf</li><li>2).word2vec</li><li>3).doc2vec</li><li>4).cw2vec<h6 id="6-距离计算"><a href="#6-距离计算" class="headerlink" title="(6).距离计算"></a>(6).距离计算</h6></li><li>1).欧氏距离</li><li>2).相似度计算</li></ul></li></ul><h4 id="7-优化算法"><a href="#7-优化算法" class="headerlink" title="7.优化算法"></a>7.优化算法</h4><ul><li>(1).正则化<ul><li>1).L1正则化</li><li>2).L2正则化<h4 id="8-深度学习算法"><a href="#8-深度学习算法" class="headerlink" title="8.深度学习算法"></a>8.深度学习算法</h4></li></ul></li><li>(1).BP </li><li>(2).CNN</li><li>(3).DNN</li><li>(3).RNN</li><li>(4).LSTM</li></ul></li></ul><h2 id="三、建模方面"><a href="#三、建模方面" class="headerlink" title="三、建模方面"></a>三、建模方面</h2><h4 id="1-模型优化·"><a href="#1-模型优化·" class="headerlink" title="1.模型优化·"></a>1.模型优化·</h4><ul><li>(1).特征选择</li><li>(2).梯度下降</li><li>(3).交叉验证</li><li>(4).参数调优</li><li>(5).模型评估：准确率、召回率、F1、AUC、ROC、损失函数 <h4 id="2-数据预处理"><a href="#2-数据预处理" class="headerlink" title="2.数据预处理"></a>2.数据预处理</h4></li><li>(1).标准化</li><li>(2).异常值处理</li><li>(3).二值化</li><li>(4).缺失值填充： 支持均值、中位数、特定值补差、多重插补</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;一、人工智能学习算法分类&quot;&gt;&lt;a href=&quot;#一、人工智能学习算法分类&quot; class=&quot;headerlink&quot; title=&quot;一、人工智能学习算法分类&quot;&gt;&lt;/a&gt;一、人工智能学习算法分类&lt;/h2&gt;&lt;p&gt; 人工智能算法大体上来说可以分类两类：基于统计的机器学习算法
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>0.文本特征提取算法总结</title>
    <link href="http://yoursite.com/2016/01/18/NLP/%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96/0.%E6%96%87%E6%9C%AC%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93%20-%20%E5%89%AF%E6%9C%AC/"/>
    <id>http://yoursite.com/2016/01/18/NLP/特征提取/0.文本特征提取算法总结 - 副本/</id>
    <published>2016-01-18T05:17:35.000Z</published>
    <updated>2018-09-18T10:10:34.298Z</updated>
    
    <content type="html"><![CDATA[<p>为分类文本作处理的特征提取算法也对最终效果有巨大影响，而特征提取算法又分为特征选择和特征抽取两大类，其中特征选择算法有互信息，文档频率，信息增益，开方检验等等十数种</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;为分类文本作处理的特征提取算法也对最终效果有巨大影响，而特征提取算法又分为特征选择和特征抽取两大类，其中特征选择算法有互信息，文档频率，信息增益，开方检验等等十数种&lt;/p&gt;

      
    
    </summary>
    
      <category term="NLP" scheme="http://yoursite.com/categories/NLP/"/>
    
      <category term="特征提取" scheme="http://yoursite.com/categories/NLP/%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96/"/>
    
    
  </entry>
  
  <entry>
    <title>1.TF-IDF算法</title>
    <link href="http://yoursite.com/2016/01/18/NLP/%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96/1.TF-IDF/"/>
    <id>http://yoursite.com/2016/01/18/NLP/特征提取/1.TF-IDF/</id>
    <published>2016-01-18T05:17:35.000Z</published>
    <updated>2018-09-18T10:11:34.114Z</updated>
    
    <content type="html"><![CDATA[<p>TF-IDF算法</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;TF-IDF算法&lt;/p&gt;

      
    
    </summary>
    
      <category term="NLP" scheme="http://yoursite.com/categories/NLP/"/>
    
      <category term="特征提取" scheme="http://yoursite.com/categories/NLP/%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96/"/>
    
    
  </entry>
  
</feed>
